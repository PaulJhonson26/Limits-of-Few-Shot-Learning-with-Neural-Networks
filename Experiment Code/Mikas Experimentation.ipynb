{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "mikas_experimentation.ipynb",
      "provenance": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UoqZoi1T0rkg"
      },
      "outputs": [],
      "source": [
        "#Mika's part for experimentation"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#imports\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import string\n",
        "import torch\n",
        "from tqdm.auto import tqdm\n",
        "\n",
        "tqdm.pandas(desc='Progress')\n",
        "import tensorflow\n",
        "\n",
        "#PyTorch imports\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader, random_split, TensorDataset\n",
        "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
        "from torchtext.legacy.data import Field, TabularDataset, BucketIterator\n",
        "from torch.autograd import Variable\n",
        "from sklearn.metrics import f1_score\n",
        "import torchtext\n",
        "import os\n",
        "\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n"
      ],
      "metadata": {
        "id": "pBhfCxW2CgCh"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive/')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zSDSXwu9OSG_",
        "outputId": "efa03723-a815-4858-d9ac-fd7dbaa4049c"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive/\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#data preparation\n",
        "path_data_sexist_comments = \"/content/drive/MyDrive/COMP550 Final Project/ISEP Sexist Data labeling.xlsx\"\n",
        "path_data_amazon = \"/content/drive/MyDrive/COMP550 Final Project/AmazonMini.csv\"\n",
        "path_data_imdb = \"/content/drive/MyDrive/COMP550 Final Project/IMDB.csv\"\n",
        "\n",
        "#CHANGE to pd.read_csv FOR OTHER DATASETS :)) \n",
        "df_sexist = pd.read_excel(path_data_sexist_comments)\n",
        "df_amazon = pd.read_csv(path_data_amazon)\n",
        "df_imdb = pd.read_csv(path_data_imdb)\n",
        "\n",
        "#path to Glove\n",
        "path_glove = \"/content/drive/MyDrive/COMP550 Final Project/glove.6B.50d.txt\""
      ],
      "metadata": {
        "id": "tfJhoB1QB9U4"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Pre-processing of the sexist comments dataset"
      ],
      "metadata": {
        "id": "9jxW1jlzwQ2T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#pre-process the sexist comment dataset\n",
        "def lowerCase(StringArray):\n",
        "    lowerArray = []\n",
        "    for i in StringArray:\n",
        "        lowerArray.append(i.lower())\n",
        "    return lowerArray\n",
        "\n",
        "def depunctuate(StringArray):\n",
        "    exclude = set(string.punctuation)\n",
        "    depStringArray = []\n",
        "    for i in StringArray:\n",
        "        s = ''.join(ch for ch in i if ch not in exclude)\n",
        "        depStringArray.append(s)\n",
        "    return depStringArray\n",
        "\n",
        "data_sexist_sentences = df_sexist['Sentences'].to_numpy()\n",
        "\n",
        "#lowercase the sentences\n",
        "data_sexist_sentences = lowerCase(data_sexist_sentences)\n",
        "\n",
        "#depunctuate\n",
        "data_sexist_sentences = depunctuate(data_sexist_sentences)\n",
        "\n",
        "#load the final data\n",
        "df_sexist['Sentences'] = pd.Series(data_sexist_sentences)"
      ],
      "metadata": {
        "id": "cAXZX96ANXNR"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Bi-LSTM model\n",
        "class BiLSTM(nn.Module):\n",
        "    def __init__(self, args):\n",
        "        super(BiLSTM, self).__init__()\n",
        "\n",
        "        #size of input vectors, that'll be the embedding size\n",
        "        self.hidden_dim = args['hidden_dim']\n",
        "\n",
        "        #number of words in the vocabulary\n",
        "        self.vocab_size = args['vocab_size']\n",
        "\n",
        "        #number of classes you're predicting (usually binary)\n",
        "        self.output_dim = args['output_dim']\n",
        "\n",
        "        #input size is the size of each word/embedding\n",
        "        self.input_size = args['input_size']\n",
        "\n",
        "        #Dropout\n",
        "        self.drp = nn.Dropout(args['drp'])\n",
        "\n",
        "        #Embedding layer\n",
        "        self.embedding = nn.Embedding(self.vocab_size, self.input_size)\n",
        "        self.embedding.weight=nn.Parameter(torch.tensor(args['embedding_matrix'], dtype=torch.float32))\n",
        "        self.embedding.weight.requires_grad=args['requires_grad']\n",
        "\n",
        "        #Bi-LSTM\n",
        "        self.lstm = nn.LSTM(input_size=self.input_size,\n",
        "                            hidden_size=self.hidden_dim,\n",
        "                            dropout = args['drp'],\n",
        "                            num_layers=2,\n",
        "                            batch_first=True,\n",
        "                            bidirectional=True)\n",
        "        \n",
        "        self.linear = nn.Linear(self.hidden_dim*2, self.output_dim)\n",
        "\n",
        "        #Softmax activation function\n",
        "        self.act = nn.Sigmoid()\n",
        "\n",
        "    #Here x is the input tensor with shape: [batch_size, sequence_length]\n",
        "    #ie. number of training examples * number of words/training examples\n",
        "    def forward(self, x):\n",
        "        x = self.embedding(x)\n",
        "\n",
        "        output, (h_n, c_n) = self.lstm(x)\n",
        "\n",
        "        h_n = torch.cat((h_n[-2, :, :], h_n[-1, :, :]), dim=1)\n",
        "\n",
        "        x = self.linear(h_n)\n",
        "\n",
        "        x = self.act(x)\n",
        "\n",
        "        x = torch.squeeze(x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "4WaqLEJpLs-G"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class CustomDataset(Dataset):\n",
        "    #TO-DO: change path_to_excel to accept a full dataframe\n",
        "    def __init__(self, df, path_to_glove, embedding_dimension, max_nb_words=None, max_seq_length=None):\n",
        "        #here you could simply receive the dataset already with embedding matrix, index, and pre-processed text\n",
        "        self.samples = []\n",
        "        self.labels = []\n",
        "        self.df = df\n",
        "        self.path_to_glove = path_to_glove\n",
        "        self.embedding_dimension = embedding_dimension\n",
        "\n",
        "        #get in _init_dataset\n",
        "        self.length = None\n",
        "        self.embedding_matrix = None\n",
        "\n",
        "        #None by default\n",
        "        self.max_nb_words = max_nb_words\n",
        "        self.max_seq_length=max_seq_length\n",
        "\n",
        "        #initialize with method\n",
        "        self._init_dataset()\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.length\n",
        "\n",
        "    #for a\n",
        "    def __getitem__(self, idx):\n",
        "        return self.samples[idx], self.labels[idx]\n",
        "\n",
        "    #input np.array of sentences\n",
        "    def preprocessing(self, np_dataset):\n",
        "        return np_dataset\n",
        "\n",
        "    def create_embedding_matrix(self, word_index, embedding_dict):\n",
        "        embedding_m = np.zeros((len(word_index) + 1, self.embedding_dimension))\n",
        "        for word, index in word_index.items():\n",
        "            if word in embedding_dict:\n",
        "                embedding_m[index] = embedding_dict[word]\n",
        "        self.embedding_matrix = embedding_m\n",
        "\n",
        "    def _init_dataset(self):\n",
        "        #create the array of labels\n",
        "        y = pd.Series.to_numpy(self.df['Label'])\n",
        "        for i in y:\n",
        "            self.labels.append(i)\n",
        "\n",
        "        #returns rows have individual sentences, non-tokenized!\n",
        "        X = self.preprocessing(pd.Series.to_numpy(self.df['Sentences']))\n",
        "\n",
        "        #create the word embeddings\n",
        "        embed_dict = {}\n",
        "        with open(path_glove, 'r', encoding=\"utf8\") as file:\n",
        "            Lines = file.readlines()\n",
        "\n",
        "            for line in Lines:\n",
        "                values = line.split()\n",
        "                word = values[0]\n",
        "\n",
        "                try:\n",
        "                    vector = np.asarray(values[1:], 'float32')\n",
        "                except:\n",
        "                    pass\n",
        "                embed_dict[word] = vector\n",
        "\n",
        "        if self.max_seq_length == None:\n",
        "            self.max_seq_length = max(len(i.split()) for i in X)\n",
        "\n",
        "        self.length = X.shape[0]\n",
        "\n",
        "        #tokenize the next and put in array of integers, where each integer is mapped to a word\n",
        "        np.random.seed(7)\n",
        "        if self.max_nb_words != None:\n",
        "            tokenizer = Tokenizer(num_words=self.max_nb_words)\n",
        "        else:\n",
        "            tokenizer = Tokenizer(num_words=len(embed_dict.keys()))\n",
        "\n",
        "        #maybe move this into another method for efficiency's sake\n",
        "        tokenizer.fit_on_texts(X)\n",
        "        sequences = tokenizer.texts_to_sequences(X)\n",
        "        word_index = tokenizer.word_index\n",
        "        text = pad_sequences(sequences, maxlen=self.max_seq_length)\n",
        "        indices = np.arange(self.length)\n",
        "        text = text[indices]\n",
        "\n",
        "        for i in text:\n",
        "            self.samples.append(i)\n",
        "        self.create_embedding_matrix(word_index=word_index, embedding_dict=embed_dict)"
      ],
      "metadata": {
        "id": "wCdzpPVYLyWe"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#load into validation and training dataset\n",
        "#SEXIST\n",
        "dataset_sexist = CustomDataset(df= df_sexist, path_to_glove=path_glove, embedding_dimension=50)\n",
        "trainset_sexist, valset_sexist = random_split(dataset_sexist, [int(np.floor(len(dataset_sexist) * 0.8)), int(np.ceil(len(dataset_sexist) * 0.2))])\n",
        "\n",
        "train_loader_sexist = DataLoader(trainset_sexist, batch_size=18, shuffle=True, num_workers=2)\n",
        "val_loader_sexist = DataLoader(valset_sexist, batch_size=128, shuffle=True, num_workers=2)\n",
        "\n",
        "#AMAZON\n",
        "dataset_amazon = CustomDataset(df= df_amazon, path_to_glove=path_glove, embedding_dimension=50)\n",
        "trainset_amazon, valset_amazon = random_split(dataset_amazon, [int(np.floor(len(dataset_amazon) * 0.7)), int(np.ceil(len(dataset_amazon) * 0.3))])\n",
        "\n",
        "train_loader_amazon = DataLoader(trainset_amazon, batch_size=8, shuffle=True, num_workers=2)\n",
        "val_loader_amazon = DataLoader(valset_amazon, batch_size=128, shuffle=True, num_workers=2)\n",
        "\n",
        "#IMDB\n",
        "dataset_imdb = CustomDataset(df= df_imdb, path_to_glove=path_glove, embedding_dimension=50)\n",
        "trainset_imdb, valset_imdb = random_split(dataset_imdb, [int(np.floor(len(dataset_imdb) * 0.7)), int(np.ceil(len(dataset_imdb) * 0.3))])\n",
        "\n",
        "train_loader_imdb = DataLoader(trainset_imdb, batch_size=8, shuffle=True, num_workers=2)\n",
        "val_loader_imdb = DataLoader(valset_imdb, batch_size=128, shuffle=True, num_workers=2)"
      ],
      "metadata": {
        "id": "MHjJxNuZL6AI"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#fill the args dictionary with the required parameters\n",
        "args = {\n",
        "    'hidden_dim' : 32, #number of hidden dim\n",
        "    'vocab_size' : dataset_sexist.embedding_matrix.shape[0], #number of rows in word embedding matrix\n",
        "    'input_size' : 50,\n",
        "    'output_dim' : 1, #number of classes you're predicting\n",
        "    'embedding_matrix' : dataset_sexist.embedding_matrix, #the embedding matrix you've built in dataset constructor\n",
        "    'drp' : 0.2, #dropout layer for forward LSTM\n",
        "    'requires_grad': False, #for embedding matrix\n",
        "}"
      ],
      "metadata": {
        "id": "GQAURpNyL8tt"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here we train the BiLSTM model with 5 epochs. We will do this ten times and then average the model's performance on the validation dataset with every new batch of batch_size points that it sees."
      ],
      "metadata": {
        "id": "bhwWRTkOi74-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "results_vanilla = {\n",
        "    'accuracy_training': [],\n",
        "    'loss': [],\n",
        "    'accuracy_validation': []\n",
        "}\n",
        "\n",
        "for t in range(10):\n",
        "    model = BiLSTM(args)\n",
        "    criterion = nn.BCELoss()\n",
        "    optimizer = optim.Adam(model.parameters())\n",
        "    training_score = []\n",
        "    loss_score = []\n",
        "    validation_score = []\n",
        "    print(t)\n",
        "    for e in range(5):\n",
        "        for i, data in enumerate(train_loader_sexist):\n",
        "            model.train()\n",
        "\n",
        "            running_loss = 0\n",
        "            # get the inputs; data is a list of [inputs, labels]\n",
        "            inputs, labels = data\n",
        "\n",
        "            # zero the parameter gradients\n",
        "            optim.Adam(model.parameters(), lr=0.01).zero_grad()\n",
        "\n",
        "            # forward + backward + optimize\n",
        "            outputs = model(inputs)\n",
        "            #Get the loss on the batch\n",
        "            targets = labels.to(torch.float32)\n",
        "            outputs = outputs.to(torch.float32)\n",
        "            \n",
        "            loss = criterion(outputs, targets)\n",
        "\n",
        "            #gets the gradient on the batch for each parameter\n",
        "            loss.backward()\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), 0.1)\n",
        "\n",
        "            #take size lr step in gradient direction\n",
        "            optimizer.step()\n",
        "            total_acc = 0\n",
        "            total_count = 0\n",
        "            total_acc += (torch.round(outputs) == labels).sum().item()\n",
        "            total_count += labels.size(0)\n",
        "\n",
        "            # print statistics\n",
        "            running_loss += loss.item()\n",
        "            \n",
        "            #validation\n",
        "            model.eval()\n",
        "\n",
        "            #set metrics to 0\n",
        "            total_accuracy_val = 0\n",
        "            total_count_val = 0\n",
        "\n",
        "            for i_val, data_val in enumerate(val_loader_sexist):\n",
        "                inputs_val, labels_val = data_val\n",
        "\n",
        "                # forward + backward + optimize\n",
        "                outputs_val = model(inputs_val)\n",
        "\n",
        "                targets_val = labels_val.to(torch.float32)\n",
        "                outputs_val = outputs_val.to(torch.float32)\n",
        "\n",
        "                total_accuracy_val += (torch.round(outputs_val) == labels_val).sum().item()\n",
        "                total_count_val += labels_val.size(0)\n",
        "            \n",
        "            training_score.append(total_acc/total_count)\n",
        "            validation_score.append(total_accuracy_val/total_count_val)\n",
        "            loss_score.append(running_loss/total_count)\n",
        "\n",
        "            #print(f'acc_train: {total_acc/total_count} \\| acc_val: {total_accuracy_val/total_count_val} \\| avg_loss: {running_loss/total_count}')\n",
        "            \n",
        "            #set the metrics to 0\n",
        "            running_loss = 0\n",
        "            total_acc, total_count = 0, 0\n",
        "    \n",
        "    results_vanilla['accuracy_training'].append(training_score)\n",
        "    results_vanilla['accuracy_validation'].append(validation_score)\n",
        "    results_vanilla['loss'].append(loss_score)"
      ],
      "metadata": {
        "id": "vKIMHNSVbUQq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6aca7029-94d0-491c-bcbc-a9d9c4077df7"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "6\n",
            "7\n",
            "8\n",
            "9\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "results_avg = {\n",
        "    'accuracy_training': [],\n",
        "    'loss': [],\n",
        "    'accuracy_validation': []\n",
        "}\n",
        "#get the averages during the training phases\n",
        "for k,v in results_vanilla.items():\n",
        "    score_raw = results_vanilla[k]\n",
        "    score_avg = []\n",
        "    for r in range(0, len(score_raw[0])):\n",
        "        avg = 0\n",
        "        for i in range(0, len(score_raw)):\n",
        "            avg += score_raw[i][r]\n",
        "        score_avg.append(avg)\n",
        "    results_avg[k] = score_avg\n",
        "print(results_avg)\n",
        "print(results_vanilla)"
      ],
      "metadata": {
        "id": "rhkVWc5dTuDM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6a5a4785-eb72-4553-ffcb-24a20cd233da"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'accuracy_training': [4.944444444444445, 5.111111111111111, 5.444444444444445, 5.388888888888888, 5.944444444444445, 5.277777777777778, 5.333333333333333, 6.055555555555556, 5.444444444444445, 5.277777777777778, 5.722222222222223, 5.500000000000001, 5.333333333333332, 5.555555555555555, 5.388888888888889, 5.444444444444444, 5.166666666666667, 5.222222222222222, 5.611111111111111, 5.888888888888888, 5.8888888888888875, 5.111111111111112, 6.111111111111111, 5.3888888888888875, 5.666666666666666, 5.611111111111111, 5.388888888888889, 5.222222222222222, 5.888888888888888, 5.666666666666667, 5.111111111111112, 6.333333333333334, 4.944444444444445, 6.555555555555555, 5.444444444444444, 6.333333333333333, 5.944444444444445, 6.222222222222222, 6.333333333333334, 6.166666666666667, 6.777777777777779, 5.722222222222222, 6.555555555555556, 6.444444444444445, 6.333333333333332, 7.166666666666668, 6.944444444444444, 7.111111111111111, 6.388888888888889, 7.000000000000001, 7.0, 7.333333333333333, 7.166666666666667, 7.166666666666668, 7.666666666666666, 7.0, 7.333333333333333, 8.0, 7.4444444444444455, 7.277777777777778, 8.055555555555555, 7.722222222222223, 7.0, 7.166666666666667, 7.611111111111111, 7.111111111111112, 6.722222222222223, 7.944444444444445, 7.666666666666667, 7.5, 6.833333333333334, 6.722222222222222, 7.222222222222222, 7.5, 7.888888888888889, 7.333333333333333, 7.444444444444444, 7.777777777777778, 8.055555555555554, 7.722222222222223, 7.555555555555555, 7.5555555555555545, 7.944444444444444, 7.722222222222222, 7.666666666666666, 7.777777777777777, 7.722222222222222, 7.722222222222222, 7.388888888888889, 7.499999999999999, 7.499999999999999, 7.777777777777778, 7.444444444444445, 8.166666666666666, 7.722222222222221, 7.61111111111111, 7.722222222222222, 7.666666666666667, 7.333333333333333, 7.666666666666666, 7.388888888888888, 8.333333333333334, 8.333333333333332, 7.777777777777778, 7.999999999999999, 7.833333333333334, 8.055555555555555, 8.444444444444445, 7.666666666666667, 7.722222222222223, 8.277777777777777, 7.833333333333333, 7.777777777777778, 8.277777777777777, 7.722222222222221, 8.333333333333334, 7.388888888888888, 8.222222222222221, 8.055555555555555, 7.944444444444445, 7.722222222222222, 7.166666666666666, 8.055555555555555, 8.0, 8.0, 7.8888888888888875, 7.944444444444445, 7.722222222222223, 7.166666666666668, 8.222222222222223, 8.333333333333334, 8.277777777777777, 8.0, 8.38888888888889, 8.055555555555555, 8.222222222222221, 7.888888888888889, 7.944444444444445, 8.111111111111112, 7.944444444444444, 7.5, 7.944444444444445, 7.888888888888888, 7.833333333333332, 8.166666666666668, 8.11111111111111, 8.555555555555557, 8.277777777777779, 7.833333333333333, 7.888888888888888, 7.999999999999999, 7.611111111111111, 8.0, 8.0, 7.944444444444445, 8.61111111111111, 8.222222222222223, 8.61111111111111, 7.777777777777778, 8.166666666666666, 8.0, 7.555555555555556, 8.222222222222221, 8.333333333333332, 8.388888888888888, 8.222222222222221, 7.999999999999999, 8.0, 8.222222222222223, 8.444444444444445, 8.38888888888889, 8.388888888888888, 7.666666666666666, 8.333333333333332, 7.722222222222221, 8.444444444444445, 8.277777777777779, 7.61111111111111, 7.944444444444445, 7.555555555555555, 8.333333333333334, 8.222222222222223, 8.388888888888888, 7.833333333333333, 8.277777777777777, 8.277777777777779, 8.166666666666668, 7.944444444444444, 8.333333333333332, 8.222222222222221, 8.11111111111111, 8.333333333333334, 8.611111111111112, 8.333333333333332, 7.777777777777778, 8.38888888888889, 7.944444444444444, 8.166666666666666, 8.111111111111112, 8.0, 8.5, 8.166666666666666, 7.999999999999999, 8.555555555555555, 7.8888888888888875, 8.500000000000002, 8.944444444444446, 8.61111111111111, 8.0, 8.222222222222223, 8.666666666666668, 8.666666666666666, 8.666666666666668, 7.722222222222221, 8.277777777777779, 8.166666666666666, 8.333333333333334, 8.500000000000002, 8.333333333333334, 8.0, 8.222222222222223, 8.5, 8.277777777777777, 8.055555555555557, 8.444444444444446, 7.944444444444445, 9.000000000000002, 8.722222222222221, 8.611111111111109, 9.0, 8.0, 8.055555555555555, 8.38888888888889, 8.666666666666666, 8.388888888888888, 8.11111111111111, 8.611111111111112, 8.11111111111111, 8.611111111111112, 7.833333333333334, 8.333333333333334, 8.055555555555555, 8.38888888888889, 8.666666666666666, 8.444444444444445, 7.722222222222221, 8.166666666666666, 8.333333333333332, 8.444444444444445, 8.5, 8.11111111111111, 8.5, 8.055555555555555, 7.888888888888888, 8.0], 'loss': [0.38544935319158763, 0.3850610819127825, 0.3803444570965237, 0.38108712434768677, 0.37804074419869316, 0.3857549495167203, 0.3814214732911852, 0.38111381729443866, 0.38310668534702724, 0.3839759296841092, 0.3807746403747134, 0.3812741206751929, 0.38492665357059896, 0.3795329034328461, 0.38315829634666443, 0.3810362981425391, 0.3873586687776778, 0.38520998093816966, 0.3790704839759403, 0.3777894344594744, 0.377843482626809, 0.38576064838303464, 0.3740728398164114, 0.3814303278923034, 0.3753972517119513, 0.3778551121552785, 0.3775414327780405, 0.3806861009862688, 0.3708267377482521, 0.37265710367096794, 0.3809718191623688, 0.367940796746148, 0.38450378841824, 0.365524043639501, 0.3767645557721456, 0.36280761493576896, 0.3704701926973131, 0.36791442169083494, 0.3690972692436642, 0.36757199300660026, 0.3628777729140388, 0.37516183985604185, 0.3556942244370778, 0.36303045021163094, 0.36286347442203093, 0.35375133819050264, 0.3539561794863807, 0.3433447976907094, 0.353413850069046, 0.34945488307211137, 0.6768958701027764, 0.33706345160802204, 0.33287364906734884, 0.3273725377188789, 0.3182679563760757, 0.3301127172178692, 0.32834120591481525, 0.3061261673768361, 0.30836522579193115, 0.31353100306457937, 0.28604277471701306, 0.3016870932446586, 0.3312853541639116, 0.31794757809903884, 0.30092382431030273, 0.30484062598811257, 0.3379722138245901, 0.27855855392085177, 0.2816445214880837, 0.2965421577294668, 0.3269769002993902, 0.3493814302815331, 0.31921433077918154, 0.29537573787901134, 0.285874358481831, 0.30717780689398444, 0.30489404168393874, 0.2884535077545378, 0.272411584854126, 0.2806997332308028, 0.2990107983350754, 0.2811163448625141, 0.2750685281223721, 0.27474096914132434, 0.30022704270150924, 0.281241923570633, 0.27219417856799233, 0.28556228015157914, 0.3032145980331633, 0.2998964885870616, 0.2922324223650826, 0.2728583779599932, 0.29108820524480605, 0.2670958621634377, 0.2734231303135554, 0.2909129858016968, 0.289799764752388, 0.2791443318128586, 0.31873468061288196, 0.2818003015385734, 0.2945742971367306, 0.44014308849970496, 0.2432228409581714, 0.2818145751953125, 0.2615001185072793, 0.2616827968094084, 0.2838078008757697, 0.22909647391902077, 0.2772057387563917, 0.2860119260019726, 0.239918385942777, 0.2773560318681929, 0.26978277166684467, 0.24453547431363, 0.2775789466169145, 0.25196848313013714, 0.30964118573400706, 0.24199524190690783, 0.26734112699826557, 0.2509579592280918, 0.27405662337938946, 0.3219026318854756, 0.2559034509791268, 0.2624205665455924, 0.25660908387766945, 0.2720190270079507, 0.25982336865531075, 0.2770138531923294, 0.31197892791695064, 0.24560807148615518, 0.24081639614370132, 0.2260506335231993, 0.26015139288372463, 0.23631661550866234, 0.2558543797996309, 0.24021092553933462, 0.2655111783080631, 0.2615829822089937, 0.2695353544420666, 0.2626553359958861, 0.2942917098601659, 0.24289995266331568, 0.2650490717755424, 0.26684631986750496, 0.24200257162253067, 0.2550253851546182, 0.21565657274590597, 0.23388581971327463, 0.24884595142470467, 0.27271633346875507, 0.2740958448913362, 0.2668623642788993, 0.5001915643612544, 0.249480578634474, 0.2662238958809111, 0.22203581697411007, 0.2395892126692666, 0.22721243318584233, 0.2623842822180854, 0.23361354072888693, 0.26193685829639435, 0.30417156053913963, 0.2377996858623293, 0.24225949992736176, 0.22153853707843357, 0.2438079300853941, 0.22869212428728738, 0.2596428559886085, 0.26112595200538635, 0.23762813044918907, 0.24584889908631644, 0.22653184831142426, 0.2782043632533815, 0.21661389867464703, 0.27154790196153855, 0.2277498965462049, 0.21532141831186083, 0.2833470834626092, 0.23265528347757122, 0.287840829955207, 0.22251319388548535, 0.2354247826668951, 0.2309785948859321, 0.2748425039980147, 0.24267390701505873, 0.24313263263967302, 0.25288771258460147, 0.2527039382192824, 0.23460819489426082, 0.22173552877373168, 0.2395052810509999, 0.22979341116216448, 0.23456542276673845, 0.22213053620523876, 0.26548853351010215, 0.23888464603159162, 0.26503658956951565, 0.2581169456243515, 0.26957055595186025, 0.25027893483638763, 0.21259536345799762, 0.2388189550903108, 0.2549469429585669, 0.45953191651238334, 0.29129786127143437, 0.22613969610797036, 0.18137789103719923, 0.2044353841079606, 0.25311463657352656, 0.2373272313012017, 0.21121006210645038, 0.20034589204523298, 0.19696358425749672, 0.26886591811974847, 0.235845610499382, 0.24196535680029124, 0.2332888154519929, 0.21977044807540044, 0.2370161215464274, 0.2670810148119926, 0.2388455006811354, 0.2143133853872617, 0.2287892583343718, 0.24588505095905727, 0.2301732748746872, 0.24436141053835547, 0.16459081404738957, 0.18838054604000515, 0.20463531629906764, 0.18959411564800477, 0.26729107979271144, 0.25675520632002086, 0.22735409852531224, 0.18884986390670142, 0.22132617731889087, 0.2509605801767773, 0.21952096703979707, 0.23478694839609993, 0.2136388768752416, 0.2658404095305337, 0.21895293560292986, 0.25221355590555405, 0.23819497889942595, 0.21100436151027677, 0.23335150629281998, 0.3040774746073617, 0.2622454712788264, 0.21111346532901132, 0.21176921990182665, 0.21593136423163942, 0.23852413147687915, 0.20478299425707924, 0.2616007857852512, 0.28675440781646305, 0.5214145001437929], 'accuracy_validation': [5.074561403508772, 5.078947368421052, 5.307017543859649, 5.451754385964913, 5.407894736842105, 5.421052631578947, 5.447368421052632, 5.456140350877193, 5.482456140350877, 5.5438596491228065, 5.574561403508771, 5.609649122807017, 5.6535087719298245, 5.6798245614035086, 5.609649122807017, 5.614035087719298, 5.570175438596491, 5.605263157894736, 5.640350877192982, 5.640350877192982, 5.62719298245614, 5.600877192982456, 5.5701754385964914, 5.56140350877193, 5.557017543859649, 5.548245614035088, 5.5701754385964914, 5.583333333333333, 5.614035087719298, 5.671052631578947, 5.745614035087719, 5.824561403508771, 5.903508771929824, 5.969298245614034, 6.083333333333333, 6.175438596491228, 6.267543859649122, 6.298245614035087, 6.342105263157894, 6.425438596491228, 6.478070175438596, 6.535087719298245, 6.6798245614035086, 6.7938596491228065, 6.978070175438597, 7.074561403508772, 7.135964912280701, 7.140350877192982, 7.2061403508771935, 7.258771929824562, 7.333333333333333, 7.394736842105262, 7.403508771929824, 7.429824561403509, 7.399122807017544, 7.478070175438598, 7.469298245614036, 7.381578947368421, 7.364035087719298, 7.302631578947367, 7.293859649122807, 7.267543859649122, 7.219298245614035, 7.267543859649122, 7.36842105263158, 7.38157894736842, 7.421052631578948, 7.517543859649123, 7.451754385964913, 7.478070175438597, 7.5, 7.49561403508772, 7.469298245614034, 7.517543859649122, 7.548245614035088, 7.592105263157895, 7.671052631578948, 7.68859649122807, 7.662280701754387, 7.6798245614035086, 7.671052631578947, 7.675438596491228, 7.701754385964913, 7.732456140350878, 7.771929824561404, 7.793859649122807, 7.811403508771931, 7.828947368421053, 7.798245614035087, 7.780701754385966, 7.807017543859649, 7.81140350877193, 7.828947368421053, 7.828947368421052, 7.824561403508771, 7.81140350877193, 7.710526315789475, 7.684210526315789, 7.671052631578949, 7.697368421052631, 7.767543859649123, 7.81140350877193, 7.8728070175438605, 7.833333333333334, 7.868421052631579, 7.864035087719299, 7.842105263157896, 7.842105263157896, 7.912280701754386, 7.9605263157894735, 8.008771929824562, 7.942982456140351, 7.942982456140351, 7.969298245614036, 7.942982456140352, 7.973684210526315, 7.9868421052631575, 7.947368421052633, 7.947368421052633, 7.964912280701753, 7.9868421052631575, 8.000000000000002, 8.021929824561404, 8.030701754385966, 7.991228070175439, 8.013157894736842, 7.978070175438597, 7.9561403508771935, 7.925438596491229, 7.969298245614034, 8.0, 7.9912280701754375, 8.013157894736842, 8.039473684210526, 8.030701754385964, 8.048245614035087, 8.035087719298245, 8.078947368421053, 8.078947368421053, 8.078947368421053, 8.118421052631579, 8.144736842105264, 8.18859649122807, 8.210526315789474, 8.18859649122807, 8.109649122807017, 8.092105263157896, 8.039473684210527, 8.039473684210526, 8.017543859649123, 8.057017543859649, 8.078947368421051, 8.100877192982455, 8.149122807017543, 8.18859649122807, 8.197368421052634, 8.157894736842106, 8.162280701754385, 8.083333333333334, 8.06578947368421, 8.118421052631579, 8.179824561403509, 8.223684210526317, 8.25, 8.228070175438596, 8.25, 8.228070175438596, 8.210526315789476, 8.201754385964913, 8.157894736842106, 8.135964912280702, 8.087719298245615, 8.074561403508772, 8.096491228070175, 8.12280701754386, 8.135964912280702, 8.140350877192983, 8.171052631578949, 8.201754385964913, 8.192982456140351, 8.223684210526317, 8.241228070175438, 8.192982456140351, 8.131578947368421, 8.105263157894736, 8.122807017543861, 8.135964912280702, 8.149122807017545, 8.135964912280702, 8.17982456140351, 8.179824561403509, 8.24561403508772, 8.232456140350878, 8.197368421052634, 8.184210526315791, 8.140350877192983, 8.140350877192983, 8.109649122807017, 8.12280701754386, 8.127192982456142, 8.135964912280702, 8.162280701754385, 8.219298245614034, 8.228070175438596, 8.24561403508772, 8.223684210526317, 8.223684210526317, 8.214912280701755, 8.192982456140351, 8.192982456140351, 8.210526315789474, 8.192982456140351, 8.201754385964913, 8.219298245614036, 8.232456140350878, 8.30263157894737, 8.26315789473684, 8.25438596491228, 8.232456140350877, 8.25, 8.25438596491228, 8.263157894736842, 8.24561403508772, 8.241228070175438, 8.25, 8.285087719298247, 8.25, 8.267543859649123, 8.267543859649123, 8.236842105263158, 8.241228070175438, 8.25438596491228, 8.219298245614036, 8.236842105263158, 8.214912280701755, 8.206140350877194, 8.271929824561402, 8.280701754385966, 8.25877192982456, 8.263157894736842, 8.267543859649123, 8.232456140350877, 8.197368421052632, 8.201754385964913, 8.263157894736844, 8.307017543859649, 8.298245614035087, 8.298245614035089, 8.23684210526316, 8.267543859649123, 8.25438596491228, 8.25438596491228, 8.24561403508772, 8.25438596491228, 8.271929824561402]}\n",
            "{'accuracy_training': [[0.4444444444444444, 0.5, 0.5, 0.2777777777777778, 0.4444444444444444, 0.4444444444444444, 0.5, 0.6111111111111112, 0.5555555555555556, 0.3333333333333333, 0.7222222222222222, 0.7222222222222222, 0.5555555555555556, 0.4444444444444444, 0.7777777777777778, 0.6666666666666666, 0.3888888888888889, 0.6111111111111112, 0.3333333333333333, 0.4444444444444444, 0.6666666666666666, 0.4444444444444444, 0.5555555555555556, 0.4444444444444444, 0.7777777777777778, 0.7222222222222222, 0.5, 0.4444444444444444, 0.5555555555555556, 0.6111111111111112, 0.5555555555555556, 0.6666666666666666, 0.4444444444444444, 0.7222222222222222, 0.5555555555555556, 0.6111111111111112, 0.3888888888888889, 0.5555555555555556, 0.5, 0.4444444444444444, 0.8333333333333334, 0.5, 0.5555555555555556, 0.5, 0.7222222222222222, 0.7222222222222222, 0.6111111111111112, 0.7777777777777778, 0.6666666666666666, 0.7777777777777778, 0.5555555555555556, 0.5555555555555556, 0.7222222222222222, 0.7777777777777778, 0.7777777777777778, 0.6666666666666666, 0.6111111111111112, 0.8333333333333334, 0.7777777777777778, 0.7777777777777778, 0.6111111111111112, 0.7777777777777778, 0.5, 0.7222222222222222, 0.5555555555555556, 0.8888888888888888, 0.7222222222222222, 0.6111111111111112, 0.8333333333333334, 0.8333333333333334, 0.7777777777777778, 0.7222222222222222, 0.8888888888888888, 0.8333333333333334, 0.8888888888888888, 0.7222222222222222, 0.6111111111111112, 0.8333333333333334, 0.8888888888888888, 0.7222222222222222, 0.9444444444444444, 0.7777777777777778, 0.8333333333333334, 0.7222222222222222, 0.7777777777777778, 0.8333333333333334, 0.8333333333333334, 0.7222222222222222, 0.7222222222222222, 0.8888888888888888, 0.7222222222222222, 0.7777777777777778, 0.7777777777777778, 0.7222222222222222, 0.6666666666666666, 0.6666666666666666, 0.7777777777777778, 0.8333333333333334, 0.9444444444444444, 0.6666666666666666, 0.7222222222222222, 0.7777777777777778, 1.0, 0.7777777777777778, 1.0, 0.8333333333333334, 0.7777777777777778, 0.8888888888888888, 0.8333333333333334, 0.8333333333333334, 0.7777777777777778, 0.7777777777777778, 0.8888888888888888, 0.8888888888888888, 0.7777777777777778, 0.9444444444444444, 0.6666666666666666, 0.9444444444444444, 0.7777777777777778, 0.9444444444444444, 0.9444444444444444, 0.7777777777777778, 0.8333333333333334, 0.8333333333333334, 0.7222222222222222, 0.5555555555555556, 0.7222222222222222, 0.7222222222222222, 0.6111111111111112, 0.8888888888888888, 0.7777777777777778, 0.7222222222222222, 0.7222222222222222, 0.9444444444444444, 0.7777777777777778, 0.9444444444444444, 0.8333333333333334, 0.7777777777777778, 0.8333333333333334, 0.7777777777777778, 0.8888888888888888, 0.8888888888888888, 0.6111111111111112, 0.8888888888888888, 0.9444444444444444, 0.8333333333333334, 0.9444444444444444, 0.7777777777777778, 0.8333333333333334, 0.7777777777777778, 0.6666666666666666, 0.6666666666666666, 0.6666666666666666, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.8888888888888888, 0.8888888888888888, 0.5555555555555556, 0.8888888888888888, 0.8888888888888888, 0.7777777777777778, 0.8888888888888888, 0.8888888888888888, 0.8888888888888888, 0.8888888888888888, 0.8333333333333334, 0.8333333333333334, 0.7777777777777778, 0.8888888888888888, 0.8888888888888888, 0.7222222222222222, 0.9444444444444444, 0.8888888888888888, 0.8333333333333334, 0.7777777777777778, 0.8333333333333334, 0.7777777777777778, 0.9444444444444444, 0.7777777777777778, 0.8888888888888888, 0.7777777777777778, 0.8888888888888888, 0.8333333333333334, 0.7777777777777778, 0.9444444444444444, 0.8333333333333334, 0.6666666666666666, 0.8888888888888888, 0.8333333333333334, 0.9444444444444444, 0.7777777777777778, 0.8333333333333334, 0.7222222222222222, 0.6666666666666666, 0.9444444444444444, 0.7222222222222222, 0.8888888888888888, 0.7777777777777778, 0.8888888888888888, 0.9444444444444444, 0.8333333333333334, 0.9444444444444444, 0.8888888888888888, 0.8333333333333334, 0.7222222222222222, 0.9444444444444444, 0.7777777777777778, 0.8333333333333334, 0.9444444444444444, 0.9444444444444444, 0.8333333333333334, 0.8333333333333334, 0.7777777777777778, 0.6666666666666666, 0.9444444444444444, 0.7222222222222222, 0.7222222222222222, 0.9444444444444444, 0.8333333333333334, 0.8333333333333334, 0.9444444444444444, 0.8333333333333334, 0.8333333333333334, 0.7777777777777778, 0.7777777777777778, 0.8333333333333334, 0.8888888888888888, 0.7777777777777778, 0.8333333333333334, 1.0, 0.8888888888888888, 0.7222222222222222, 0.9444444444444444, 0.7777777777777778, 0.9444444444444444, 0.8333333333333334, 0.9444444444444444, 0.8333333333333334, 0.7777777777777778, 0.7777777777777778, 0.8333333333333334, 0.6666666666666666, 0.8333333333333334, 0.8888888888888888, 0.6666666666666666, 0.7777777777777778, 0.8333333333333334, 0.8888888888888888, 0.7777777777777778, 0.9444444444444444, 0.7777777777777778, 0.8888888888888888, 0.7222222222222222, 0.7777777777777778], [0.7222222222222222, 0.3888888888888889, 0.6666666666666666, 0.6111111111111112, 0.7222222222222222, 0.3888888888888889, 0.6666666666666666, 0.5, 0.5, 0.3888888888888889, 0.6666666666666666, 0.6111111111111112, 0.3888888888888889, 0.7222222222222222, 0.3333333333333333, 0.5, 0.2777777777777778, 0.5, 0.3888888888888889, 0.3333333333333333, 0.4444444444444444, 0.6111111111111112, 0.6666666666666666, 0.7222222222222222, 0.7222222222222222, 0.7222222222222222, 0.4444444444444444, 0.5, 0.6111111111111112, 0.5555555555555556, 0.3333333333333333, 0.5555555555555556, 0.5, 0.5555555555555556, 0.3333333333333333, 0.6111111111111112, 0.6666666666666666, 0.5555555555555556, 0.8888888888888888, 0.6111111111111112, 0.7777777777777778, 0.4444444444444444, 0.4444444444444444, 0.5555555555555556, 0.5555555555555556, 0.7222222222222222, 0.6111111111111112, 0.4444444444444444, 0.5555555555555556, 0.5555555555555556, 0.8888888888888888, 0.6111111111111112, 0.7222222222222222, 0.7222222222222222, 0.8888888888888888, 0.7777777777777778, 0.7777777777777778, 0.7777777777777778, 0.7777777777777778, 0.7222222222222222, 0.7222222222222222, 0.8333333333333334, 0.7222222222222222, 0.8333333333333334, 0.8888888888888888, 0.4444444444444444, 0.5, 0.8888888888888888, 0.5, 0.5, 0.7777777777777778, 0.7777777777777778, 0.6111111111111112, 0.7777777777777778, 0.8888888888888888, 0.7777777777777778, 0.8888888888888888, 0.8888888888888888, 0.7777777777777778, 0.8333333333333334, 0.7222222222222222, 0.7222222222222222, 0.8888888888888888, 0.8888888888888888, 0.6666666666666666, 0.5555555555555556, 0.8333333333333334, 0.7222222222222222, 0.6111111111111112, 0.7222222222222222, 0.7222222222222222, 0.8333333333333334, 0.5555555555555556, 0.9444444444444444, 0.7777777777777778, 0.6666666666666666, 0.7777777777777778, 0.7777777777777778, 0.8333333333333334, 0.8333333333333334, 0.5555555555555556, 0.7777777777777778, 0.6111111111111112, 0.6666666666666666, 0.7777777777777778, 0.6666666666666666, 0.7777777777777778, 0.8333333333333334, 0.8888888888888888, 0.7777777777777778, 0.8333333333333334, 0.7777777777777778, 0.7222222222222222, 0.5, 0.8333333333333334, 0.7777777777777778, 0.7777777777777778, 0.8888888888888888, 0.8333333333333334, 0.7222222222222222, 0.8333333333333334, 0.8333333333333334, 0.7777777777777778, 0.7222222222222222, 0.7222222222222222, 0.8888888888888888, 0.5, 0.6111111111111112, 0.8888888888888888, 0.8333333333333334, 0.9444444444444444, 0.8333333333333334, 0.7222222222222222, 0.7777777777777778, 0.8888888888888888, 0.8888888888888888, 0.7777777777777778, 0.7777777777777778, 0.9444444444444444, 0.7777777777777778, 0.7222222222222222, 0.7222222222222222, 0.8333333333333334, 0.9444444444444444, 0.8333333333333334, 0.7777777777777778, 0.8888888888888888, 0.8888888888888888, 0.8333333333333334, 0.8888888888888888, 0.8888888888888888, 0.8888888888888888, 0.7777777777777778, 0.8888888888888888, 0.7222222222222222, 0.8888888888888888, 0.8333333333333334, 0.9444444444444444, 0.6666666666666666, 0.7222222222222222, 0.6666666666666666, 0.6666666666666666, 0.7777777777777778, 0.6666666666666666, 0.7777777777777778, 0.7222222222222222, 0.6666666666666666, 0.7222222222222222, 0.7222222222222222, 0.8888888888888888, 0.8888888888888888, 0.7777777777777778, 0.8333333333333334, 0.8888888888888888, 0.7777777777777778, 0.8888888888888888, 0.6666666666666666, 0.6111111111111112, 0.7777777777777778, 0.6111111111111112, 0.8333333333333334, 0.8333333333333334, 0.8888888888888888, 0.8333333333333334, 0.8888888888888888, 0.7777777777777778, 0.7777777777777778, 0.7222222222222222, 0.7222222222222222, 0.9444444444444444, 0.7222222222222222, 0.6111111111111112, 0.7222222222222222, 0.7222222222222222, 0.7777777777777778, 0.8888888888888888, 0.6666666666666666, 0.8888888888888888, 0.8888888888888888, 0.5555555555555556, 0.8333333333333334, 0.7777777777777778, 0.7222222222222222, 0.7777777777777778, 0.6666666666666666, 0.7777777777777778, 0.7777777777777778, 0.8888888888888888, 0.8333333333333334, 0.7777777777777778, 0.8888888888888888, 0.8888888888888888, 0.8333333333333334, 0.6666666666666666, 0.8888888888888888, 0.6666666666666666, 0.8888888888888888, 0.9444444444444444, 0.8888888888888888, 0.7777777777777778, 0.9444444444444444, 0.8888888888888888, 0.7777777777777778, 0.8333333333333334, 0.7222222222222222, 0.8888888888888888, 0.9444444444444444, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.7222222222222222, 0.8333333333333334, 0.7777777777777778, 0.7222222222222222, 0.8333333333333334, 0.7222222222222222, 0.9444444444444444, 0.7222222222222222, 0.8888888888888888, 0.7777777777777778, 0.7777777777777778, 0.8888888888888888, 0.8333333333333334, 0.9444444444444444, 0.8888888888888888, 0.7777777777777778, 0.8888888888888888, 0.8333333333333334, 0.8333333333333334, 1.0, 0.6666666666666666, 0.7222222222222222, 0.9444444444444444, 0.7777777777777778, 0.7777777777777778], [0.4444444444444444, 0.5, 0.5, 0.7222222222222222, 0.5, 0.6111111111111112, 0.3333333333333333, 0.6111111111111112, 0.6111111111111112, 0.7222222222222222, 0.4444444444444444, 0.5555555555555556, 0.6111111111111112, 0.6666666666666666, 0.5, 0.5555555555555556, 0.6666666666666666, 0.3888888888888889, 0.7222222222222222, 0.5, 0.4444444444444444, 0.4444444444444444, 0.7222222222222222, 0.3333333333333333, 0.5555555555555556, 0.3333333333333333, 0.5, 0.5, 0.7777777777777778, 0.6666666666666666, 0.5555555555555556, 0.8333333333333334, 0.6666666666666666, 0.6111111111111112, 0.7777777777777778, 0.6111111111111112, 0.6666666666666666, 0.6111111111111112, 0.6666666666666666, 0.7777777777777778, 0.7222222222222222, 0.6666666666666666, 0.8333333333333334, 0.7777777777777778, 0.7222222222222222, 0.7222222222222222, 0.6666666666666666, 0.8888888888888888, 0.7222222222222222, 0.8888888888888888, 0.7777777777777778, 0.7222222222222222, 0.5555555555555556, 0.6111111111111112, 0.7222222222222222, 0.8333333333333334, 0.7777777777777778, 0.7222222222222222, 0.8888888888888888, 0.3888888888888889, 0.9444444444444444, 0.5, 0.6111111111111112, 0.7777777777777778, 0.7777777777777778, 0.7222222222222222, 0.6666666666666666, 0.8888888888888888, 0.8888888888888888, 0.6111111111111112, 0.7777777777777778, 0.5, 0.8333333333333334, 0.7777777777777778, 0.7777777777777778, 0.6111111111111112, 0.7777777777777778, 0.6111111111111112, 0.7777777777777778, 0.8333333333333334, 0.7222222222222222, 0.7777777777777778, 0.7777777777777778, 0.8888888888888888, 0.7222222222222222, 0.7222222222222222, 0.7222222222222222, 0.7777777777777778, 0.9444444444444444, 0.7777777777777778, 0.8333333333333334, 0.7777777777777778, 0.8333333333333334, 0.9444444444444444, 0.8888888888888888, 0.8333333333333334, 0.8888888888888888, 0.7777777777777778, 0.7777777777777778, 0.6111111111111112, 0.7777777777777778, 0.7777777777777778, 0.9444444444444444, 0.6666666666666666, 0.6111111111111112, 0.7222222222222222, 0.7777777777777778, 0.8888888888888888, 0.9444444444444444, 0.8888888888888888, 0.8888888888888888, 0.7222222222222222, 0.7222222222222222, 0.8888888888888888, 0.8333333333333334, 0.8888888888888888, 0.6111111111111112, 0.7777777777777778, 0.8333333333333334, 0.8333333333333334, 0.6666666666666666, 0.6111111111111112, 0.8888888888888888, 0.8333333333333334, 0.8333333333333334, 0.7222222222222222, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.7777777777777778, 0.8333333333333334, 0.7777777777777778, 0.7222222222222222, 0.8333333333333334, 0.8888888888888888, 0.9444444444444444, 0.8333333333333334, 0.6666666666666666, 0.6111111111111112, 0.8888888888888888, 0.8888888888888888, 0.7777777777777778, 0.7777777777777778, 0.5, 0.7222222222222222, 0.9444444444444444, 0.8333333333333334, 0.8333333333333334, 1.0, 0.6111111111111112, 0.8333333333333334, 0.6666666666666666, 0.6666666666666666, 0.7222222222222222, 0.8888888888888888, 0.8888888888888888, 0.7777777777777778, 0.8333333333333334, 0.8888888888888888, 0.8888888888888888, 0.7777777777777778, 0.6666666666666666, 0.8888888888888888, 0.9444444444444444, 0.8888888888888888, 0.8333333333333334, 0.8888888888888888, 0.8888888888888888, 0.8333333333333334, 0.7222222222222222, 0.6666666666666666, 0.9444444444444444, 0.6111111111111112, 0.8888888888888888, 0.9444444444444444, 0.8888888888888888, 0.8888888888888888, 0.7222222222222222, 0.8333333333333334, 0.7777777777777778, 0.8333333333333334, 1.0, 0.8888888888888888, 0.7777777777777778, 0.8333333333333334, 0.8333333333333334, 0.9444444444444444, 0.9444444444444444, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.7777777777777778, 0.8888888888888888, 0.9444444444444444, 0.8888888888888888, 0.8888888888888888, 0.8333333333333334, 0.7222222222222222, 0.7777777777777778, 0.8333333333333334, 0.8333333333333334, 0.7222222222222222, 0.6666666666666666, 0.7777777777777778, 0.9444444444444444, 0.9444444444444444, 0.9444444444444444, 0.8888888888888888, 0.8888888888888888, 0.8888888888888888, 0.8888888888888888, 0.8333333333333334, 0.7777777777777778, 0.8888888888888888, 0.7777777777777778, 0.9444444444444444, 0.7222222222222222, 0.8888888888888888, 0.8888888888888888, 0.7222222222222222, 0.6666666666666666, 0.7222222222222222, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.8888888888888888, 1.0, 0.9444444444444444, 1.0, 0.7777777777777778, 0.7777777777777778, 0.8888888888888888, 0.9444444444444444, 0.8888888888888888, 0.8333333333333334, 0.9444444444444444, 0.7222222222222222, 0.8333333333333334, 0.6666666666666666, 0.9444444444444444, 0.6666666666666666, 0.8333333333333334, 0.9444444444444444, 0.8888888888888888, 0.9444444444444444, 0.7777777777777778, 0.6666666666666666, 0.6666666666666666, 0.7222222222222222, 0.8888888888888888, 0.8333333333333334, 0.6666666666666666, 0.7222222222222222, 0.7777777777777778], [0.3888888888888889, 0.5555555555555556, 0.5, 0.5, 0.6666666666666666, 0.5, 0.6111111111111112, 0.6111111111111112, 0.6111111111111112, 0.4444444444444444, 0.5555555555555556, 0.3888888888888889, 0.5, 0.4444444444444444, 0.3333333333333333, 0.5, 0.5555555555555556, 0.5, 0.6666666666666666, 0.7777777777777778, 0.7777777777777778, 0.8333333333333334, 0.7222222222222222, 0.6111111111111112, 0.4444444444444444, 0.6666666666666666, 0.5555555555555556, 0.5555555555555556, 0.3888888888888889, 0.5, 0.5555555555555556, 0.5555555555555556, 0.6666666666666666, 0.6111111111111112, 0.4444444444444444, 0.5, 0.5, 0.7222222222222222, 0.6666666666666666, 0.6111111111111112, 0.7222222222222222, 0.5, 0.6111111111111112, 0.6666666666666666, 0.6666666666666666, 0.7777777777777778, 0.6111111111111112, 0.7222222222222222, 0.6111111111111112, 0.7777777777777778, 0.4444444444444444, 0.7777777777777778, 0.6666666666666666, 0.8333333333333334, 0.6666666666666666, 1.0, 0.8333333333333334, 0.9444444444444444, 0.7777777777777778, 0.7222222222222222, 0.8333333333333334, 0.6666666666666666, 0.7777777777777778, 0.5555555555555556, 0.6111111111111112, 0.6666666666666666, 0.5555555555555556, 0.8333333333333334, 0.7777777777777778, 0.8333333333333334, 0.8333333333333334, 0.6666666666666666, 0.7777777777777778, 0.6666666666666666, 0.8888888888888888, 0.7777777777777778, 0.7222222222222222, 0.7222222222222222, 0.7777777777777778, 0.8333333333333334, 0.7222222222222222, 0.7777777777777778, 0.8888888888888888, 0.6666666666666666, 0.8888888888888888, 0.7777777777777778, 0.7777777777777778, 0.6111111111111112, 0.6111111111111112, 0.6666666666666666, 0.6666666666666666, 0.6111111111111112, 0.8888888888888888, 0.7222222222222222, 0.8888888888888888, 0.6111111111111112, 0.6666666666666666, 0.8888888888888888, 0.6666666666666666, 0.7777777777777778, 0.7777777777777778, 0.7777777777777778, 0.7222222222222222, 0.7777777777777778, 0.7777777777777778, 0.7222222222222222, 0.7777777777777778, 0.8333333333333334, 0.6666666666666666, 0.7222222222222222, 0.8333333333333334, 0.6666666666666666, 0.7777777777777778, 0.9444444444444444, 0.7222222222222222, 0.7777777777777778, 0.8333333333333334, 0.8333333333333334, 0.7222222222222222, 0.7222222222222222, 0.7222222222222222, 0.8333333333333334, 0.7777777777777778, 0.8888888888888888, 0.7222222222222222, 0.8888888888888888, 0.7777777777777778, 0.8333333333333334, 0.8333333333333334, 0.7222222222222222, 0.8888888888888888, 0.8333333333333334, 0.8888888888888888, 0.7222222222222222, 0.7777777777777778, 0.8888888888888888, 0.8333333333333334, 0.8888888888888888, 0.8333333333333334, 0.6666666666666666, 0.7222222222222222, 0.8333333333333334, 0.7222222222222222, 0.7777777777777778, 0.7777777777777778, 0.7777777777777778, 0.8333333333333334, 0.7777777777777778, 0.7222222222222222, 0.8333333333333334, 0.6111111111111112, 0.7777777777777778, 0.5555555555555556, 0.8888888888888888, 0.8333333333333334, 0.9444444444444444, 0.7777777777777778, 0.7222222222222222, 0.7777777777777778, 0.7777777777777778, 0.7777777777777778, 0.7222222222222222, 0.8888888888888888, 0.8333333333333334, 0.7222222222222222, 0.7777777777777778, 0.7777777777777778, 0.8333333333333334, 0.9444444444444444, 0.8333333333333334, 0.7777777777777778, 0.7777777777777778, 0.7222222222222222, 0.8333333333333334, 0.7222222222222222, 0.8333333333333334, 0.7777777777777778, 0.9444444444444444, 0.8333333333333334, 0.7777777777777778, 0.8333333333333334, 0.7222222222222222, 0.9444444444444444, 0.7777777777777778, 0.8333333333333334, 0.6666666666666666, 0.7222222222222222, 0.8888888888888888, 0.8888888888888888, 0.8888888888888888, 0.7777777777777778, 0.9444444444444444, 0.9444444444444444, 1.0, 0.6111111111111112, 0.8333333333333334, 0.8333333333333334, 0.9444444444444444, 0.7222222222222222, 0.7222222222222222, 0.9444444444444444, 0.8333333333333334, 0.9444444444444444, 0.8888888888888888, 0.7777777777777778, 0.6666666666666666, 0.8333333333333334, 0.9444444444444444, 0.8333333333333334, 0.8333333333333334, 0.9444444444444444, 1.0, 0.9444444444444444, 0.6111111111111112, 0.8888888888888888, 0.8888888888888888, 0.7222222222222222, 0.7777777777777778, 0.8333333333333334, 0.7777777777777778, 0.8888888888888888, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.7222222222222222, 0.6111111111111112, 1.0, 0.8888888888888888, 1.0, 0.8333333333333334, 0.6666666666666666, 0.7777777777777778, 0.8888888888888888, 0.8333333333333334, 0.8333333333333334, 0.8888888888888888, 0.7222222222222222, 0.8333333333333334, 0.8333333333333334, 0.7222222222222222, 0.9444444444444444, 0.7777777777777778, 0.8888888888888888, 0.8888888888888888, 0.9444444444444444, 0.7777777777777778, 0.8333333333333334, 0.9444444444444444, 0.8888888888888888, 0.8888888888888888, 0.7777777777777778, 0.9444444444444444, 0.6666666666666666, 0.6666666666666666, 0.6666666666666666], [0.2777777777777778, 0.6111111111111112, 0.5555555555555556, 0.5, 0.3333333333333333, 0.2777777777777778, 0.5, 0.5555555555555556, 0.5, 0.7222222222222222, 0.7222222222222222, 0.5555555555555556, 0.6111111111111112, 0.6111111111111112, 0.7222222222222222, 0.4444444444444444, 0.6666666666666666, 0.5555555555555556, 0.5, 0.5555555555555556, 0.6666666666666666, 0.3333333333333333, 0.6666666666666666, 0.5, 0.5, 0.5555555555555556, 0.4444444444444444, 0.6666666666666666, 0.6666666666666666, 0.6111111111111112, 0.6111111111111112, 0.7222222222222222, 0.4444444444444444, 0.4444444444444444, 0.5555555555555556, 0.5555555555555556, 0.5, 0.5, 0.5555555555555556, 0.3333333333333333, 0.7222222222222222, 0.7222222222222222, 0.5555555555555556, 0.5555555555555556, 0.5555555555555556, 0.8333333333333334, 0.7777777777777778, 0.7222222222222222, 0.7222222222222222, 0.7777777777777778, 0.7777777777777778, 0.8333333333333334, 0.8333333333333334, 0.6111111111111112, 0.8333333333333334, 0.5555555555555556, 0.6666666666666666, 0.7222222222222222, 0.7222222222222222, 0.6666666666666666, 0.8888888888888888, 0.9444444444444444, 0.8333333333333334, 0.7222222222222222, 0.8888888888888888, 0.6666666666666666, 0.5, 0.7777777777777778, 0.7222222222222222, 0.7777777777777778, 0.6111111111111112, 0.7222222222222222, 0.6666666666666666, 0.7222222222222222, 0.7777777777777778, 0.6666666666666666, 0.6666666666666666, 0.6666666666666666, 0.6666666666666666, 0.8333333333333334, 0.6111111111111112, 0.7222222222222222, 0.7222222222222222, 0.7777777777777778, 0.7222222222222222, 0.8888888888888888, 0.8333333333333334, 0.8888888888888888, 0.8333333333333334, 0.6666666666666666, 0.7777777777777778, 0.8333333333333334, 0.8333333333333334, 0.8888888888888888, 0.8888888888888888, 0.7222222222222222, 0.8888888888888888, 0.7222222222222222, 0.7777777777777778, 0.8888888888888888, 0.6666666666666666, 0.8888888888888888, 0.7777777777777778, 0.8333333333333334, 0.9444444444444444, 0.7777777777777778, 0.8888888888888888, 0.9444444444444444, 0.8888888888888888, 0.7777777777777778, 0.7222222222222222, 0.8333333333333334, 0.8333333333333334, 0.8888888888888888, 0.6666666666666666, 0.7777777777777778, 0.7222222222222222, 0.8333333333333334, 0.8333333333333334, 0.7777777777777778, 0.7777777777777778, 0.6111111111111112, 0.6666666666666666, 0.7777777777777778, 0.9444444444444444, 0.8333333333333334, 0.8888888888888888, 0.8888888888888888, 0.8333333333333334, 0.7222222222222222, 0.7222222222222222, 0.8888888888888888, 0.8888888888888888, 1.0, 0.7777777777777778, 0.6666666666666666, 0.7777777777777778, 0.7222222222222222, 0.7777777777777778, 0.8333333333333334, 0.6666666666666666, 0.6666666666666666, 0.8888888888888888, 0.6666666666666666, 0.7222222222222222, 0.6666666666666666, 0.8888888888888888, 0.6666666666666666, 0.7222222222222222, 0.8333333333333334, 0.8333333333333334, 0.7222222222222222, 0.7777777777777778, 0.8333333333333334, 0.8888888888888888, 0.9444444444444444, 0.8333333333333334, 0.9444444444444444, 0.7222222222222222, 0.8333333333333334, 0.7222222222222222, 0.7222222222222222, 0.8333333333333334, 0.8888888888888888, 0.8888888888888888, 0.8888888888888888, 0.9444444444444444, 0.7777777777777778, 0.9444444444444444, 0.7222222222222222, 0.9444444444444444, 0.8333333333333334, 0.7777777777777778, 0.9444444444444444, 0.6111111111111112, 0.8888888888888888, 0.8333333333333334, 0.7777777777777778, 0.8333333333333334, 0.8333333333333334, 0.8888888888888888, 0.7777777777777778, 0.7777777777777778, 0.8888888888888888, 0.7777777777777778, 0.8333333333333334, 0.7777777777777778, 0.7777777777777778, 0.8333333333333334, 0.8888888888888888, 0.6666666666666666, 0.8888888888888888, 0.9444444444444444, 0.7222222222222222, 0.8333333333333334, 0.8333333333333334, 0.7777777777777778, 0.7777777777777778, 0.8333333333333334, 0.8333333333333334, 0.8888888888888888, 0.8888888888888888, 0.8333333333333334, 0.8888888888888888, 0.7777777777777778, 0.8333333333333334, 0.7777777777777778, 0.8888888888888888, 0.7777777777777778, 0.6111111111111112, 0.8888888888888888, 0.8888888888888888, 0.8888888888888888, 0.8333333333333334, 0.7222222222222222, 1.0, 0.9444444444444444, 0.8888888888888888, 0.7777777777777778, 0.7777777777777778, 0.8333333333333334, 1.0, 0.8888888888888888, 0.7777777777777778, 0.8888888888888888, 0.7222222222222222, 0.9444444444444444, 0.9444444444444444, 0.8333333333333334, 0.8888888888888888, 0.8888888888888888, 0.7777777777777778, 0.8888888888888888, 0.9444444444444444, 0.8333333333333334, 0.7777777777777778, 0.9444444444444444, 0.8333333333333334, 0.8888888888888888, 0.8333333333333334, 0.7777777777777778, 0.7777777777777778, 0.9444444444444444, 0.7777777777777778, 0.8888888888888888, 0.8888888888888888, 0.6666666666666666, 0.7777777777777778, 0.7777777777777778, 0.8888888888888888, 0.7777777777777778, 0.8333333333333334, 0.8333333333333334, 0.7222222222222222, 0.8888888888888888], [0.5555555555555556, 0.5555555555555556, 0.7222222222222222, 0.6666666666666666, 0.7777777777777778, 0.5, 0.3888888888888889, 0.6666666666666666, 0.4444444444444444, 0.5555555555555556, 0.5555555555555556, 0.4444444444444444, 0.6111111111111112, 0.6111111111111112, 0.5555555555555556, 0.6666666666666666, 0.5555555555555556, 0.5, 0.5555555555555556, 0.6666666666666666, 0.5555555555555556, 0.4444444444444444, 0.7222222222222222, 0.4444444444444444, 0.5, 0.5, 0.7222222222222222, 0.5, 0.4444444444444444, 0.6111111111111112, 0.5, 0.5555555555555556, 0.3888888888888889, 0.7222222222222222, 0.7777777777777778, 0.5555555555555556, 0.5555555555555556, 0.6666666666666666, 0.6666666666666666, 0.7222222222222222, 0.7222222222222222, 0.7777777777777778, 0.7222222222222222, 0.7777777777777778, 0.7777777777777778, 0.6666666666666666, 0.6666666666666666, 0.8333333333333334, 0.5555555555555556, 0.5555555555555556, 0.4444444444444444, 0.7777777777777778, 0.7222222222222222, 0.7222222222222222, 0.7222222222222222, 0.5, 0.8333333333333334, 0.8333333333333334, 0.6666666666666666, 0.7777777777777778, 0.8333333333333334, 0.7222222222222222, 0.7777777777777778, 0.8333333333333334, 0.7777777777777778, 0.7222222222222222, 0.6666666666666666, 0.6666666666666666, 0.8888888888888888, 0.7222222222222222, 0.7777777777777778, 0.6111111111111112, 0.6111111111111112, 0.7777777777777778, 0.6666666666666666, 0.6666666666666666, 0.7777777777777778, 0.6666666666666666, 0.8888888888888888, 0.6666666666666666, 0.6666666666666666, 0.7222222222222222, 0.7222222222222222, 0.7777777777777778, 0.7777777777777778, 0.6666666666666666, 0.6666666666666666, 1.0, 0.7222222222222222, 0.6666666666666666, 0.7777777777777778, 0.8333333333333334, 0.6666666666666666, 0.7777777777777778, 0.7222222222222222, 0.7777777777777778, 0.8888888888888888, 0.7777777777777778, 0.7222222222222222, 0.5555555555555556, 0.7222222222222222, 1.0, 0.9444444444444444, 0.8333333333333334, 0.7222222222222222, 0.7222222222222222, 0.8333333333333334, 0.9444444444444444, 0.6111111111111112, 0.8888888888888888, 0.7222222222222222, 0.7777777777777778, 0.6666666666666666, 0.7777777777777778, 0.8888888888888888, 0.6666666666666666, 0.7222222222222222, 0.7222222222222222, 0.8333333333333334, 0.8888888888888888, 0.7777777777777778, 0.7222222222222222, 0.8333333333333334, 0.7777777777777778, 0.8333333333333334, 0.6111111111111112, 0.7222222222222222, 0.7222222222222222, 0.6666666666666666, 0.8888888888888888, 0.7222222222222222, 0.8333333333333334, 0.8333333333333334, 0.9444444444444444, 0.8333333333333334, 0.7777777777777778, 0.7777777777777778, 0.6666666666666666, 0.8888888888888888, 0.7777777777777778, 0.7777777777777778, 0.7777777777777778, 0.7777777777777778, 0.8333333333333334, 0.7222222222222222, 0.8333333333333334, 0.6666666666666666, 0.7777777777777778, 0.5555555555555556, 0.7777777777777778, 0.9444444444444444, 0.6111111111111112, 0.7777777777777778, 0.6666666666666666, 0.8333333333333334, 0.8888888888888888, 0.8888888888888888, 0.9444444444444444, 0.7777777777777778, 0.8333333333333334, 0.7777777777777778, 0.7777777777777778, 0.7777777777777778, 0.8888888888888888, 0.7777777777777778, 0.6666666666666666, 0.8333333333333334, 0.7777777777777778, 0.9444444444444444, 0.9444444444444444, 0.8888888888888888, 0.7777777777777778, 0.7222222222222222, 0.6666666666666666, 0.8333333333333334, 0.7777777777777778, 0.8888888888888888, 0.8333333333333334, 0.6666666666666666, 0.6666666666666666, 0.8333333333333334, 0.7777777777777778, 0.8333333333333334, 0.7222222222222222, 0.8333333333333334, 1.0, 0.9444444444444444, 0.8333333333333334, 0.8333333333333334, 0.9444444444444444, 0.7777777777777778, 0.7222222222222222, 0.9444444444444444, 0.8333333333333334, 0.8888888888888888, 0.8333333333333334, 0.8888888888888888, 0.7777777777777778, 0.7222222222222222, 0.7777777777777778, 0.8333333333333334, 0.6666666666666666, 0.6111111111111112, 1.0, 0.7222222222222222, 0.8888888888888888, 0.9444444444444444, 0.9444444444444444, 0.7777777777777778, 0.8888888888888888, 0.7777777777777778, 0.8333333333333334, 0.9444444444444444, 0.8333333333333334, 0.8888888888888888, 0.7777777777777778, 0.8333333333333334, 0.8888888888888888, 0.8333333333333334, 1.0, 0.7777777777777778, 0.9444444444444444, 0.7777777777777778, 0.7777777777777778, 0.9444444444444444, 0.8888888888888888, 0.8888888888888888, 0.8888888888888888, 0.8888888888888888, 1.0, 0.8888888888888888, 0.6666666666666666, 0.7222222222222222, 0.8333333333333334, 0.8888888888888888, 0.6666666666666666, 0.8888888888888888, 0.7777777777777778, 0.8888888888888888, 0.8333333333333334, 0.7777777777777778, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.7777777777777778, 0.7222222222222222, 0.8888888888888888, 0.8333333333333334, 1.0, 0.8888888888888888, 0.7222222222222222, 0.8888888888888888, 0.7222222222222222, 0.6666666666666666, 0.6666666666666666], [0.4444444444444444, 0.4444444444444444, 0.5, 0.6111111111111112, 0.6666666666666666, 0.7777777777777778, 0.5, 0.6666666666666666, 0.5, 0.5555555555555556, 0.4444444444444444, 0.3888888888888889, 0.3888888888888889, 0.6111111111111112, 0.3333333333333333, 0.6666666666666666, 0.5555555555555556, 0.7222222222222222, 0.6111111111111112, 0.7222222222222222, 0.6111111111111112, 0.5555555555555556, 0.4444444444444444, 0.6666666666666666, 0.3888888888888889, 0.5, 0.6666666666666666, 0.4444444444444444, 0.6111111111111112, 0.5, 0.5555555555555556, 0.7222222222222222, 0.6111111111111112, 0.7777777777777778, 0.3888888888888889, 0.7777777777777778, 0.2777777777777778, 0.5555555555555556, 0.5, 0.5555555555555556, 0.5, 0.4444444444444444, 0.7777777777777778, 0.5555555555555556, 0.5555555555555556, 0.3888888888888889, 0.7222222222222222, 0.6111111111111112, 0.4444444444444444, 0.6111111111111112, 0.4444444444444444, 0.8333333333333334, 0.5555555555555556, 0.6666666666666666, 0.7777777777777778, 0.6666666666666666, 0.8888888888888888, 0.7222222222222222, 0.6111111111111112, 0.7777777777777778, 0.7777777777777778, 0.7777777777777778, 0.6666666666666666, 0.6111111111111112, 0.7777777777777778, 0.6666666666666666, 0.7777777777777778, 0.8888888888888888, 0.6666666666666666, 0.7777777777777778, 0.5, 0.7222222222222222, 0.5555555555555556, 0.7222222222222222, 0.8333333333333334, 0.8333333333333334, 0.7222222222222222, 0.7777777777777778, 0.9444444444444444, 0.7222222222222222, 0.7777777777777778, 0.6111111111111112, 0.7777777777777778, 0.7777777777777778, 0.7222222222222222, 0.7777777777777778, 0.6666666666666666, 0.7777777777777778, 0.7222222222222222, 0.7777777777777778, 0.5555555555555556, 0.8888888888888888, 0.8888888888888888, 0.8333333333333334, 0.8333333333333334, 0.7777777777777778, 0.7777777777777778, 0.6666666666666666, 0.7777777777777778, 0.8333333333333334, 0.8888888888888888, 0.7777777777777778, 0.8333333333333334, 0.7777777777777778, 0.8333333333333334, 0.7777777777777778, 0.8333333333333334, 0.6111111111111112, 0.8333333333333334, 0.7777777777777778, 0.8888888888888888, 0.8333333333333334, 0.9444444444444444, 0.8333333333333334, 0.7777777777777778, 1.0, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.9444444444444444, 0.6111111111111112, 0.8888888888888888, 0.8333333333333334, 0.7777777777777778, 0.8333333333333334, 1.0, 0.7777777777777778, 0.6111111111111112, 0.6666666666666666, 0.8888888888888888, 0.8888888888888888, 0.8333333333333334, 0.7222222222222222, 0.6666666666666666, 0.6111111111111112, 0.6666666666666666, 0.7777777777777778, 0.5555555555555556, 0.8888888888888888, 0.7222222222222222, 0.7222222222222222, 0.8888888888888888, 0.7777777777777778, 0.7777777777777778, 0.9444444444444444, 1.0, 0.8333333333333334, 0.7777777777777778, 0.8333333333333334, 0.7777777777777778, 0.8333333333333334, 1.0, 0.7777777777777778, 0.6111111111111112, 0.7777777777777778, 0.8888888888888888, 0.7222222222222222, 0.9444444444444444, 0.7777777777777778, 0.8888888888888888, 0.8888888888888888, 0.7777777777777778, 0.7222222222222222, 0.7777777777777778, 0.8888888888888888, 0.8333333333333334, 0.7777777777777778, 0.7222222222222222, 0.8333333333333334, 0.7222222222222222, 0.8888888888888888, 0.7777777777777778, 0.8888888888888888, 0.8888888888888888, 1.0, 0.8888888888888888, 0.7777777777777778, 0.7222222222222222, 0.7222222222222222, 0.8333333333333334, 0.9444444444444444, 0.7222222222222222, 0.8333333333333334, 0.8333333333333334, 0.9444444444444444, 0.9444444444444444, 0.8333333333333334, 0.8888888888888888, 0.7777777777777778, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.8888888888888888, 0.7222222222222222, 0.6666666666666666, 0.8333333333333334, 0.8333333333333334, 0.7777777777777778, 0.8888888888888888, 0.8333333333333334, 0.9444444444444444, 0.6666666666666666, 1.0, 0.8333333333333334, 1.0, 0.9444444444444444, 0.7777777777777778, 0.6666666666666666, 0.8888888888888888, 0.6666666666666666, 0.8333333333333334, 0.8333333333333334, 0.7222222222222222, 0.7777777777777778, 0.7222222222222222, 0.7777777777777778, 0.9444444444444444, 0.7777777777777778, 0.6666666666666666, 0.9444444444444444, 0.7777777777777778, 0.8333333333333334, 0.7777777777777778, 0.9444444444444444, 0.9444444444444444, 0.8888888888888888, 0.8333333333333334, 0.8333333333333334, 0.9444444444444444, 0.8888888888888888, 0.7777777777777778, 0.7777777777777778, 0.7777777777777778, 0.9444444444444444, 0.7777777777777778, 0.8333333333333334, 0.8333333333333334, 0.7222222222222222, 0.9444444444444444, 0.6666666666666666, 0.7777777777777778, 0.8888888888888888, 0.8333333333333334, 0.9444444444444444, 0.5555555555555556, 0.8888888888888888, 0.8333333333333334, 0.8888888888888888, 0.7777777777777778, 0.8888888888888888, 0.7777777777777778, 0.8333333333333334, 0.8888888888888888, 0.6666666666666666], [0.6111111111111112, 0.6111111111111112, 0.6666666666666666, 0.6111111111111112, 0.5555555555555556, 0.5555555555555556, 0.7222222222222222, 0.3888888888888889, 0.6111111111111112, 0.7222222222222222, 0.6111111111111112, 0.6666666666666666, 0.5, 0.5, 0.5555555555555556, 0.5555555555555556, 0.5, 0.3333333333333333, 0.7222222222222222, 0.5, 0.6111111111111112, 0.4444444444444444, 0.5555555555555556, 0.4444444444444444, 0.6111111111111112, 0.6111111111111112, 0.3888888888888889, 0.5, 0.5555555555555556, 0.3888888888888889, 0.4444444444444444, 0.6666666666666666, 0.4444444444444444, 0.8888888888888888, 0.7777777777777778, 0.7222222222222222, 0.8333333333333334, 0.7222222222222222, 0.7777777777777778, 0.8333333333333334, 0.6666666666666666, 0.4444444444444444, 0.8888888888888888, 0.7222222222222222, 0.6111111111111112, 0.7222222222222222, 0.7777777777777778, 0.7777777777777778, 0.5555555555555556, 0.6666666666666666, 1.0, 0.7777777777777778, 0.7222222222222222, 0.7222222222222222, 0.7222222222222222, 0.6111111111111112, 0.5555555555555556, 0.8333333333333334, 0.7777777777777778, 0.8888888888888888, 0.8888888888888888, 0.7222222222222222, 0.7777777777777778, 0.7222222222222222, 0.7222222222222222, 0.9444444444444444, 0.8888888888888888, 0.8888888888888888, 0.9444444444444444, 0.7777777777777778, 0.6111111111111112, 0.4444444444444444, 0.6111111111111112, 0.8333333333333334, 0.6666666666666666, 0.7222222222222222, 0.8888888888888888, 0.9444444444444444, 0.8333333333333334, 0.8888888888888888, 0.7777777777777778, 0.7777777777777778, 0.7777777777777778, 0.6666666666666666, 0.7777777777777778, 0.8333333333333334, 0.7777777777777778, 0.7222222222222222, 0.6111111111111112, 0.8333333333333334, 0.8333333333333334, 0.7777777777777778, 0.5555555555555556, 0.6666666666666666, 0.7222222222222222, 0.8333333333333334, 0.4444444444444444, 0.6666666666666666, 0.4444444444444444, 0.7777777777777778, 0.7222222222222222, 1.0, 0.8888888888888888, 0.8333333333333334, 0.7222222222222222, 0.7777777777777778, 0.6111111111111112, 0.8888888888888888, 0.7222222222222222, 0.6666666666666666, 0.9444444444444444, 0.7222222222222222, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.8888888888888888, 0.7222222222222222, 0.7777777777777778, 0.6666666666666666, 0.7777777777777778, 0.6111111111111112, 0.6111111111111112, 0.8888888888888888, 0.7777777777777778, 0.8888888888888888, 0.8888888888888888, 0.8888888888888888, 0.7222222222222222, 0.4444444444444444, 0.9444444444444444, 0.9444444444444444, 0.8333333333333334, 0.6666666666666666, 0.8888888888888888, 0.8333333333333334, 0.8888888888888888, 0.7777777777777778, 0.8888888888888888, 0.8888888888888888, 0.6666666666666666, 0.7777777777777778, 0.7777777777777778, 0.7777777777777778, 0.8333333333333334, 0.9444444444444444, 0.7777777777777778, 0.7222222222222222, 0.9444444444444444, 0.7777777777777778, 0.7222222222222222, 0.8333333333333334, 0.8888888888888888, 0.8888888888888888, 0.7777777777777778, 0.7222222222222222, 0.8888888888888888, 0.7777777777777778, 1.0, 0.8333333333333334, 0.8333333333333334, 0.7777777777777778, 0.7777777777777778, 0.6666666666666666, 0.8888888888888888, 0.9444444444444444, 0.8888888888888888, 0.6666666666666666, 0.7777777777777778, 0.6111111111111112, 0.9444444444444444, 0.8333333333333334, 0.8333333333333334, 0.7777777777777778, 0.8333333333333334, 0.7222222222222222, 0.6666666666666666, 0.7222222222222222, 0.7222222222222222, 0.8888888888888888, 0.7777777777777778, 0.7777777777777778, 0.7777777777777778, 0.8333333333333334, 0.7777777777777778, 0.8888888888888888, 0.7222222222222222, 0.7222222222222222, 0.7777777777777778, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.8888888888888888, 0.8888888888888888, 0.8333333333333334, 0.8333333333333334, 0.8888888888888888, 0.7222222222222222, 0.8333333333333334, 0.8888888888888888, 0.6666666666666666, 0.8333333333333334, 0.8333333333333334, 0.7777777777777778, 0.7777777777777778, 0.8333333333333334, 0.9444444444444444, 1.0, 0.7777777777777778, 0.6666666666666666, 0.8888888888888888, 0.8888888888888888, 0.8888888888888888, 0.7222222222222222, 0.8888888888888888, 0.7777777777777778, 0.7777777777777778, 0.7222222222222222, 0.9444444444444444, 0.7222222222222222, 0.7777777777777778, 0.7777777777777778, 0.8333333333333334, 0.7777777777777778, 0.6666666666666666, 0.9444444444444444, 0.8333333333333334, 0.8888888888888888, 0.8888888888888888, 0.8333333333333334, 0.7777777777777778, 0.7222222222222222, 0.7777777777777778, 0.9444444444444444, 0.9444444444444444, 0.8333333333333334, 0.8333333333333334, 0.6111111111111112, 0.8333333333333334, 0.8333333333333334, 0.7222222222222222, 0.8888888888888888, 0.7777777777777778, 0.8888888888888888, 0.8333333333333334, 0.7777777777777778, 0.8333333333333334, 0.8333333333333334, 0.9444444444444444, 0.9444444444444444, 0.8333333333333334, 0.8333333333333334, 0.9444444444444444, 0.7222222222222222, 0.9444444444444444, 0.8888888888888888], [0.5, 0.4444444444444444, 0.3333333333333333, 0.3333333333333333, 0.6666666666666666, 0.6666666666666666, 0.4444444444444444, 0.7777777777777778, 0.5, 0.5555555555555556, 0.5, 0.5, 0.5555555555555556, 0.3333333333333333, 0.6666666666666666, 0.6111111111111112, 0.5555555555555556, 0.4444444444444444, 0.6111111111111112, 0.6111111111111112, 0.5555555555555556, 0.4444444444444444, 0.5555555555555556, 0.6111111111111112, 0.6111111111111112, 0.6111111111111112, 0.6666666666666666, 0.6666666666666666, 0.7777777777777778, 0.7222222222222222, 0.3333333333333333, 0.5555555555555556, 0.3333333333333333, 0.6111111111111112, 0.5, 0.6111111111111112, 0.6666666666666666, 0.6111111111111112, 0.3888888888888889, 0.5, 0.4444444444444444, 0.4444444444444444, 0.5, 0.6666666666666666, 0.6111111111111112, 0.7222222222222222, 0.7777777777777778, 0.7222222222222222, 0.7777777777777778, 0.6111111111111112, 0.7777777777777778, 0.6666666666666666, 0.8888888888888888, 0.6111111111111112, 0.7777777777777778, 0.6666666666666666, 0.5555555555555556, 0.7222222222222222, 0.6111111111111112, 0.8888888888888888, 0.8333333333333334, 0.8888888888888888, 0.7222222222222222, 0.7222222222222222, 0.7777777777777778, 0.6111111111111112, 0.8888888888888888, 0.7777777777777778, 0.6111111111111112, 0.6666666666666666, 0.6111111111111112, 0.8333333333333334, 0.8888888888888888, 0.5, 0.8888888888888888, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.7222222222222222, 0.7222222222222222, 0.8333333333333334, 0.8888888888888888, 0.7222222222222222, 0.8333333333333334, 0.8333333333333334, 0.7777777777777778, 0.7222222222222222, 0.7777777777777778, 0.7222222222222222, 0.8333333333333334, 0.7222222222222222, 0.7222222222222222, 0.8888888888888888, 0.5555555555555556, 0.8888888888888888, 0.8888888888888888, 0.7222222222222222, 0.7222222222222222, 0.8333333333333334, 0.7777777777777778, 0.7777777777777778, 0.6666666666666666, 0.8888888888888888, 0.7777777777777778, 0.9444444444444444, 0.9444444444444444, 0.9444444444444444, 0.7777777777777778, 0.6666666666666666, 0.7777777777777778, 0.7777777777777778, 0.5555555555555556, 0.8888888888888888, 0.7777777777777778, 0.8888888888888888, 0.7777777777777778, 0.7222222222222222, 0.7222222222222222, 0.7222222222222222, 0.6111111111111112, 0.9444444444444444, 0.7777777777777778, 0.6666666666666666, 0.7777777777777778, 0.8333333333333334, 0.7777777777777778, 0.7222222222222222, 0.7222222222222222, 0.9444444444444444, 0.7777777777777778, 0.7222222222222222, 0.8333333333333334, 0.8333333333333334, 0.7777777777777778, 0.8333333333333334, 0.7777777777777778, 0.8888888888888888, 0.8888888888888888, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.8888888888888888, 0.7222222222222222, 0.9444444444444444, 0.8888888888888888, 0.8888888888888888, 0.7777777777777778, 0.8888888888888888, 0.7222222222222222, 1.0, 0.8888888888888888, 0.6666666666666666, 0.8333333333333334, 0.7777777777777778, 0.7777777777777778, 0.8888888888888888, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.7777777777777778, 0.7777777777777778, 0.7222222222222222, 0.7222222222222222, 0.8888888888888888, 0.8888888888888888, 0.8888888888888888, 0.8888888888888888, 0.6666666666666666, 0.7777777777777778, 0.5555555555555556, 0.8333333333333334, 0.8888888888888888, 0.6111111111111112, 0.7777777777777778, 0.7777777777777778, 0.7222222222222222, 0.7777777777777778, 0.7222222222222222, 0.6666666666666666, 0.7777777777777778, 0.7777777777777778, 0.7222222222222222, 0.8333333333333334, 0.8888888888888888, 0.6111111111111112, 0.9444444444444444, 0.8888888888888888, 0.9444444444444444, 0.9444444444444444, 0.7777777777777778, 0.8333333333333334, 0.7777777777777778, 0.8333333333333334, 0.9444444444444444, 0.9444444444444444, 0.7777777777777778, 0.8333333333333334, 0.9444444444444444, 0.8888888888888888, 0.7777777777777778, 0.8888888888888888, 0.8888888888888888, 0.7777777777777778, 0.7777777777777778, 0.6666666666666666, 0.8888888888888888, 0.8888888888888888, 0.9444444444444444, 0.8333333333333334, 0.8888888888888888, 0.7222222222222222, 0.8888888888888888, 0.8888888888888888, 0.8888888888888888, 0.8333333333333334, 0.6666666666666666, 0.8888888888888888, 1.0, 0.9444444444444444, 0.8888888888888888, 0.7777777777777778, 0.8333333333333334, 0.7222222222222222, 0.7777777777777778, 0.9444444444444444, 0.7222222222222222, 1.0, 0.9444444444444444, 0.7222222222222222, 0.8333333333333334, 0.7222222222222222, 0.9444444444444444, 0.8333333333333334, 0.8888888888888888, 0.7777777777777778, 0.9444444444444444, 0.8888888888888888, 0.8333333333333334, 0.8888888888888888, 0.8888888888888888, 0.6666666666666666, 0.7777777777777778, 0.7777777777777778, 0.7222222222222222, 0.9444444444444444, 0.8333333333333334, 0.8888888888888888, 0.8888888888888888, 0.8333333333333334, 0.8888888888888888], [0.5555555555555556, 0.5, 0.5, 0.5555555555555556, 0.6111111111111112, 0.5555555555555556, 0.6666666666666666, 0.6666666666666666, 0.6111111111111112, 0.2777777777777778, 0.5, 0.6666666666666666, 0.6111111111111112, 0.6111111111111112, 0.6111111111111112, 0.2777777777777778, 0.4444444444444444, 0.6666666666666666, 0.5, 0.7777777777777778, 0.5555555555555556, 0.5555555555555556, 0.5, 0.6111111111111112, 0.5555555555555556, 0.3888888888888889, 0.5, 0.4444444444444444, 0.5, 0.5, 0.6666666666666666, 0.5, 0.4444444444444444, 0.6111111111111112, 0.3333333333333333, 0.7777777777777778, 0.8888888888888888, 0.7222222222222222, 0.7222222222222222, 0.7777777777777778, 0.6666666666666666, 0.7777777777777778, 0.6666666666666666, 0.6666666666666666, 0.5555555555555556, 0.8888888888888888, 0.7222222222222222, 0.6111111111111112, 0.7777777777777778, 0.7777777777777778, 0.8888888888888888, 0.7777777777777778, 0.7777777777777778, 0.8888888888888888, 0.7777777777777778, 0.7222222222222222, 0.8333333333333334, 0.8888888888888888, 0.8333333333333334, 0.6666666666666666, 0.7222222222222222, 0.8888888888888888, 0.6111111111111112, 0.6666666666666666, 0.8333333333333334, 0.7777777777777778, 0.5555555555555556, 0.7222222222222222, 0.8333333333333334, 1.0, 0.5555555555555556, 0.7222222222222222, 0.7777777777777778, 0.8888888888888888, 0.6111111111111112, 0.7222222222222222, 0.5555555555555556, 0.8333333333333334, 0.6666666666666666, 0.6666666666666666, 0.8888888888888888, 0.8333333333333334, 0.6666666666666666, 0.8333333333333334, 0.7777777777777778, 0.8888888888888888, 0.8333333333333334, 0.7777777777777778, 0.8333333333333334, 0.7777777777777778, 0.7777777777777778, 0.7222222222222222, 0.7222222222222222, 0.7777777777777778, 0.7777777777777778, 0.8333333333333334, 0.7222222222222222, 0.8333333333333334, 0.6666666666666666, 0.8888888888888888, 0.7777777777777778, 0.7777777777777778, 0.9444444444444444, 0.7222222222222222, 0.8333333333333334, 0.8888888888888888, 0.8333333333333334, 0.6666666666666666, 0.5, 0.7222222222222222, 0.8888888888888888, 0.9444444444444444, 0.8333333333333334, 0.8333333333333334, 0.6111111111111112, 0.7222222222222222, 0.7222222222222222, 0.8888888888888888, 1.0, 0.7222222222222222, 0.8333333333333334, 0.6111111111111112, 0.7222222222222222, 0.8888888888888888, 0.7777777777777778, 0.8333333333333334, 0.8333333333333334, 0.8888888888888888, 0.7222222222222222, 0.8333333333333334, 0.8333333333333334, 0.9444444444444444, 0.8888888888888888, 0.7222222222222222, 0.8333333333333334, 0.7777777777777778, 0.8333333333333334, 0.8888888888888888, 0.8888888888888888, 0.8333333333333334, 0.5, 0.9444444444444444, 0.7777777777777778, 0.7777777777777778, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.8888888888888888, 0.7222222222222222, 0.8333333333333334, 0.7222222222222222, 0.8333333333333334, 0.8888888888888888, 0.7222222222222222, 0.9444444444444444, 0.7222222222222222, 0.7777777777777778, 0.8333333333333334, 0.7222222222222222, 0.7777777777777778, 0.8888888888888888, 0.7222222222222222, 0.8888888888888888, 0.7777777777777778, 0.9444444444444444, 0.8888888888888888, 0.8333333333333334, 0.8888888888888888, 0.8333333333333334, 0.7777777777777778, 0.8888888888888888, 0.9444444444444444, 0.8333333333333334, 0.7222222222222222, 0.8333333333333334, 0.8888888888888888, 0.8888888888888888, 0.8333333333333334, 0.6666666666666666, 0.8333333333333334, 0.8888888888888888, 0.8333333333333334, 0.8888888888888888, 0.7222222222222222, 0.8333333333333334, 0.7777777777777778, 0.7777777777777778, 0.6666666666666666, 0.7222222222222222, 0.6666666666666666, 0.7777777777777778, 0.9444444444444444, 0.7777777777777778, 0.7777777777777778, 0.7222222222222222, 0.7777777777777778, 0.8333333333333334, 0.5555555555555556, 0.8333333333333334, 0.9444444444444444, 0.7777777777777778, 0.7222222222222222, 0.8333333333333334, 0.7777777777777778, 0.8888888888888888, 0.8333333333333334, 0.8888888888888888, 0.8888888888888888, 0.9444444444444444, 0.8333333333333334, 0.8888888888888888, 0.7222222222222222, 0.8888888888888888, 0.7777777777777778, 0.8888888888888888, 0.8888888888888888, 0.8888888888888888, 0.7777777777777778, 0.7777777777777778, 0.6666666666666666, 0.8333333333333334, 0.7222222222222222, 0.8333333333333334, 0.7777777777777778, 0.7777777777777778, 0.6666666666666666, 0.8888888888888888, 0.8333333333333334, 0.8888888888888888, 0.9444444444444444, 0.7222222222222222, 0.7777777777777778, 0.8333333333333334, 1.0, 0.7222222222222222, 0.9444444444444444, 0.9444444444444444, 0.7777777777777778, 1.0, 0.7777777777777778, 0.8333333333333334, 0.8333333333333334, 0.7777777777777778, 0.8888888888888888, 0.5555555555555556, 0.8888888888888888, 0.8333333333333334, 0.8888888888888888, 0.8333333333333334, 0.7777777777777778, 0.7777777777777778, 0.8888888888888888, 0.8888888888888888, 0.9444444444444444, 1.0]], 'loss': [[0.03887405660417345, 0.038493875000211925, 0.03843091262711419, 0.03885499636332194, 0.0385629932085673, 0.03856491380267673, 0.03833615779876709, 0.038125819630093045, 0.03837225172254774, 0.03936325841479831, 0.03742735253440009, 0.03730717963642544, 0.037986311647627086, 0.03893677062458462, 0.03617912530899048, 0.036948164304097496, 0.03979316684934828, 0.03736124767197503, 0.04054336746533712, 0.03942950566609701, 0.036368466085857816, 0.03913088970714145, 0.037913786040412054, 0.0393157336446974, 0.03521596723132663, 0.03597054216596815, 0.0383115377691057, 0.038604292604658336, 0.0375593900680542, 0.03711699114905463, 0.03753858142428928, 0.03609115216467115, 0.038907534546322294, 0.03569855623775058, 0.03662246134546068, 0.03666195604536268, 0.03874441981315613, 0.03640050358242459, 0.03784150878588358, 0.03881579306390551, 0.0337847736146715, 0.03866265548600091, 0.036626451545291476, 0.03903169102138943, 0.03510183095932007, 0.036075443029403687, 0.03610025180710687, 0.033149659633636475, 0.036170257462395564, 0.03285801410675049, 0.07885416348775227, 0.03852077987458971, 0.03482729858822293, 0.03064609898461236, 0.031823303964402944, 0.03405577606625027, 0.03733835617701212, 0.029471741782294378, 0.029666913880242243, 0.029230114486482408, 0.035791622267829046, 0.03060555789205763, 0.042250090175204806, 0.030421571599112615, 0.03800035516421, 0.018308248784806993, 0.03232817517386542, 0.0330162677499983, 0.030232148038016424, 0.025979886452356975, 0.029243717590967815, 0.028138349453608196, 0.024862781167030334, 0.025120110975371465, 0.022199294633335538, 0.02736864321761661, 0.03788642088572184, 0.026203382346365187, 0.021825336747699313, 0.02565715379185147, 0.021578005618519254, 0.03131646580166287, 0.02368870708677504, 0.027514533864127264, 0.035126580132378474, 0.027235802676942613, 0.02191077172756195, 0.025518702136145696, 0.02538408835728963, 0.020620667272143893, 0.024042709006203547, 0.026409602827496, 0.022852369480662875, 0.03503057029512194, 0.03578835725784302, 0.038513382275899254, 0.03430055578549703, 0.0266291747490565, 0.01844916409916348, 0.029816375838385686, 0.02730790442890591, 0.045162717501322426, 0.01585506068335639, 0.0282909009191725, 0.016616990168889362, 0.025474889410866633, 0.030856175555123225, 0.02265806992848714, 0.022897235221332975, 0.02210951182577345, 0.025731581780645583, 0.02628796464867062, 0.017670212520493403, 0.01955691973368327, 0.026116942365964253, 0.02328727642695109, 0.03299970428148905, 0.01189586685763465, 0.03029295139842563, 0.015175513095325895, 0.017587920029958088, 0.02380642791589101, 0.0234849966234631, 0.02630944550037384, 0.035381088654200234, 0.041506426201926336, 0.03020709753036499, 0.030709366003672283, 0.03926450676388211, 0.01779141028722127, 0.028331051270167034, 0.030187653170691595, 0.033982929256227284, 0.018091272976663377, 0.02843299176957872, 0.017684333854251437, 0.027453069885571797, 0.02809089422225952, 0.025900413592656452, 0.03088623285293579, 0.01745230456193288, 0.021144673228263855, 0.03643614053726196, 0.020813537968529597, 0.016232488883866206, 0.019437733623716567, 0.018013220694330003, 0.024906494551234774, 0.020502014292611018, 0.03235095077090793, 0.03497033649020725, 0.03254323204358419, 0.07019262843661839, 0.022639546129438613, 0.024987399578094482, 0.01925524075826009, 0.02223091655307346, 0.016636220945252314, 0.04319655895233154, 0.021609839465883043, 0.021996748116281297, 0.02685590088367462, 0.019030054410298664, 0.017750269836849637, 0.019942283630371094, 0.02333816885948181, 0.019037091069751315, 0.017214410834842257, 0.023281671934657626, 0.019630902343326144, 0.019906547334459092, 0.027421818839179143, 0.019050232238239713, 0.016359451744291518, 0.02231875889831119, 0.0240486396683587, 0.019569890366660223, 0.030099295907550387, 0.01196935772895813, 0.030314726961983576, 0.01786805358197954, 0.02754618227481842, 0.020374120937453374, 0.02443027827474806, 0.023111911283599004, 0.017261344525549147, 0.02498361137178209, 0.028050757116741605, 0.018421126736534968, 0.02006040347947015, 0.016436219215393066, 0.0266353620423211, 0.025368814667065937, 0.03196887506379022, 0.03466863764656915, 0.016531402866045635, 0.029321981800927058, 0.01743007368511624, 0.031351986858579844, 0.022011422448688082, 0.012765728765063815, 0.02498307658566369, 0.013061093787352243, 0.043720112906561956, 0.026417341497209337, 0.029845651653077867, 0.015690093239148457, 0.023675945070054796, 0.023179607258902654, 0.01450527376598782, 0.01677518089612325, 0.02412144508626726, 0.028231355879041884, 0.030771401193406846, 0.03668567869398329, 0.015027383963267008, 0.022666768895255193, 0.02894552217589484, 0.016711402270529006, 0.02181921237044864, 0.01557712091339959, 0.011573952933152517, 0.022251904010772705, 0.021397372086842854, 0.02456005745463901, 0.026229484213723078, 0.026050802734163072, 0.01679184039433797, 0.02147242095735338, 0.01954471402698093, 0.007277737889024947, 0.01599871450000339, 0.03379626075426737, 0.0127025478416019, 0.02641483810212877, 0.014214022292031182, 0.019116905000474717, 0.009787633187241025, 0.025371200508541532, 0.02549535036087036, 0.025484936104880437, 0.02751080526245965, 0.039123492108450994, 0.023361153072781034, 0.018752563330862258, 0.03753336270650228, 0.02897832791010539, 0.019822509752379522, 0.019330074389775593, 0.02748275962140825, 0.010962172514862485, 0.027631241414282057, 0.021119715438948736, 0.0307312806447347, 0.04886277185546027], [0.036978072590298124, 0.039832234382629395, 0.03699123197131687, 0.03760114312171936, 0.03626875744925605, 0.04030492901802063, 0.03664887613720364, 0.03885436389181349, 0.038907766342163086, 0.040257510211732656, 0.036741928921805486, 0.037364555729760066, 0.04021237293879191, 0.03569771183861627, 0.0409057272805108, 0.038714690340889826, 0.041572507884767324, 0.03841837247212728, 0.03968369960784912, 0.04042154881689283, 0.03906732797622681, 0.03734727700551351, 0.03664092222849528, 0.03666778405507406, 0.03638338049252828, 0.036468810505337186, 0.038433074951171875, 0.03894716501235962, 0.0369596348868476, 0.03762244847085741, 0.03964017497168647, 0.03785244623819987, 0.03833419746822781, 0.037720166974597506, 0.03908322254816691, 0.036535064379374184, 0.036655545234680176, 0.03739163610670301, 0.03393793437216017, 0.037253346708085805, 0.03574334250556098, 0.0373390085167355, 0.03842580318450928, 0.03728857636451721, 0.036739120880762734, 0.0333446827199724, 0.03651853402455648, 0.037039230267206825, 0.03655321730507745, 0.03632421294848124, 0.0663227637608846, 0.037312842077679105, 0.03308788935343424, 0.03515441550148858, 0.03065299325519138, 0.0324474208884769, 0.0326805743906233, 0.03279558155271742, 0.03345296449131436, 0.03129918376604716, 0.034134348233540855, 0.02844213114844428, 0.03213795026143392, 0.0294270118077596, 0.025859170489841037, 0.04284823271963331, 0.035235153304206, 0.024304247564739652, 0.03999211721950107, 0.038516107532713145, 0.0287555820412106, 0.027345849408043757, 0.03338750865724352, 0.0290737251440684, 0.028442187441719904, 0.026833832263946533, 0.024939147962464228, 0.0237114859951867, 0.028990738921695285, 0.027032262749142118, 0.027813606792026095, 0.02735230326652527, 0.02203414175245497, 0.02349394890997145, 0.03880046804745992, 0.03860993517769708, 0.024601325392723083, 0.030200971497429743, 0.03962787654664782, 0.032109724150763616, 0.03113886382844713, 0.02138452563020918, 0.03679488764868842, 0.018825560808181763, 0.02310373220178816, 0.03557639982965258, 0.0251931498448054, 0.02680103646384345, 0.026997021502918668, 0.026345002982351515, 0.03625083631939358, 0.060211896896362305, 0.03744870755407545, 0.031654804944992065, 0.03182074427604675, 0.03533977270126343, 0.027104824781417847, 0.026314126120673284, 0.019032042887475755, 0.02781135506100125, 0.025723571578661602, 0.03131753206253052, 0.0289539893468221, 0.043432888057496816, 0.025497645139694214, 0.024181235167715285, 0.027619406580924988, 0.018784397178226046, 0.02427221503522661, 0.03282807601822747, 0.023356803589397006, 0.02853126327196757, 0.024876798192660015, 0.029885258939531114, 0.02808424499299791, 0.017721086740493774, 0.04764779739909702, 0.0423876510726081, 0.02071644862492879, 0.02481638060675727, 0.019372996356752183, 0.02028374539481269, 0.03320742646853129, 0.023086607456207275, 0.022481646802690294, 0.021493585573302373, 0.0276822696129481, 0.026334315538406372, 0.01691987117131551, 0.027431511216693454, 0.029186010360717773, 0.026065520114368863, 0.02312449448638492, 0.01276140163342158, 0.02420659363269806, 0.028161671426561143, 0.015560661753018698, 0.022345696886380512, 0.021572705772187974, 0.01889554328388638, 0.021699117289649114, 0.018097988433308072, 0.05858963065677219, 0.01924926373693678, 0.030392636855443318, 0.022017689214812383, 0.022233903408050537, 0.014978422058953179, 0.037168661753336586, 0.02237566477722592, 0.03794667455885145, 0.038466304540634155, 0.03000215358204312, 0.038721174001693726, 0.02131092713938819, 0.03221560186809964, 0.028474562697940402, 0.03288669056362576, 0.031099319458007812, 0.02391207880443997, 0.022918204466501873, 0.02579258382320404, 0.021834206249978807, 0.016622745328479342, 0.028027911980946858, 0.019419764479001362, 0.03168665369351705, 0.0382650891939799, 0.026131848494211834, 0.03262562884224786, 0.023911681440141466, 0.025431179338031344, 0.01886052555508084, 0.02513201203611162, 0.023661307162708707, 0.030803908904393513, 0.03094515535566542, 0.02559046447277069, 0.02576338251431783, 0.018232620424694486, 0.027050445477167766, 0.03676631384425693, 0.03530868887901306, 0.02991840574476454, 0.019873907168706257, 0.019818897048632305, 0.0364964505036672, 0.024804822272724576, 0.021883774134847853, 0.03263383772638109, 0.02096268203523424, 0.02698419491449992, 0.0333253178331587, 0.04201834731631809, 0.03882228003607856, 0.025996105538474187, 0.023713340361913044, 0.025272693898942735, 0.02504219611485799, 0.026101067662239075, 0.022875413298606873, 0.020817099346054926, 0.02719057599703471, 0.03009951114654541, 0.022605028417375352, 0.03347761101192898, 0.023803518878089056, 0.01738558875189887, 0.023242112663057115, 0.028816266192330256, 0.014161624842219882, 0.02153475085894267, 0.026329191194640264, 0.025877472427156236, 0.03442871901724073, 0.01914880507522159, 0.01155172950691647, 0.02361988855732812, 0.023036272989379034, 0.023801479074690077, 0.033370637231402926, 0.02158491313457489, 0.027214758925967746, 0.026278613342179194, 0.01866454879442851, 0.02898878852526347, 0.017529896563953824, 0.02855236331621806, 0.020181154211362202, 0.021775195995966595, 0.027753712402449712, 0.013317080007659065, 0.022543774710761175, 0.019330523080295987, 0.016850334074762132, 0.024781352943844266, 0.018445577886369493, 0.023759293887350295, 0.01999903056356642, 0.009542518191867404, 0.03421955638461643, 0.027505485547913447, 0.014383185240957472, 0.029064615567525227, 0.06288909249835545], [0.03885533743434482, 0.03848977552519904, 0.03851473662588331, 0.037788314951790705, 0.03844296269946628, 0.03803875048955282, 0.038735293679767184, 0.03796466191609701, 0.03795819150076972, 0.03728677166832818, 0.03809619943300883, 0.03753633300463358, 0.03770742813746134, 0.03738095362981161, 0.03852832979626126, 0.03812320033709208, 0.03667040997081333, 0.03948895467652215, 0.036473754379484385, 0.03795091311136881, 0.038781566752327815, 0.03856791390313043, 0.03538473116026984, 0.040946235259373985, 0.037188675668504506, 0.03907650046878391, 0.037245230542288885, 0.03775952921973334, 0.03500182098812527, 0.03629005617565579, 0.03771593835618761, 0.035702003373040095, 0.03670755690998501, 0.03699485460917155, 0.03580452336205377, 0.03562528888384501, 0.036717245976130165, 0.03744548559188843, 0.0345636076397366, 0.03380597299999661, 0.035577197869618736, 0.036043491628434926, 0.03168264031410217, 0.032592174079683095, 0.0336667795976003, 0.035634630256228976, 0.03462695082028707, 0.02901285555627611, 0.03137793474727207, 0.029214080837037828, 0.052733533912234835, 0.03183561232354906, 0.03548000587357415, 0.032630324363708496, 0.0329910847875807, 0.02808990412288242, 0.03005243672264947, 0.030322992139392428, 0.023728138870663114, 0.04605273405710856, 0.017898334397210017, 0.041297194030549794, 0.03727841046121386, 0.026060680548350017, 0.027216075195206538, 0.02747498287094964, 0.03402402003606161, 0.020342513918876648, 0.01794921358426412, 0.03923550579282972, 0.029253522555033367, 0.04510772228240967, 0.02551449669731988, 0.028774393929375544, 0.028809924920399983, 0.032778263092041016, 0.02975134054819743, 0.037013620138168335, 0.024841967556211684, 0.029010335604349773, 0.03106135129928589, 0.02855048245853848, 0.028444886207580566, 0.021485396557384066, 0.03399398591783312, 0.03426880637804667, 0.03058101733525594, 0.023770267764727276, 0.017090878552860685, 0.030927346812354192, 0.023961025807592604, 0.025895085599687364, 0.024463236331939697, 0.01814690563413832, 0.022188483013047114, 0.025065516432126362, 0.021412962012820773, 0.02362394995159573, 0.025044221017095778, 0.03836166858673096, 0.025947461525599163, 0.042044639587402344, 0.01744863556491004, 0.037459846999910146, 0.03381901648309496, 0.025225015150176153, 0.03401445349057516, 0.018847026758723788, 0.01716567575931549, 0.020021900534629822, 0.020875619517432317, 0.03303364912668864, 0.027396607730123732, 0.019212717811266582, 0.024379503395822313, 0.025433793663978577, 0.038476377725601196, 0.029240263832939997, 0.02423497537771861, 0.025318039788140193, 0.03761562373903063, 0.042752828862931996, 0.02113310992717743, 0.023691963818338182, 0.019042480323049758, 0.034009820885128446, 0.023495796653959487, 0.024094626307487488, 0.025086123082372878, 0.025766308108965557, 0.022002692023913067, 0.022717957695325215, 0.032151752048068576, 0.019606123367945354, 0.018846703900231257, 0.016308378842141893, 0.025155643622080486, 0.03481490082210965, 0.039791342284944325, 0.01886805726422204, 0.019302308559417725, 0.026577976014879014, 0.03152755896250407, 0.041119780805375844, 0.03105719221962823, 0.01858341693878174, 0.024572546283404034, 0.026741257972187467, 0.009555190801620483, 0.04173485769165887, 0.02346984710958269, 0.033419142166773476, 0.056708984904819064, 0.030855125851101346, 0.0205764373143514, 0.018494811322953966, 0.023489710357454088, 0.024046970738304987, 0.0179202804962794, 0.02273242672284444, 0.03178684910138448, 0.042083647516038686, 0.01746600866317749, 0.013883983923329247, 0.021885030799441867, 0.026658160818947688, 0.019835771785842046, 0.02125039365556505, 0.02493010958035787, 0.03214312924279107, 0.037829816341400146, 0.01631246507167816, 0.04627680116229587, 0.018135648634698655, 0.016134629646937054, 0.01967362066109975, 0.020201702912648518, 0.02973357505268521, 0.022953854666815862, 0.026019554999139573, 0.017083355122142367, 0.01276555491818322, 0.024955525994300842, 0.03269864784346686, 0.02796238660812378, 0.024082995123333402, 0.014303417669402229, 0.015452802181243896, 0.02266646259360843, 0.021374871333440144, 0.025556080871158175, 0.022715449333190918, 0.031969587008158364, 0.020530533459451463, 0.012984115216467116, 0.021735320488611858, 0.02103752228948805, 0.024119863907496136, 0.02947857313685947, 0.029103590382470026, 0.026892764700783625, 0.020952170093854267, 0.03460146321190728, 0.08253805504904853, 0.03255101044972738, 0.013445861637592316, 0.010351224078072442, 0.015093202392260233, 0.020357838935322232, 0.020447166429625616, 0.01738673945267995, 0.016094181272718642, 0.02221685979101393, 0.02409720089700487, 0.014504679375224643, 0.03341108229425219, 0.013265993860032823, 0.030952619181738958, 0.01761918928888109, 0.024312921696239047, 0.025293290615081787, 0.03582722610897488, 0.02986647023095025, 0.01682962973912557, 0.024440533585018583, 0.020131289958953857, 0.014690973692470126, 0.008346478144327799, 0.016279876232147217, 0.012591038313176896, 0.03320535355144077, 0.03239282303386264, 0.017462051577038236, 0.013273918794261085, 0.015993676251835294, 0.019531516565216914, 0.011689411269293891, 0.03183562888039483, 0.02530845006306966, 0.04071768787172106, 0.014532758129967583, 0.04624784655041165, 0.025870437423388164, 0.012320079737239413, 0.021841148535410564, 0.011514945162667168, 0.030037681261698406, 0.03560731808344523, 0.040574209557639226, 0.032269173198276095, 0.016103088855743408, 0.022120801938904658, 0.03716591000556946, 0.03438838322957357, 0.06754732794231838], [0.03967180516984728, 0.03838346070713467, 0.038605564170413546, 0.038441098398632474, 0.037712004449632436, 0.03855018152130975, 0.03791578610738119, 0.03799865974320306, 0.03775824109713236, 0.03871489895714654, 0.03834716810120477, 0.03906841741667853, 0.03862009114689297, 0.038403458065456815, 0.03914999630716112, 0.037880384259753756, 0.03819367620680067, 0.038276606135898165, 0.03787835439046224, 0.03763011760181851, 0.037746293677224055, 0.03749468922615051, 0.037602742513020836, 0.03770981232325236, 0.03889671630329556, 0.03792349497477213, 0.038207401831944786, 0.03782779640621609, 0.03861432605319553, 0.038166155417760216, 0.03791265024079217, 0.03767377800411648, 0.03685733013682895, 0.037187563048468694, 0.03715344932344225, 0.037338339620166354, 0.03774159153302511, 0.03631144099765354, 0.03758050004641215, 0.036484605736202665, 0.034830593400531344, 0.038516481717427574, 0.03702866699960497, 0.036914179722468056, 0.036053968800438776, 0.034714718659718834, 0.03634396857685513, 0.03559748331705729, 0.03507192929585775, 0.034723404381010264, 0.07884875933329265, 0.03266070948706733, 0.03416856792238024, 0.03188710742526584, 0.03484254413180881, 0.027647132674853008, 0.03040055765046014, 0.029684050215615168, 0.029189387957255047, 0.0301844146516588, 0.02954420116212633, 0.032431840896606445, 0.02944903572400411, 0.03505421678225199, 0.03486824697918362, 0.032742977142333984, 0.04206321636835734, 0.02429995106326209, 0.026597685284084745, 0.024302701155344646, 0.022561505436897278, 0.03338660134209527, 0.02970535225338406, 0.03337576323085361, 0.020473917325337727, 0.02692325578795539, 0.030622455808851454, 0.0358774463335673, 0.0319677127732171, 0.023601570063167147, 0.033643398020002574, 0.029282487101025052, 0.020811075965563457, 0.03302818867895338, 0.020933437678549025, 0.02682764662636651, 0.025722139411502414, 0.03497547242376539, 0.03598835402064853, 0.037716819180382624, 0.03753196199735006, 0.041911307308408946, 0.020965543058183458, 0.03406874338785807, 0.018649175763130188, 0.034801893764071994, 0.034276763598124184, 0.02086809277534485, 0.03560520542992486, 0.028469360536999173, 0.027139061027103, 0.052511632442474365, 0.029236919350094266, 0.02676437298456828, 0.02821052074432373, 0.02593967318534851, 0.031053231822119817, 0.021261693702803716, 0.03201360503832499, 0.029255860381656222, 0.024939863218201533, 0.02958169248369005, 0.02555291685793135, 0.02055610219637553, 0.027472022506925795, 0.026854382620917425, 0.024345234036445618, 0.027306733859909907, 0.02898149357901679, 0.024616214964124892, 0.027504132853613958, 0.027972429990768433, 0.02977901366021898, 0.025229654378361173, 0.031234254439671833, 0.023394455512364704, 0.023867410090234544, 0.023180900348557368, 0.021948917044533625, 0.024824453724755183, 0.025183733966615465, 0.023242205381393433, 0.017857847942246333, 0.03423623243967692, 0.02358821862273746, 0.02024101714293162, 0.026791993114683364, 0.019155179460843403, 0.025793055693308514, 0.03873262140485975, 0.03466228644053141, 0.024563454919391207, 0.03573763039377001, 0.029684672753016155, 0.020162973139021132, 0.026443334089385137, 0.021138833628760442, 0.025602890385521784, 0.02942605482207404, 0.025871904359923467, 0.04210429059134589, 0.029359098937776353, 0.07340455717510647, 0.018677242928081088, 0.022192402018441096, 0.013213155998124016, 0.030109660493002996, 0.032555470863978066, 0.023561886615223356, 0.026724557081858318, 0.027488072713216145, 0.030222975545459323, 0.01402149928940667, 0.024533051583502028, 0.02720751530594296, 0.022952803307109408, 0.02387719021903144, 0.02366944154103597, 0.01526625951131185, 0.03071647220187717, 0.028511656655205622, 0.025199501050843134, 0.026293130384551153, 0.023217486010657415, 0.0328031943904029, 0.02073190112908681, 0.021855599350399442, 0.013733588986926608, 0.018095855911572773, 0.02719344695409139, 0.023845604724354215, 0.025006241268581815, 0.01422270139058431, 0.028008765644497342, 0.019653005732430354, 0.036320010821024575, 0.034393363528781466, 0.020934235718515184, 0.024032326208220586, 0.02390924592812856, 0.027533501386642456, 0.01268232199880812, 0.01613049374686347, 0.009685640533765158, 0.04631997810469733, 0.02307105561097463, 0.02230570051405165, 0.011638083391719393, 0.040449149078793, 0.03279162115520901, 0.014099140961964926, 0.023724911941422358, 0.012353252205583785, 0.044596334298451744, 0.03014861543973287, 0.04526636335584852, 0.022887398799260456, 0.009122507439719306, 0.017963220675786335, 0.021323111322191026, 0.01538468235068851, 0.00884040527873569, 0.011359992954466078, 0.04610235161251492, 0.01658371090888977, 0.01754062043295966, 0.030814952320522733, 0.032965107096566096, 0.024079841044214036, 0.029128286573621962, 0.018674060702323914, 0.02387042674753401, 0.02501679625776079, 0.027874612145953707, 0.03886281781726413, 0.03705570764011807, 0.007458686000770993, 0.01982903977235158, 0.007493988507323795, 0.028884977102279663, 0.03728759951061673, 0.02320813967121972, 0.0205646554629008, 0.023205290238062542, 0.020242364870177373, 0.01949341098467509, 0.038013325797186956, 0.026206894053353205, 0.027484594119919672, 0.025268334481451247, 0.01131417022811042, 0.02418751186794705, 0.024067262808481853, 0.02092808485031128, 0.01599795122941335, 0.033927463822894625, 0.02499558362695906, 0.009484952522648705, 0.016218051314353943, 0.018091784583197698, 0.02441056900554233, 0.012298814124531217, 0.041986925734413996, 0.044340948263804116, 0.08069351646635267], [0.039274791876475014, 0.038267148865593806, 0.038415104150772095, 0.038449363576041326, 0.0387572877936893, 0.038935042089886136, 0.03843163119422065, 0.038391444418165416, 0.03846596346961127, 0.03817250000105964, 0.03838230835066901, 0.03829539815584818, 0.03823293911086188, 0.03830119967460632, 0.0380511118306054, 0.038384738895628184, 0.03829892807536655, 0.03834905558162265, 0.038307627042134605, 0.03804698255327013, 0.037538349628448486, 0.03881838917732239, 0.03728315896458096, 0.03802163071102566, 0.03809205691019694, 0.03762774997287326, 0.03873251544104682, 0.037252343363232084, 0.03693870703379313, 0.037731448809305825, 0.037062889999813504, 0.03656481371985541, 0.0388409694035848, 0.03824722766876221, 0.03807961940765381, 0.03715349568261041, 0.03851771685812208, 0.03764294584592184, 0.035709652635786265, 0.03910514712333679, 0.035847872495651245, 0.03520155284139845, 0.03684207797050476, 0.03832762440045675, 0.037744502226511635, 0.03441275821791755, 0.03573168979750739, 0.034174948930740356, 0.0339887175295088, 0.0335630112224155, 0.06522235605451795, 0.03420408897929721, 0.03050257099999322, 0.033826066388024226, 0.02775294416480594, 0.0377215842405955, 0.0330313675933414, 0.03202947974205017, 0.030391686492496066, 0.03357577323913574, 0.02649799982706706, 0.022905526889695063, 0.030694418483310275, 0.030753311183717515, 0.0233471906847424, 0.03509578439924452, 0.040023207664489746, 0.029894487725363836, 0.029131515158547297, 0.026442327433162265, 0.038539475864834256, 0.02838137083583408, 0.036258578300476074, 0.03224987785021464, 0.02839661969078912, 0.03437128331926134, 0.03376567363739014, 0.03741819991005792, 0.03453141119745043, 0.026221896211306255, 0.03931891918182373, 0.027149370974964566, 0.029482427570554946, 0.02285908493730757, 0.03185416923628913, 0.02293363379107581, 0.025093706117735967, 0.02499259014924367, 0.023352338208092585, 0.034846107165018715, 0.028385536538230047, 0.022836016284094915, 0.028585745228661433, 0.0249670147895813, 0.02319038576549954, 0.031573086977005005, 0.02008953359391954, 0.02759253316455417, 0.03230690956115723, 0.021430464254485235, 0.03633818030357361, 0.040271355046166316, 0.026392421788639493, 0.028338313102722168, 0.01625094645553165, 0.02520047624905904, 0.02090260883172353, 0.017504508296648662, 0.02067668404844072, 0.02740444905228085, 0.029206030898623996, 0.02563529544406467, 0.02310902542538113, 0.019966805974642437, 0.03786615530649821, 0.02105730440881517, 0.02783580289946662, 0.02114064660337236, 0.030002726448906794, 0.024991706013679504, 0.02651025851567586, 0.03909207714928521, 0.02968801392449273, 0.025044581956333585, 0.01965378887123532, 0.02222506867514716, 0.020182882746060688, 0.01990038487646315, 0.021762910816404555, 0.033331967062420316, 0.028140849537319608, 0.015292594830195108, 0.021154888802104525, 0.013765286240312789, 0.023142584496074252, 0.035380433003107704, 0.026029633151160345, 0.02903177671962314, 0.028470115529166326, 0.02105110718144311, 0.03406290875540839, 0.03404327233632406, 0.017174442609151203, 0.039185802141825356, 0.02630982630782657, 0.03374962343109979, 0.02001394165886773, 0.03036009603076511, 0.02794757816526625, 0.02119968334833781, 0.027817202938927546, 0.03337559103965759, 0.06302134195963542, 0.020822798212369282, 0.0200010703669654, 0.014151251978344388, 0.02515147129694621, 0.017469738920529682, 0.02951781948407491, 0.023445632722642686, 0.0263863785399331, 0.030979828702078924, 0.021041148238711886, 0.020110666751861572, 0.019254513912730746, 0.02070567674107022, 0.013775810599327087, 0.024119332432746887, 0.017145595616764493, 0.03266462352540758, 0.015100512239668105, 0.022118106484413147, 0.0310061342186398, 0.015801688035329182, 0.031957268714904785, 0.02038818597793579, 0.029706574148601957, 0.02712683876355489, 0.017693936824798584, 0.023564583725399442, 0.021503478288650513, 0.02374336454603407, 0.024755216307110257, 0.020708959963586595, 0.029041654533810086, 0.02469704548517863, 0.027396008372306824, 0.0305507845348782, 0.023254210750261944, 0.01627038584815131, 0.032626854048834905, 0.01955012811554803, 0.017322608166270785, 0.029215961694717407, 0.019979288180669148, 0.024546785487069026, 0.029447048902511597, 0.03150956498252021, 0.028683427307340834, 0.02728654444217682, 0.019548694292704265, 0.022426343626446195, 0.024188607931137085, 0.04039567046695285, 0.025415614247322083, 0.02305839459101359, 0.03108782238430447, 0.021429301963912115, 0.02878809968630473, 0.034918771849738225, 0.015668421983718872, 0.020481697387165494, 0.015931357940038044, 0.027722623613145616, 0.027202506860097248, 0.008952021598815918, 0.016721429096327886, 0.021613210439682007, 0.03388749228583442, 0.031007442209455702, 0.026277129848798115, 0.009449294871754117, 0.015950229432847764, 0.025809200273619756, 0.01914441419972314, 0.03130185604095459, 0.012416290740172068, 0.017899529801474676, 0.026112465394867793, 0.018059308330218, 0.020484776960478887, 0.02831760048866272, 0.01645649141735501, 0.012829555405510796, 0.022676135102907818, 0.03276142146852282, 0.015167527728610568, 0.022437956598069932, 0.018962653146849737, 0.022023008929358587, 0.03100777996910943, 0.031664868195851646, 0.012982173098458184, 0.021590077214770846, 0.02019300063451131, 0.01712002356847127, 0.0491429070631663, 0.02115928630034129, 0.027903321716520522, 0.017617500490612455, 0.024083299769295588, 0.021962727109591167, 0.02499326401286655, 0.032528463337156505, 0.027936240037282307], [0.03819356030888028, 0.03827515575620863, 0.036501785119374595, 0.037018156713909574, 0.035399532980389066, 0.03892890281147427, 0.04061954551272922, 0.03635150194168091, 0.03981962468889025, 0.03813728027873569, 0.03827461269166735, 0.03987869620323181, 0.037334190474616155, 0.03709503014882406, 0.03784797920121087, 0.036726954910490245, 0.03794787327448527, 0.038767864306767784, 0.03790687852435642, 0.03666931390762329, 0.038611789544423424, 0.039108597569995456, 0.035172743929757014, 0.03918173246913486, 0.03829151391983032, 0.03855199615160624, 0.035689602295557656, 0.038289775451024376, 0.03845330079396566, 0.036415007379319936, 0.03841128283076816, 0.03725950585471259, 0.03861534926626417, 0.03581184148788452, 0.03612108031908671, 0.03762557771470812, 0.03703595532311334, 0.03693450159496731, 0.036905275450812444, 0.036332382096184626, 0.03622216317388746, 0.03654869066344367, 0.035376141468683876, 0.03521664606200324, 0.03534373972151014, 0.03580464588271247, 0.035281595256593495, 0.03329699238141378, 0.035411443975236684, 0.03659681479136149, 0.0743574168947008, 0.03382946385277642, 0.03326220313707987, 0.03147270282109579, 0.03245052033000522, 0.03504619664616055, 0.030263248417112563, 0.028090106116400823, 0.035224358240763344, 0.02752366993162367, 0.027822673320770264, 0.0322881473435296, 0.02840954727596707, 0.02601434290409088, 0.026862468984391954, 0.03185789783795675, 0.0391236576769087, 0.03678393363952637, 0.023691945605807833, 0.03447801205846998, 0.027474638488557603, 0.041979975170559354, 0.03711659047338697, 0.030205839210086398, 0.03496462768978543, 0.0351006719801161, 0.03464069962501526, 0.031736393769582115, 0.023754994074503582, 0.03773118389977349, 0.036344571246041193, 0.02855254875289069, 0.030542360411749944, 0.027127655016051397, 0.030320578151279025, 0.027979516320758395, 0.030539780855178833, 0.017727507485283747, 0.030712750222947862, 0.031866245799594455, 0.029847509331173368, 0.02766697771019406, 0.029864821169111464, 0.025662150647905137, 0.027943008475833468, 0.028358581993314955, 0.023226598898569744, 0.027547816435496014, 0.036100288232167564, 0.04103655285305447, 0.0329687860276964, 0.03201605876286825, 0.016530166069666546, 0.024689435958862305, 0.027007506953345403, 0.0310688316822052, 0.029307583967844646, 0.016361784603860643, 0.03410043981340197, 0.020802529321776494, 0.031165344847573176, 0.026215602954228718, 0.03048687842157152, 0.030828134881125555, 0.02274553312195672, 0.038807325892978244, 0.034955104192097984, 0.030249555905659992, 0.023898591597874958, 0.02024296588367886, 0.029990126689275105, 0.030576063527001276, 0.024894487526681688, 0.03009253740310669, 0.019951944549878437, 0.042967783080206975, 0.02463829020659129, 0.026835329002804227, 0.036655147870381675, 0.024097793632083468, 0.030721038579940796, 0.022147885627216764, 0.025531444284651015, 0.01819337573316362, 0.02627147568596734, 0.022557901011572942, 0.023417519198523626, 0.03460330433315701, 0.021571980582343206, 0.02198701600233714, 0.023854686154259577, 0.02702214651637607, 0.023899361491203308, 0.024954731265703838, 0.029658214913474187, 0.024442811806996662, 0.03325786524348789, 0.025021311309602525, 0.04048262702094184, 0.026481287346945867, 0.01556660897201962, 0.03557880719502767, 0.06266525056627062, 0.031216674380832247, 0.024055911435021296, 0.022892711891068354, 0.020864577756987676, 0.018477790885501437, 0.024942624900076125, 0.020917422241634794, 0.03202603260676066, 0.028850790527131822, 0.02440222932232751, 0.021674712498982746, 0.02487624188264211, 0.0351803633901808, 0.018661452664269343, 0.02898065249125163, 0.014309558603498671, 0.01577850017282698, 0.018720045685768127, 0.025171665681733027, 0.029236743847529095, 0.0292538669374254, 0.025472564829720393, 0.030119329690933228, 0.012747872206899855, 0.020200666454103258, 0.02827143006854587, 0.038715726799435086, 0.019266080525186326, 0.03175496061642965, 0.023051538401179843, 0.03149043851428562, 0.026570422781838313, 0.012991312477323744, 0.014881602591938443, 0.023962005972862244, 0.023855917983584933, 0.01078615254826016, 0.02449835009045071, 0.03226422601275974, 0.01202000594801373, 0.024113607075479295, 0.01808001266585456, 0.023146127661069233, 0.015573357542355856, 0.033572226762771606, 0.03490682442982992, 0.025401373704274494, 0.023655220866203308, 0.03604966070916918, 0.04096905390421549, 0.025766099492708843, 0.0333077245288425, 0.017435158292452495, 0.01370837539434433, 0.013154421415593889, 0.028026617235607572, 0.02117905682987637, 0.02819437450832791, 0.015452648202578226, 0.013401321238941617, 0.019968857367833454, 0.019215822219848633, 0.023301063312424555, 0.028322279453277588, 0.01648793617884318, 0.02070882585313585, 0.008671920332643721, 0.03261115815904406, 0.0132834083504147, 0.03229901194572449, 0.02222625083393521, 0.008850915564431084, 0.015526036421457926, 0.018195761574639216, 0.019242617819044325, 0.0173962050014072, 0.008663679162661234, 0.01995253066221873, 0.04318187965287103, 0.03680261969566345, 0.019528471761279635, 0.01717695759402381, 0.041185862488216825, 0.020039487216207717, 0.0195858230193456, 0.014621426661809286, 0.027437705132696364, 0.028925902313656278, 0.02108833028210534, 0.024550856815444097, 0.02105860908826192, 0.02166719569100274, 0.03891580965783861, 0.020235659347640142, 0.023742177420192294, 0.006799512439303928, 0.01782259013917711, 0.03380483720037672, 0.015597566962242126, 0.02867837415801154, 0.040488766299353704, 0.07892394728130764], [0.03883552551269531, 0.038592527310053505, 0.03855791687965393, 0.03837724526723226, 0.03830587863922119, 0.03752591212590536, 0.03860211372375488, 0.037467108832465276, 0.03858439458741082, 0.038416544596354164, 0.039346264468299016, 0.04002203212844001, 0.03976199362013075, 0.03772661752170987, 0.03984170820977953, 0.03713742560810513, 0.038065565956963435, 0.036655558480156794, 0.037324878904554576, 0.03643744521670871, 0.03747393025292291, 0.03814815481503805, 0.03956795732180277, 0.03632992837164137, 0.04027801089816623, 0.03902063104841444, 0.036348813109927706, 0.03926331798235575, 0.03720788160959879, 0.038189205858442515, 0.03715784351030985, 0.035422689384884305, 0.03736319475703769, 0.03529808256361219, 0.03951402505238851, 0.033720817830827504, 0.04148119025760227, 0.036436120669047035, 0.03796823488341437, 0.03762029608090719, 0.037155098385281034, 0.0386436382929484, 0.0336495836575826, 0.036693294843037925, 0.03648553623093499, 0.03809511661529541, 0.035782953103383384, 0.03562485178311666, 0.038970881038241915, 0.037615315781699285, 0.06927912765079075, 0.03167468971676297, 0.035153465138541326, 0.037155869934293956, 0.03244952360788981, 0.035480754243002996, 0.032305929395887584, 0.03367950187789069, 0.03574043181207445, 0.03162464830610487, 0.03258499171998766, 0.030916492144266765, 0.03608052929242452, 0.036012603176964655, 0.029328058163324993, 0.035597903860939875, 0.029754367139604356, 0.024498547116915386, 0.032931003305647105, 0.03001317712995741, 0.03843969106674194, 0.03239659468332926, 0.03953744636641608, 0.028500003947152033, 0.02707043124569787, 0.02779535452524821, 0.030730260743035212, 0.026966763867272273, 0.02143075896633996, 0.03310019771258036, 0.028462737798690796, 0.032465835412343345, 0.03207942181163364, 0.027653276920318604, 0.027294443713294134, 0.029285328255759344, 0.035398506455951266, 0.031179987721972995, 0.03253262903955248, 0.031956384579340615, 0.04116921623547872, 0.0210572663280699, 0.02266750733057658, 0.02602376209364997, 0.025163233280181885, 0.025302345554033916, 0.027996152639389038, 0.03008300397131178, 0.027157987157503765, 0.024163610405392118, 0.021308657195832994, 0.04938434229956733, 0.02699239220884111, 0.02729106280538771, 0.024580139252874587, 0.030043615235222712, 0.026672446065478854, 0.03682503435346815, 0.027876916858885024, 0.029247734281751845, 0.02401735054122077, 0.02457123001416524, 0.021448190013567608, 0.021104180150561862, 0.029092411200205486, 0.01400504344039493, 0.02380946609708998, 0.022911609874831304, 0.025437590148713853, 0.02639834748374091, 0.016377932495541044, 0.04447199238671197, 0.02153064144982232, 0.02582687470648024, 0.024413910177018907, 0.023420812355147466, 0.011118385526869033, 0.026615483893288508, 0.03488559524218241, 0.032250788476732045, 0.022963348362180922, 0.02032977839310964, 0.02025819652610355, 0.0355891916486952, 0.03432966272036234, 0.030533330308066473, 0.0337287982304891, 0.03369889656702677, 0.04769754078653124, 0.019734483626153734, 0.03043301569090949, 0.02489195101790958, 0.019322888718711004, 0.024582965506447688, 0.028545611434512667, 0.017686216367615595, 0.01078578746981091, 0.02770892447895474, 0.022611796855926514, 0.024572961860232882, 0.024919365843137104, 0.018992639250225492, 0.02149976458814409, 0.026514651046858892, 0.04187452793121338, 0.027268456088172063, 0.018534758024745517, 0.033428056372536555, 0.01671632793214586, 0.024441083272298176, 0.01787387662463718, 0.023169168167644076, 0.029042154550552368, 0.0337654087278578, 0.028645296891530354, 0.019082350863350764, 0.023041930463578966, 0.031175875001483493, 0.03881394863128662, 0.023266777396202087, 0.03203475475311279, 0.021134618255827162, 0.025677045186360676, 0.017548991574181452, 0.018873598840501573, 0.011677972972393036, 0.017886282669173345, 0.02648122443093194, 0.029404150115119085, 0.03372363249460856, 0.019417938258912828, 0.012685409022702111, 0.027571060591273837, 0.024356368515226576, 0.021987496150864497, 0.018289979961183336, 0.016623104612032574, 0.022754568192693923, 0.01779836416244507, 0.022948058115111455, 0.01751234961880578, 0.025167564551035564, 0.025285282068782382, 0.010943152010440826, 0.03184345364570618, 0.03746019469367133, 0.02383865581618415, 0.023840801583396062, 0.02375810510582394, 0.016355522804790072, 0.022341627213690016, 0.016852969924608868, 0.0387993421819475, 0.02066738572385576, 0.027916053930918377, 0.014143935508198209, 0.01168299631939994, 0.028894970814387005, 0.03359502553939819, 0.018729031085968018, 0.03672841522428724, 0.024039649301105075, 0.021644373734792072, 0.02694151798884074, 0.031019422743055556, 0.03373746077219645, 0.02905815177493625, 0.015838980674743652, 0.017495587468147278, 0.03506843911276923, 0.013137925002310012, 0.029836012257470026, 0.01893280777666304, 0.024789757198757596, 0.016775412691964045, 0.010659032397800021, 0.015888551870981853, 0.01788141330083211, 0.025200193126996357, 0.017569879690806072, 0.014816724591785006, 0.02862743867768182, 0.027224628461731806, 0.02195009920332167, 0.014551783601442972, 0.025894070665041607, 0.02104681067996555, 0.023112389776441786, 0.02747420635488298, 0.018633008003234863, 0.029227399163775973, 0.021260299616389804, 0.016280025243759155, 0.021135350068410236, 0.01015708264377382, 0.04760910073916117, 0.019139554765489366, 0.01924819913175371, 0.016527311669455633, 0.02603901094860501, 0.019704823692639668, 0.02349951697720422, 0.024480592873361375, 0.021454867389467027, 0.0537653730975257], [0.03787830471992493, 0.03768788443671332, 0.037126352389653526, 0.037513252761628896, 0.0382707417011261, 0.03853840629259745, 0.03562697105937534, 0.04074674844741821, 0.036920375294155545, 0.03539795676867167, 0.03709383805592855, 0.036039110687043935, 0.03934053911103143, 0.0391559104124705, 0.03793632984161377, 0.0380392869313558, 0.03922496239344279, 0.04230327076382107, 0.034974233971701726, 0.038577410909864635, 0.03664645883772108, 0.04020561112297906, 0.037923985057406955, 0.03908538818359375, 0.03667787379688687, 0.03660183151563009, 0.039911617835362755, 0.037812928358713783, 0.03778731822967529, 0.03823925389183892, 0.03807776504092746, 0.03640410304069519, 0.03843572404649523, 0.03550801012251112, 0.03683392206827799, 0.036470383405685425, 0.03512528207567003, 0.036375194787979126, 0.03676074743270874, 0.03508517146110535, 0.037295252084732056, 0.03994609912236532, 0.03325730893346998, 0.03619358936945597, 0.037708265913857356, 0.03573051757282681, 0.03281593322753906, 0.035509745279947914, 0.037311550643708974, 0.03559608923064338, 0.057885169982910156, 0.03220318092240228, 0.03410312864515516, 0.030275235573450725, 0.033671021461486816, 0.03390181064605713, 0.03661212987369961, 0.029325816366407607, 0.030674212508731417, 0.027294246686829463, 0.02544552253352271, 0.029793792300754122, 0.028884145948621962, 0.033792843421300255, 0.03289611803160773, 0.019117282496558294, 0.021595809194776747, 0.02249048153559367, 0.018478482961654663, 0.028036346038182575, 0.03605017397138807, 0.05229055219226413, 0.038355761104159884, 0.023614370160632663, 0.036945114533106484, 0.03257402777671814, 0.018969486157099407, 0.016130901045269437, 0.023193192150857713, 0.02036377125316196, 0.03038439154624939, 0.02858591079711914, 0.02960733241505093, 0.03812473350101047, 0.028514954778883193, 0.027321880062421162, 0.029766105943255954, 0.033516125546561346, 0.041572117143207125, 0.02380656533771091, 0.026317258675893147, 0.026482199629147846, 0.044043156835767955, 0.03367770380443997, 0.03018138474888272, 0.024618208408355713, 0.051805850532319814, 0.04049373004171583, 0.04900067382388645, 0.024237655931048922, 0.035606718725628324, 0.021419849660661485, 0.020737096667289734, 0.0222216977013482, 0.02991409765349494, 0.027444798085424636, 0.04103930128945245, 0.0167983074982961, 0.03222809235254923, 0.03625342912144131, 0.01684305237399207, 0.0332112279203203, 0.02523861328760783, 0.02662573092513614, 0.022762628065215215, 0.021895019544495478, 0.03288735946019491, 0.02594587869114346, 0.033513668510648936, 0.026378336879942153, 0.033256855275895864, 0.03473919298913744, 0.019032830993334453, 0.026489579015307956, 0.023760947916242812, 0.021679055359628465, 0.02245498365826077, 0.03240854210323758, 0.051643441120783486, 0.018490465150939092, 0.01909518904156155, 0.025706668694814045, 0.03552972939279345, 0.020864973465601604, 0.024512166778246563, 0.021648395392629836, 0.0270874732070499, 0.021118753486209445, 0.023084902101092868, 0.03559673163625929, 0.03090324666765001, 0.026495433515972562, 0.028813461462656658, 0.025727848211924236, 0.02305894096692403, 0.029525273376040988, 0.032632950279447764, 0.014229890373018052, 0.0245097643799252, 0.028641018602583144, 0.025113920370737713, 0.01740443706512451, 0.034576346476872764, 0.028235766622755263, 0.02729301154613495, 0.02087826861275567, 0.030207385619481403, 0.012081802719169192, 0.025208241409725614, 0.024386670854356553, 0.02761730717288123, 0.028645101520750258, 0.04260360532336765, 0.022229067153400846, 0.014410383171505399, 0.01739815870920817, 0.03671562340524462, 0.027609283725420635, 0.046980977058410645, 0.01788751780986786, 0.026409867737028334, 0.02673960394329495, 0.029282712274127536, 0.025402545928955078, 0.02855075730217828, 0.03504984908633762, 0.02397115197446611, 0.029826379484600492, 0.023015911380449932, 0.027722908390892878, 0.024714690115716722, 0.027457608116997614, 0.026395759648746915, 0.02805307838651869, 0.017761740419599745, 0.02978672252760993, 0.029964596033096313, 0.02592829366525014, 0.02417533430788252, 0.021852236655023363, 0.027231723070144653, 0.02220879660712348, 0.024554711249139573, 0.023121681478288438, 0.020726799964904785, 0.01821252206961314, 0.03324626220597161, 0.020091561807526484, 0.019346614678700764, 0.035597794585757785, 0.015486485428280301, 0.021826407975620694, 0.027327307396464877, 0.059909568892584905, 0.0242931991815567, 0.012670973108874427, 0.014088897241486443, 0.026663409339057073, 0.03491027818785773, 0.020170388950241938, 0.01522735423511929, 0.019464805722236633, 0.0261827376153734, 0.019881440533532038, 0.031708995501200356, 0.0248806807729933, 0.032487720251083374, 0.012524435917536417, 0.030032313532299466, 0.02709624171257019, 0.027341865830951266, 0.01594956053627862, 0.026609768470128376, 0.03517821762296888, 0.015582048230701022, 0.021771427657869127, 0.02069615986612108, 0.014190253284242418, 0.024027776387002733, 0.028878258334265813, 0.03173916207419501, 0.02723602619436052, 0.013057289024194082, 0.012736761735545265, 0.02856290340423584, 0.02286508513821496, 0.04897626241048177, 0.02289072010252211, 0.025677051809098985, 0.030557122495439317, 0.01749776966041989, 0.025332454178068373, 0.016886757479773626, 0.0263151443666882, 0.03455612394544813, 0.02468056811226739, 0.014100634389453463, 0.01417257719569736, 0.011965125799179077, 0.020869723624653287, 0.018248215317726135, 0.014088335964414809, 0.02922730975680881, 0.01673370599746704, 0.04926301704512702], [0.038390421205096774, 0.03860914707183838, 0.03869307372305128, 0.03866280118624369, 0.038159824079937406, 0.03816428780555725, 0.03866993718677097, 0.03752890229225159, 0.03847026493814257, 0.038252545727623835, 0.0384525822268592, 0.038421554697884455, 0.038080202208624944, 0.039398243029912315, 0.037371383772956, 0.03787790073288812, 0.038039041890038386, 0.039010961850484215, 0.037491434150271945, 0.037586063146591187, 0.0378737383418613, 0.039051185051600136, 0.03773639268345303, 0.037129600842793785, 0.03675469756126404, 0.03698539733886719, 0.03617239660686917, 0.03563494814766778, 0.03416093852784899, 0.03502179516686334, 0.04120223389731513, 0.037270569139056735, 0.04254228207800123, 0.036602778567208186, 0.0384131305747562, 0.03576517105102539, 0.03350057204564413, 0.03666741649309794, 0.0406805674235026, 0.036220408148235746, 0.039301402038998075, 0.03815526432461209, 0.03616125716103448, 0.03399170438448588, 0.03706746300061544, 0.03567509518729316, 0.03573977616098192, 0.03426259093814426, 0.03433960345056322, 0.037457562155193753, 0.07047840621736315, 0.033075975047217474, 0.03076751364601983, 0.036199986934661865, 0.029281225469377305, 0.0332733690738678, 0.0369096232785119, 0.0328090058432685, 0.034570386012395225, 0.024402065409554377, 0.02833848198254903, 0.025657100809945002, 0.028757698006100126, 0.03325056698587206, 0.03457148869832357, 0.03626691301663717, 0.025025318066279095, 0.03125681479771932, 0.037946850061416626, 0.031885766320758395, 0.03393504354688856, 0.029464039537641738, 0.024884288509686787, 0.04243323869175381, 0.02463587290710873, 0.028297745519214206, 0.024419814348220825, 0.027191170387797885, 0.0290480719672309, 0.027622911665174697, 0.03264486458566454, 0.02567525042427911, 0.024981957342889573, 0.029933899641036987, 0.027742647462420993, 0.025540384981367324, 0.028064784076478746, 0.03304435809453329, 0.028962171740002103, 0.027887781461079914, 0.024577207035488553, 0.03272622161441379, 0.029168632295396592, 0.022064256999227736, 0.03608912560674879, 0.023914663328064814, 0.022757301727930706, 0.03204226493835449, 0.033806307448281184, 0.027104528413878545, 0.026447870665126376, 0.059800843397776283, 0.036297904120551214, 0.020436565081278484, 0.02894982033305698, 0.016870856285095215, 0.018023340238465205, 0.020510777831077576, 0.026508748531341553, 0.033215453227361046, 0.02431609895494249, 0.029272278149922688, 0.04921822084320916, 0.020272261566585965, 0.02496336731645796, 0.02089581886927287, 0.0305909083949195, 0.03460291028022766, 0.03375824623637729, 0.028057210975223117, 0.03877361284361945, 0.013518128958013322, 0.026043143537309434, 0.034387161334355675, 0.026104191939036053, 0.020837008953094482, 0.031402124298943415, 0.02794325351715088, 0.026394744714101154, 0.017729866835806105, 0.027408788601557415, 0.02831526266203986, 0.023804055319892034, 0.019762602117326524, 0.029829243818918865, 0.022527673178248935, 0.02787148952484131, 0.01700424485736423, 0.019924783044391207, 0.02404121392303043, 0.028974668847190008, 0.01739377114507887, 0.023196486963166132, 0.02199469010035197, 0.019654148154788546, 0.034900274541642934, 0.016061524550120037, 0.020781180924839444, 0.019591338104671903, 0.027539907230271235, 0.02152660820219252, 0.030760258436203003, 0.02840999762217204, 0.02261929214000702, 0.038588245709737144, 0.023751881387498643, 0.02248233722315894, 0.03493230872684055, 0.01532179613908132, 0.02262534697850545, 0.022686911953820124, 0.024332717061042786, 0.021481772263844807, 0.02145964072810279, 0.027312237355444167, 0.026232442922062345, 0.02460687193605635, 0.03577359186278449, 0.020059294170803495, 0.019977619250615437, 0.020071226689550612, 0.020559602313571505, 0.02934911184840732, 0.02907369203037686, 0.04842282666100396, 0.026213021741973028, 0.020383839805920918, 0.042597277296913996, 0.01833424965540568, 0.025303799245092604, 0.03652984897295634, 0.025524692402945623, 0.029153982798258465, 0.031199478440814547, 0.023782784740130108, 0.022971433069970872, 0.030863831440607708, 0.024961244728830125, 0.0221153414911694, 0.03356980284055074, 0.015411580602327982, 0.02173522611459096, 0.018378222982088726, 0.014615727795494927, 0.024853358666102093, 0.025081611341900296, 0.03140487273534139, 0.029028505086898804, 0.012478773792584738, 0.01294569174448649, 0.026609732045067683, 0.019191592931747437, 0.01146786328819063, 0.03715918130344815, 0.03332148657904731, 0.01927077935801612, 0.01740729312102, 0.02667458520995246, 0.02964208523432414, 0.037771407100889415, 0.020934457580248516, 0.019140375985039607, 0.016919965545336407, 0.0190387186076906, 0.019218140178256564, 0.03441759943962097, 0.022674192984898884, 0.01944084134366777, 0.02068252530362871, 0.02041106919447581, 0.03903753227657742, 0.025892987847328186, 0.007874694135453966, 0.014065227574772306, 0.017397529549068876, 0.026532320512665644, 0.019089985224935744, 0.027307742171817355, 0.027048988474739924, 0.015017698208491007, 0.03586255179511176, 0.008485462930467393, 0.01423777143160502, 0.03572923276159498, 0.026149613989724055, 0.033077650600009494, 0.012221158378654055, 0.02372875644101037, 0.015256184670660231, 0.031205008427302044, 0.01585666338602702, 0.02151644726594289, 0.027060273620817397, 0.024205586976475187, 0.01810223360856374, 0.04519166217909919, 0.03157190481821696, 0.02592672242058648, 0.028897762298583984, 0.013210285041067336, 0.02268839379151662, 0.023840730388959248, 0.015485563211970858, 0.021846195062001545, 0.03565090232425266], [0.038497477769851685, 0.03842987285719977, 0.03850777943929037, 0.03838075200716654, 0.03816076119740804, 0.03820362355973986, 0.037835160891215004, 0.03768460618125068, 0.037849611706203885, 0.039976663059658475, 0.03861238559087118, 0.03734084301524692, 0.03765058517456055, 0.037437008486853704, 0.037346604797575206, 0.0412035518222385, 0.03955253627565172, 0.036578088998794556, 0.03848625553978814, 0.03504013352923923, 0.037735561529795326, 0.03788794080416361, 0.03884641991721259, 0.03704248203171624, 0.03761835892995199, 0.03962815801302592, 0.03848924239476522, 0.03929400444030762, 0.038143419557147555, 0.037864741351869374, 0.036252458890279136, 0.037699735826916166, 0.037899649805492826, 0.03645496235953437, 0.039139121770858765, 0.0359115203221639, 0.03495067358016968, 0.03630917602115207, 0.037149240573247276, 0.036848869588640004, 0.037120077345106334, 0.036104957262674965, 0.036644293202294245, 0.03678096996413337, 0.03695226709047953, 0.0342637300491333, 0.03501452671156989, 0.03567643960316976, 0.034218314621183604, 0.035506377617518105, 0.06291417280832927, 0.031746109326680504, 0.03152100576294793, 0.02812472979227702, 0.03235279520352682, 0.032448768615722656, 0.02874698241551717, 0.02791789174079895, 0.02572674552599589, 0.032344152530034385, 0.027984599272410076, 0.02734930978880988, 0.03734352853563097, 0.03716042968961927, 0.02797465191947089, 0.02553040285905202, 0.038799289200041026, 0.03167130880885654, 0.024693560269143846, 0.01765232781569163, 0.04272354973687066, 0.03089037537574768, 0.02959152725007799, 0.022028414739502802, 0.03393636809455024, 0.03513472941186693, 0.03916874196794298, 0.02620414396127065, 0.032827400498920016, 0.030358450280295476, 0.017758952246771917, 0.022185689873165555, 0.03339621755811903, 0.023520251115163166, 0.025645777583122253, 0.021238989300198026, 0.02051604125234816, 0.03063629733191596, 0.02799139420191447, 0.02815884682867262, 0.025261133909225464, 0.026489175028271146, 0.0316823058658176, 0.028629193703333538, 0.03112624420060052, 0.023188907239172194, 0.028740896119011775, 0.02346272932158576, 0.034266902340783015, 0.020835081736246746, 0.025258820917871263, 0.03731975290510389, 0.016283536950747173, 0.03466757469707065, 0.024330336186620925, 0.019074868824746873, 0.024833834833568998, 0.03201514482498169, 0.04470629824532403, 0.03988970319430033, 0.01709987223148346, 0.018229559063911438, 0.020708117220136855, 0.022979733016755845, 0.03668273819817437, 0.03555128309461805, 0.03612182206577725, 0.019917378822962444, 0.012948668665356107, 0.026951548126008775, 0.02308335734738244, 0.03644222683376736, 0.035440415143966675, 0.015463509493403964, 0.028982232014338177, 0.02425750924481286, 0.024808600544929504, 0.022938316067059834, 0.033621092637379967, 0.026508637600474887, 0.017596708403693304, 0.017826881673600938, 0.016673122843106587, 0.033120950063069664, 0.02441968520482381, 0.03183587723308139, 0.02029328876071506, 0.017730716201994155, 0.02038134965631697, 0.024326360887951322, 0.04546027382214864, 0.014701753854751587, 0.0258166061507331, 0.02602088948090871, 0.023116581969790988, 0.022095029552777607, 0.023619241184658475, 0.01618807680077023, 0.032646881209479436, 0.025428218974007502, 0.03690854708353678, 0.01733116971121894, 0.031123061974843342, 0.028650217586093478, 0.016262253125508625, 0.040112349722120494, 0.024284491936365765, 0.02260565095477634, 0.028830084535810683, 0.02435489661163754, 0.016128006908628676, 0.030565126074684992, 0.018709060218599107, 0.028131524721781414, 0.01669410698943668, 0.02004420260588328, 0.02066581944624583, 0.016963183879852295, 0.029239217440287273, 0.021650509701834783, 0.024346267183621723, 0.016081882847679987, 0.020198245843251545, 0.02519778245025211, 0.018986390696631536, 0.020427611139085557, 0.017311851183573406, 0.025283147891362507, 0.03678468863169352, 0.022656821542316012, 0.018372462855445013, 0.02350959016217126, 0.021638163261943393, 0.028764476378758747, 0.029141197601954143, 0.025927879744105868, 0.028533021608988445, 0.03451878163549635, 0.032525728146235146, 0.032731751600901283, 0.025648176670074463, 0.010068022542529635, 0.028227008051342435, 0.028016951349046495, 0.03615898225042555, 0.02928072876400418, 0.022364737259017095, 0.042081442144181996, 0.02723332742849986, 0.01615153584215376, 0.030233287149005465, 0.02582762638727824, 0.01885364121860928, 0.06276116106245253, 0.01910453538099925, 0.025006473064422607, 0.020760450098249648, 0.01445434656408098, 0.011609667705165016, 0.02218195630444421, 0.022035022576649983, 0.03189358446333143, 0.013885043561458588, 0.024242295159233943, 0.017101625601450603, 0.017219833201832242, 0.01347380793756909, 0.02361620631482866, 0.03255683183670044, 0.04074921541743808, 0.02673379249042935, 0.027095764875411987, 0.023658384879430134, 0.03183731105592516, 0.03013082676463657, 0.03600545061959161, 0.01855187283621894, 0.023271742794248793, 0.016567129227850173, 0.016583083404435053, 0.0332940055264367, 0.027722208036316767, 0.02053757177458869, 0.010615372823344337, 0.03089335560798645, 0.012948751449584961, 0.01572018199496799, 0.026648783021503024, 0.01330195532904731, 0.022727987832493253, 0.017351844244533114, 0.020087912678718567, 0.02882992559009128, 0.02075975305504269, 0.05523387259907193, 0.022803185714615717, 0.025597640209727816, 0.018190428614616394, 0.023554820153448317, 0.03298601839277479, 0.03429917494455973, 0.016237773829036288, 0.024079945352342393, 0.015177182025379606, 0.015882311595810786]], 'accuracy_validation': [[0.4473684210526316, 0.4473684210526316, 0.4473684210526316, 0.5263157894736842, 0.5394736842105263, 0.5526315789473685, 0.5526315789473685, 0.5526315789473685, 0.5526315789473685, 0.5526315789473685, 0.5526315789473685, 0.5526315789473685, 0.5526315789473685, 0.5526315789473685, 0.5526315789473685, 0.5526315789473685, 0.5526315789473685, 0.5526315789473685, 0.5526315789473685, 0.5526315789473685, 0.5526315789473685, 0.5526315789473685, 0.5526315789473685, 0.5526315789473685, 0.5526315789473685, 0.5526315789473685, 0.5526315789473685, 0.5526315789473685, 0.5526315789473685, 0.5526315789473685, 0.5526315789473685, 0.5526315789473685, 0.5526315789473685, 0.5526315789473685, 0.5570175438596491, 0.5570175438596491, 0.5570175438596491, 0.5570175438596491, 0.5614035087719298, 0.5745614035087719, 0.5877192982456141, 0.5921052631578947, 0.6271929824561403, 0.6578947368421053, 0.7105263157894737, 0.7324561403508771, 0.7543859649122807, 0.7631578947368421, 0.7587719298245614, 0.7587719298245614, 0.7587719298245614, 0.75, 0.7456140350877193, 0.7456140350877193, 0.7368421052631579, 0.75, 0.7543859649122807, 0.75, 0.7368421052631579, 0.7105263157894737, 0.6754385964912281, 0.6491228070175439, 0.6491228070175439, 0.6710526315789473, 0.706140350877193, 0.7149122807017544, 0.7324561403508771, 0.75, 0.7587719298245614, 0.7631578947368421, 0.7719298245614035, 0.7807017543859649, 0.7675438596491229, 0.7631578947368421, 0.7631578947368421, 0.7719298245614035, 0.7675438596491229, 0.7719298245614035, 0.7807017543859649, 0.7719298245614035, 0.7719298245614035, 0.7543859649122807, 0.7587719298245614, 0.7631578947368421, 0.7675438596491229, 0.7763157894736842, 0.7894736842105263, 0.7850877192982456, 0.7982456140350878, 0.7894736842105263, 0.7982456140350878, 0.7850877192982456, 0.7894736842105263, 0.7719298245614035, 0.7719298245614035, 0.7763157894736842, 0.7763157894736842, 0.7807017543859649, 0.7894736842105263, 0.8026315789473685, 0.8114035087719298, 0.8157894736842105, 0.8201754385964912, 0.7982456140350878, 0.7982456140350878, 0.7982456140350878, 0.8026315789473685, 0.8026315789473685, 0.8026315789473685, 0.7894736842105263, 0.7807017543859649, 0.7719298245614035, 0.7631578947368421, 0.7631578947368421, 0.7631578947368421, 0.7763157894736842, 0.7894736842105263, 0.7982456140350878, 0.8157894736842105, 0.8157894736842105, 0.8333333333333334, 0.8333333333333334, 0.8421052631578947, 0.8421052631578947, 0.8377192982456141, 0.8508771929824561, 0.8377192982456141, 0.8289473684210527, 0.8157894736842105, 0.8245614035087719, 0.8333333333333334, 0.8289473684210527, 0.8289473684210527, 0.8289473684210527, 0.8333333333333334, 0.8333333333333334, 0.8289473684210527, 0.8333333333333334, 0.8201754385964912, 0.8245614035087719, 0.8377192982456141, 0.8333333333333334, 0.8333333333333334, 0.8377192982456141, 0.8377192982456141, 0.8245614035087719, 0.8289473684210527, 0.8333333333333334, 0.8421052631578947, 0.8421052631578947, 0.8377192982456141, 0.8421052631578947, 0.8421052631578947, 0.8552631578947368, 0.8508771929824561, 0.8421052631578947, 0.8289473684210527, 0.8245614035087719, 0.8201754385964912, 0.8289473684210527, 0.8464912280701754, 0.8552631578947368, 0.8508771929824561, 0.8464912280701754, 0.8421052631578947, 0.8508771929824561, 0.8552631578947368, 0.8464912280701754, 0.8464912280701754, 0.8421052631578947, 0.8421052631578947, 0.8333333333333334, 0.8289473684210527, 0.8289473684210527, 0.8421052631578947, 0.8421052631578947, 0.8421052631578947, 0.8421052631578947, 0.8333333333333334, 0.8377192982456141, 0.8377192982456141, 0.8377192982456141, 0.8464912280701754, 0.8464912280701754, 0.8377192982456141, 0.8421052631578947, 0.8333333333333334, 0.8333333333333334, 0.8289473684210527, 0.8245614035087719, 0.8245614035087719, 0.8245614035087719, 0.8245614035087719, 0.8201754385964912, 0.8245614035087719, 0.8289473684210527, 0.8289473684210527, 0.8289473684210527, 0.8289473684210527, 0.8245614035087719, 0.8157894736842105, 0.8201754385964912, 0.8157894736842105, 0.8245614035087719, 0.8245614035087719, 0.8245614035087719, 0.8289473684210527, 0.8289473684210527, 0.8333333333333334, 0.8421052631578947, 0.8377192982456141, 0.8377192982456141, 0.8333333333333334, 0.8421052631578947, 0.8421052631578947, 0.8508771929824561, 0.8464912280701754, 0.8289473684210527, 0.8157894736842105, 0.8157894736842105, 0.8333333333333334, 0.8421052631578947, 0.8377192982456141, 0.8421052631578947, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.8421052631578947, 0.8421052631578947, 0.8421052631578947, 0.8421052631578947, 0.8421052631578947, 0.8377192982456141, 0.8377192982456141, 0.8201754385964912, 0.793859649122807, 0.8070175438596491, 0.8114035087719298, 0.8070175438596491, 0.8026315789473685, 0.8026315789473685, 0.8026315789473685, 0.8026315789473685, 0.8026315789473685, 0.8157894736842105, 0.8245614035087719, 0.8070175438596491, 0.8026315789473685, 0.8201754385964912, 0.8421052631578947, 0.8508771929824561, 0.8552631578947368, 0.8421052631578947, 0.8377192982456141, 0.8377192982456141], [0.5526315789473685, 0.5526315789473685, 0.5526315789473685, 0.5526315789473685, 0.5526315789473685, 0.5526315789473685, 0.5526315789473685, 0.5526315789473685, 0.5526315789473685, 0.5526315789473685, 0.5526315789473685, 0.5526315789473685, 0.5526315789473685, 0.5526315789473685, 0.5526315789473685, 0.5526315789473685, 0.5526315789473685, 0.5526315789473685, 0.5526315789473685, 0.5526315789473685, 0.5526315789473685, 0.5526315789473685, 0.5526315789473685, 0.5526315789473685, 0.5526315789473685, 0.5526315789473685, 0.5526315789473685, 0.5526315789473685, 0.5526315789473685, 0.5526315789473685, 0.5526315789473685, 0.5526315789473685, 0.5526315789473685, 0.5570175438596491, 0.5657894736842105, 0.5789473684210527, 0.5833333333333334, 0.5877192982456141, 0.5789473684210527, 0.5745614035087719, 0.5745614035087719, 0.5745614035087719, 0.5745614035087719, 0.5745614035087719, 0.5833333333333334, 0.5964912280701754, 0.6008771929824561, 0.6008771929824561, 0.6140350877192983, 0.6359649122807017, 0.6710526315789473, 0.7105263157894737, 0.7149122807017544, 0.7324561403508771, 0.7412280701754386, 0.7675438596491229, 0.7631578947368421, 0.7412280701754386, 0.75, 0.7412280701754386, 0.7412280701754386, 0.7368421052631579, 0.7324561403508771, 0.7192982456140351, 0.7105263157894737, 0.706140350877193, 0.7105263157894737, 0.7149122807017544, 0.7236842105263158, 0.7324561403508771, 0.7412280701754386, 0.7324561403508771, 0.7192982456140351, 0.7324561403508771, 0.7280701754385965, 0.7368421052631579, 0.7587719298245614, 0.7587719298245614, 0.7587719298245614, 0.7587719298245614, 0.7675438596491229, 0.7631578947368421, 0.7675438596491229, 0.7719298245614035, 0.7763157894736842, 0.7763157894736842, 0.7763157894736842, 0.793859649122807, 0.7894736842105263, 0.7807017543859649, 0.7894736842105263, 0.793859649122807, 0.8026315789473685, 0.8114035087719298, 0.8026315789473685, 0.7807017543859649, 0.7412280701754386, 0.7105263157894737, 0.6929824561403509, 0.6798245614035088, 0.6929824561403509, 0.6973684210526315, 0.7149122807017544, 0.7456140350877193, 0.7719298245614035, 0.7982456140350878, 0.793859649122807, 0.8026315789473685, 0.8245614035087719, 0.8245614035087719, 0.8245614035087719, 0.8245614035087719, 0.8201754385964912, 0.8245614035087719, 0.8070175438596491, 0.8289473684210527, 0.8289473684210527, 0.8289473684210527, 0.8245614035087719, 0.7982456140350878, 0.793859649122807, 0.7850877192982456, 0.793859649122807, 0.7850877192982456, 0.7807017543859649, 0.7894736842105263, 0.7894736842105263, 0.793859649122807, 0.793859649122807, 0.7982456140350878, 0.8245614035087719, 0.8201754385964912, 0.8289473684210527, 0.8289473684210527, 0.8245614035087719, 0.8289473684210527, 0.8377192982456141, 0.8377192982456141, 0.8377192982456141, 0.8157894736842105, 0.8157894736842105, 0.8157894736842105, 0.8070175438596491, 0.8070175438596491, 0.8070175438596491, 0.8114035087719298, 0.8114035087719298, 0.8114035087719298, 0.8157894736842105, 0.8114035087719298, 0.8157894736842105, 0.8157894736842105, 0.8333333333333334, 0.8157894736842105, 0.8333333333333334, 0.8201754385964912, 0.8157894736842105, 0.8070175438596491, 0.7894736842105263, 0.7850877192982456, 0.793859649122807, 0.793859649122807, 0.8070175438596491, 0.8201754385964912, 0.8245614035087719, 0.8377192982456141, 0.8333333333333334, 0.8245614035087719, 0.8289473684210527, 0.8289473684210527, 0.8377192982456141, 0.8245614035087719, 0.8157894736842105, 0.8070175438596491, 0.793859649122807, 0.793859649122807, 0.793859649122807, 0.8026315789473685, 0.8070175438596491, 0.8114035087719298, 0.8508771929824561, 0.8421052631578947, 0.8421052631578947, 0.8201754385964912, 0.8201754385964912, 0.8157894736842105, 0.8157894736842105, 0.8157894736842105, 0.8201754385964912, 0.8508771929824561, 0.8464912280701754, 0.8508771929824561, 0.8245614035087719, 0.8026315789473685, 0.7982456140350878, 0.7894736842105263, 0.7763157894736842, 0.7719298245614035, 0.7719298245614035, 0.7763157894736842, 0.7894736842105263, 0.7982456140350878, 0.8201754385964912, 0.8333333333333334, 0.8508771929824561, 0.8377192982456141, 0.8333333333333334, 0.8289473684210527, 0.8333333333333334, 0.8333333333333334, 0.8377192982456141, 0.8377192982456141, 0.8640350877192983, 0.8421052631578947, 0.8245614035087719, 0.8245614035087719, 0.7982456140350878, 0.8026315789473685, 0.8026315789473685, 0.8070175438596491, 0.8201754385964912, 0.8289473684210527, 0.8377192982456141, 0.8421052631578947, 0.8377192982456141, 0.8421052631578947, 0.8377192982456141, 0.8377192982456141, 0.8464912280701754, 0.8333333333333334, 0.8201754385964912, 0.8201754385964912, 0.8114035087719298, 0.8070175438596491, 0.8070175438596491, 0.8070175438596491, 0.8114035087719298, 0.8070175438596491, 0.8070175438596491, 0.8114035087719298, 0.8157894736842105, 0.8201754385964912, 0.8289473684210527, 0.8421052631578947, 0.8421052631578947, 0.8377192982456141, 0.8333333333333334, 0.8377192982456141, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.8377192982456141, 0.8421052631578947, 0.8421052631578947, 0.8245614035087719], [0.5526315789473685, 0.5526315789473685, 0.5526315789473685, 0.5526315789473685, 0.5526315789473685, 0.5526315789473685, 0.5526315789473685, 0.5526315789473685, 0.5526315789473685, 0.5526315789473685, 0.5526315789473685, 0.5526315789473685, 0.5526315789473685, 0.5526315789473685, 0.5526315789473685, 0.5526315789473685, 0.5526315789473685, 0.5526315789473685, 0.5526315789473685, 0.5526315789473685, 0.5526315789473685, 0.5526315789473685, 0.5526315789473685, 0.5570175438596491, 0.5570175438596491, 0.5570175438596491, 0.5745614035087719, 0.5921052631578947, 0.631578947368421, 0.6666666666666666, 0.706140350877193, 0.7368421052631579, 0.7368421052631579, 0.7456140350877193, 0.7543859649122807, 0.75, 0.7324561403508771, 0.7280701754385965, 0.7368421052631579, 0.7368421052631579, 0.7324561403508771, 0.7368421052631579, 0.7324561403508771, 0.7236842105263158, 0.7192982456140351, 0.7192982456140351, 0.7280701754385965, 0.7280701754385965, 0.7280701754385965, 0.7324561403508771, 0.7236842105263158, 0.7192982456140351, 0.7236842105263158, 0.7236842105263158, 0.7280701754385965, 0.7236842105263158, 0.7149122807017544, 0.7017543859649122, 0.6929824561403509, 0.6842105263157895, 0.6929824561403509, 0.6973684210526315, 0.7017543859649122, 0.7236842105263158, 0.7412280701754386, 0.75, 0.7587719298245614, 0.7763157894736842, 0.7719298245614035, 0.7807017543859649, 0.7631578947368421, 0.7675438596491229, 0.75, 0.7543859649122807, 0.7631578947368421, 0.7675438596491229, 0.7807017543859649, 0.7894736842105263, 0.7763157894736842, 0.7719298245614035, 0.7675438596491229, 0.7807017543859649, 0.7763157894736842, 0.7543859649122807, 0.7631578947368421, 0.7807017543859649, 0.7763157894736842, 0.7850877192982456, 0.7850877192982456, 0.7850877192982456, 0.7850877192982456, 0.7850877192982456, 0.7894736842105263, 0.7850877192982456, 0.8070175438596491, 0.8026315789473685, 0.8026315789473685, 0.8070175438596491, 0.8070175438596491, 0.793859649122807, 0.793859649122807, 0.7982456140350878, 0.7894736842105263, 0.7631578947368421, 0.7631578947368421, 0.7675438596491229, 0.7631578947368421, 0.7719298245614035, 0.7719298245614035, 0.7850877192982456, 0.7850877192982456, 0.7894736842105263, 0.793859649122807, 0.8157894736842105, 0.8245614035087719, 0.8114035087719298, 0.7982456140350878, 0.793859649122807, 0.7982456140350878, 0.8114035087719298, 0.8289473684210527, 0.8245614035087719, 0.8245614035087719, 0.8289473684210527, 0.8245614035087719, 0.8245614035087719, 0.7850877192982456, 0.7763157894736842, 0.7675438596491229, 0.7675438596491229, 0.7807017543859649, 0.7807017543859649, 0.7763157894736842, 0.7763157894736842, 0.7807017543859649, 0.7807017543859649, 0.7850877192982456, 0.8070175438596491, 0.8114035087719298, 0.8114035087719298, 0.8157894736842105, 0.8289473684210527, 0.8201754385964912, 0.8289473684210527, 0.8157894736842105, 0.8201754385964912, 0.8157894736842105, 0.7982456140350878, 0.793859649122807, 0.7850877192982456, 0.7807017543859649, 0.7850877192982456, 0.7894736842105263, 0.8070175438596491, 0.8114035087719298, 0.8201754385964912, 0.793859649122807, 0.7982456140350878, 0.793859649122807, 0.793859649122807, 0.793859649122807, 0.8026315789473685, 0.8201754385964912, 0.8157894736842105, 0.8070175438596491, 0.8026315789473685, 0.7982456140350878, 0.7982456140350878, 0.793859649122807, 0.7894736842105263, 0.7894736842105263, 0.7894736842105263, 0.7894736842105263, 0.8070175438596491, 0.8201754385964912, 0.8289473684210527, 0.8333333333333334, 0.8201754385964912, 0.8201754385964912, 0.8245614035087719, 0.8157894736842105, 0.8289473684210527, 0.8157894736842105, 0.8070175438596491, 0.8026315789473685, 0.8070175438596491, 0.8114035087719298, 0.8245614035087719, 0.8201754385964912, 0.8245614035087719, 0.8114035087719298, 0.8333333333333334, 0.8333333333333334, 0.8026315789473685, 0.7982456140350878, 0.7982456140350878, 0.8114035087719298, 0.8026315789473685, 0.8026315789473685, 0.8070175438596491, 0.8026315789473685, 0.7982456140350878, 0.8026315789473685, 0.8114035087719298, 0.8333333333333334, 0.8333333333333334, 0.8421052631578947, 0.8421052631578947, 0.8377192982456141, 0.8289473684210527, 0.8377192982456141, 0.8245614035087719, 0.8114035087719298, 0.8157894736842105, 0.8157894736842105, 0.8377192982456141, 0.8421052631578947, 0.8421052631578947, 0.8421052631578947, 0.8596491228070176, 0.8508771929824561, 0.8596491228070176, 0.8508771929824561, 0.8464912280701754, 0.8508771929824561, 0.8552631578947368, 0.8421052631578947, 0.8377192982456141, 0.8114035087719298, 0.7982456140350878, 0.793859649122807, 0.8026315789473685, 0.8114035087719298, 0.8245614035087719, 0.8201754385964912, 0.8245614035087719, 0.8377192982456141, 0.8464912280701754, 0.8464912280701754, 0.8421052631578947, 0.8377192982456141, 0.8333333333333334, 0.8157894736842105, 0.8157894736842105, 0.8245614035087719, 0.8333333333333334, 0.8508771929824561, 0.8377192982456141, 0.8201754385964912, 0.8157894736842105, 0.8114035087719298, 0.8157894736842105, 0.793859649122807, 0.7807017543859649, 0.7807017543859649], [0.5526315789473685, 0.5526315789473685, 0.5526315789473685, 0.5526315789473685, 0.5526315789473685, 0.5526315789473685, 0.5526315789473685, 0.5526315789473685, 0.5526315789473685, 0.5526315789473685, 0.5526315789473685, 0.5526315789473685, 0.5526315789473685, 0.5526315789473685, 0.5526315789473685, 0.5657894736842105, 0.5789473684210527, 0.6228070175438597, 0.6578947368421053, 0.6666666666666666, 0.6535087719298246, 0.6271929824561403, 0.5964912280701754, 0.5833333333333334, 0.5789473684210527, 0.5701754385964912, 0.5657894736842105, 0.5614035087719298, 0.5526315789473685, 0.5657894736842105, 0.5657894736842105, 0.5657894736842105, 0.5657894736842105, 0.5570175438596491, 0.5657894736842105, 0.5657894736842105, 0.5789473684210527, 0.5833333333333334, 0.5921052631578947, 0.5921052631578947, 0.5921052631578947, 0.618421052631579, 0.6535087719298246, 0.6710526315789473, 0.6842105263157895, 0.6842105263157895, 0.6798245614035088, 0.6798245614035088, 0.6885964912280702, 0.7017543859649122, 0.7280701754385965, 0.75, 0.7543859649122807, 0.7675438596491229, 0.7587719298245614, 0.7543859649122807, 0.7543859649122807, 0.7456140350877193, 0.75, 0.7456140350877193, 0.7456140350877193, 0.7368421052631579, 0.7017543859649122, 0.706140350877193, 0.7017543859649122, 0.7280701754385965, 0.7324561403508771, 0.75, 0.7324561403508771, 0.7587719298245614, 0.7631578947368421, 0.7631578947368421, 0.7631578947368421, 0.7631578947368421, 0.7763157894736842, 0.7719298245614035, 0.7675438596491229, 0.7675438596491229, 0.7587719298245614, 0.7675438596491229, 0.7543859649122807, 0.7456140350877193, 0.7456140350877193, 0.75, 0.75, 0.7543859649122807, 0.7543859649122807, 0.7587719298245614, 0.7412280701754386, 0.7324561403508771, 0.7368421052631579, 0.7368421052631579, 0.7280701754385965, 0.7324561403508771, 0.7236842105263158, 0.7368421052631579, 0.7324561403508771, 0.7368421052631579, 0.7368421052631579, 0.7456140350877193, 0.7456140350877193, 0.7543859649122807, 0.7850877192982456, 0.8026315789473685, 0.793859649122807, 0.7763157894736842, 0.7850877192982456, 0.7850877192982456, 0.793859649122807, 0.8070175438596491, 0.8157894736842105, 0.7719298245614035, 0.7719298245614035, 0.7719298245614035, 0.7675438596491229, 0.7719298245614035, 0.7763157894736842, 0.7675438596491229, 0.7763157894736842, 0.7807017543859649, 0.7763157894736842, 0.7894736842105263, 0.7894736842105263, 0.7982456140350878, 0.793859649122807, 0.8026315789473685, 0.8114035087719298, 0.7982456140350878, 0.7894736842105263, 0.7850877192982456, 0.7807017543859649, 0.7850877192982456, 0.793859649122807, 0.7982456140350878, 0.8026315789473685, 0.8201754385964912, 0.8201754385964912, 0.8245614035087719, 0.8245614035087719, 0.8289473684210527, 0.8157894736842105, 0.8157894736842105, 0.8245614035087719, 0.8377192982456141, 0.8201754385964912, 0.8026315789473685, 0.7982456140350878, 0.7850877192982456, 0.7807017543859649, 0.7850877192982456, 0.7763157894736842, 0.7719298245614035, 0.7719298245614035, 0.7807017543859649, 0.7850877192982456, 0.7894736842105263, 0.7894736842105263, 0.8026315789473685, 0.7982456140350878, 0.8026315789473685, 0.8026315789473685, 0.8070175438596491, 0.8245614035087719, 0.8377192982456141, 0.8333333333333334, 0.8377192982456141, 0.8333333333333334, 0.8333333333333334, 0.8377192982456141, 0.8289473684210527, 0.8289473684210527, 0.8245614035087719, 0.8245614035087719, 0.8245614035087719, 0.8245614035087719, 0.8245614035087719, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.8289473684210527, 0.8377192982456141, 0.8333333333333334, 0.8333333333333334, 0.8157894736842105, 0.8289473684210527, 0.8201754385964912, 0.8333333333333334, 0.8333333333333334, 0.8245614035087719, 0.8289473684210527, 0.8289473684210527, 0.8289473684210527, 0.8333333333333334, 0.8333333333333334, 0.8245614035087719, 0.8333333333333334, 0.8421052631578947, 0.8245614035087719, 0.8333333333333334, 0.8377192982456141, 0.8464912280701754, 0.8464912280701754, 0.8377192982456141, 0.8333333333333334, 0.8289473684210527, 0.8289473684210527, 0.8201754385964912, 0.8245614035087719, 0.8201754385964912, 0.8245614035087719, 0.8201754385964912, 0.8245614035087719, 0.8201754385964912, 0.8201754385964912, 0.8201754385964912, 0.8245614035087719, 0.8245614035087719, 0.8289473684210527, 0.8377192982456141, 0.8464912280701754, 0.8289473684210527, 0.8201754385964912, 0.7982456140350878, 0.7982456140350878, 0.8114035087719298, 0.8245614035087719, 0.8289473684210527, 0.8333333333333334, 0.8333333333333334, 0.8377192982456141, 0.8157894736842105, 0.8157894736842105, 0.8157894736842105, 0.8114035087719298, 0.8070175438596491, 0.8114035087719298, 0.8070175438596491, 0.8114035087719298, 0.8114035087719298, 0.8114035087719298, 0.8157894736842105, 0.8114035087719298, 0.8070175438596491, 0.8114035087719298, 0.8114035087719298, 0.8333333333333334, 0.8289473684210527, 0.8377192982456141, 0.8070175438596491, 0.8114035087719298, 0.8114035087719298, 0.8026315789473685, 0.8026315789473685, 0.8114035087719298, 0.8245614035087719], [0.4473684210526316, 0.4473684210526316, 0.4473684210526316, 0.4473684210526316, 0.4473684210526316, 0.4473684210526316, 0.47368421052631576, 0.4824561403508772, 0.5087719298245614, 0.5701754385964912, 0.6008771929824561, 0.6359649122807017, 0.6798245614035088, 0.706140350877193, 0.6359649122807017, 0.6271929824561403, 0.5701754385964912, 0.5614035087719298, 0.5614035087719298, 0.5526315789473685, 0.5526315789473685, 0.5526315789473685, 0.5526315789473685, 0.5526315789473685, 0.5526315789473685, 0.5526315789473685, 0.5614035087719298, 0.5614035087719298, 0.5614035087719298, 0.5570175438596491, 0.5570175438596491, 0.5526315789473685, 0.5526315789473685, 0.5526315789473685, 0.5570175438596491, 0.5570175438596491, 0.5570175438596491, 0.5614035087719298, 0.5789473684210527, 0.6052631578947368, 0.6228070175438597, 0.6447368421052632, 0.6666666666666666, 0.6798245614035088, 0.7324561403508771, 0.7368421052631579, 0.7368421052631579, 0.7324561403508771, 0.7368421052631579, 0.7368421052631579, 0.7412280701754386, 0.7456140350877193, 0.7324561403508771, 0.7324561403508771, 0.7192982456140351, 0.7324561403508771, 0.7412280701754386, 0.7412280701754386, 0.7456140350877193, 0.7368421052631579, 0.7412280701754386, 0.7412280701754386, 0.7324561403508771, 0.7412280701754386, 0.7368421052631579, 0.7412280701754386, 0.7324561403508771, 0.7412280701754386, 0.7368421052631579, 0.7368421052631579, 0.7456140350877193, 0.7412280701754386, 0.7105263157894737, 0.7192982456140351, 0.706140350877193, 0.7105263157894737, 0.7324561403508771, 0.7236842105263158, 0.7368421052631579, 0.7324561403508771, 0.7324561403508771, 0.7456140350877193, 0.7543859649122807, 0.7719298245614035, 0.7894736842105263, 0.7850877192982456, 0.7631578947368421, 0.7456140350877193, 0.7280701754385965, 0.7192982456140351, 0.7236842105263158, 0.7368421052631579, 0.7456140350877193, 0.7543859649122807, 0.7719298245614035, 0.7807017543859649, 0.7719298245614035, 0.7675438596491229, 0.7543859649122807, 0.75, 0.7631578947368421, 0.7763157894736842, 0.7763157894736842, 0.7763157894736842, 0.7763157894736842, 0.7719298245614035, 0.7719298245614035, 0.7631578947368421, 0.7631578947368421, 0.7675438596491229, 0.7719298245614035, 0.7763157894736842, 0.7850877192982456, 0.7850877192982456, 0.7763157894736842, 0.7763157894736842, 0.7807017543859649, 0.7850877192982456, 0.7807017543859649, 0.7894736842105263, 0.7894736842105263, 0.793859649122807, 0.793859649122807, 0.793859649122807, 0.793859649122807, 0.793859649122807, 0.793859649122807, 0.7982456140350878, 0.793859649122807, 0.7982456140350878, 0.793859649122807, 0.7807017543859649, 0.7807017543859649, 0.7850877192982456, 0.7894736842105263, 0.7850877192982456, 0.7807017543859649, 0.7807017543859649, 0.7850877192982456, 0.7982456140350878, 0.8026315789473685, 0.8245614035087719, 0.8201754385964912, 0.8114035087719298, 0.8114035087719298, 0.8026315789473685, 0.8026315789473685, 0.793859649122807, 0.793859649122807, 0.793859649122807, 0.8201754385964912, 0.8377192982456141, 0.8245614035087719, 0.8201754385964912, 0.8157894736842105, 0.8114035087719298, 0.8201754385964912, 0.8245614035087719, 0.8245614035087719, 0.8201754385964912, 0.8377192982456141, 0.8377192982456141, 0.8421052631578947, 0.8377192982456141, 0.8377192982456141, 0.8377192982456141, 0.8333333333333334, 0.8289473684210527, 0.8157894736842105, 0.8157894736842105, 0.8114035087719298, 0.8157894736842105, 0.8114035087719298, 0.8201754385964912, 0.8245614035087719, 0.8201754385964912, 0.8289473684210527, 0.8377192982456141, 0.8333333333333334, 0.8333333333333334, 0.8157894736842105, 0.8201754385964912, 0.8289473684210527, 0.8201754385964912, 0.8026315789473685, 0.793859649122807, 0.7982456140350878, 0.793859649122807, 0.8026315789473685, 0.7982456140350878, 0.8070175438596491, 0.8201754385964912, 0.8289473684210527, 0.8289473684210527, 0.8245614035087719, 0.8289473684210527, 0.8201754385964912, 0.8245614035087719, 0.8201754385964912, 0.8245614035087719, 0.8201754385964912, 0.8333333333333334, 0.8289473684210527, 0.8377192982456141, 0.8245614035087719, 0.8289473684210527, 0.8289473684210527, 0.8289473684210527, 0.8245614035087719, 0.8333333333333334, 0.8333333333333334, 0.8289473684210527, 0.8289473684210527, 0.8289473684210527, 0.8289473684210527, 0.8333333333333334, 0.8377192982456141, 0.8421052631578947, 0.8377192982456141, 0.8377192982456141, 0.8377192982456141, 0.8333333333333334, 0.8377192982456141, 0.8421052631578947, 0.8377192982456141, 0.8377192982456141, 0.8333333333333334, 0.8333333333333334, 0.8377192982456141, 0.8333333333333334, 0.8421052631578947, 0.8333333333333334, 0.8245614035087719, 0.8157894736842105, 0.8114035087719298, 0.8114035087719298, 0.8157894736842105, 0.8201754385964912, 0.8289473684210527, 0.8289473684210527, 0.8464912280701754, 0.8421052631578947, 0.8464912280701754, 0.8421052631578947, 0.8421052631578947, 0.8508771929824561, 0.8377192982456141, 0.8289473684210527, 0.8245614035087719, 0.8201754385964912, 0.8201754385964912, 0.8157894736842105, 0.8201754385964912, 0.8333333333333334, 0.8289473684210527], [0.5526315789473685, 0.5526315789473685, 0.5526315789473685, 0.5526315789473685, 0.5526315789473685, 0.5526315789473685, 0.5526315789473685, 0.5526315789473685, 0.5526315789473685, 0.5526315789473685, 0.5526315789473685, 0.5526315789473685, 0.5526315789473685, 0.5526315789473685, 0.5526315789473685, 0.5526315789473685, 0.5526315789473685, 0.5526315789473685, 0.5526315789473685, 0.5526315789473685, 0.5526315789473685, 0.5526315789473685, 0.5526315789473685, 0.5526315789473685, 0.5526315789473685, 0.5526315789473685, 0.5526315789473685, 0.5526315789473685, 0.5526315789473685, 0.5526315789473685, 0.5526315789473685, 0.5614035087719298, 0.5833333333333334, 0.5921052631578947, 0.618421052631579, 0.6403508771929824, 0.7105263157894737, 0.7324561403508771, 0.7368421052631579, 0.7675438596491229, 0.7675438596491229, 0.7456140350877193, 0.7412280701754386, 0.7368421052631579, 0.7236842105263158, 0.7324561403508771, 0.7368421052631579, 0.7324561403508771, 0.7324561403508771, 0.7456140350877193, 0.7456140350877193, 0.7631578947368421, 0.7543859649122807, 0.7368421052631579, 0.7280701754385965, 0.7324561403508771, 0.7324561403508771, 0.7236842105263158, 0.7236842105263158, 0.7324561403508771, 0.7412280701754386, 0.7456140350877193, 0.7587719298245614, 0.7412280701754386, 0.7631578947368421, 0.7412280701754386, 0.7280701754385965, 0.7280701754385965, 0.7149122807017544, 0.7149122807017544, 0.7324561403508771, 0.7368421052631579, 0.7456140350877193, 0.7587719298245614, 0.7631578947368421, 0.7543859649122807, 0.7631578947368421, 0.7587719298245614, 0.7324561403508771, 0.7236842105263158, 0.7236842105263158, 0.7280701754385965, 0.7412280701754386, 0.7587719298245614, 0.7587719298245614, 0.7587719298245614, 0.7719298245614035, 0.7807017543859649, 0.7763157894736842, 0.7894736842105263, 0.7850877192982456, 0.7850877192982456, 0.7763157894736842, 0.7850877192982456, 0.7850877192982456, 0.7850877192982456, 0.7675438596491229, 0.7631578947368421, 0.7631578947368421, 0.7631578947368421, 0.7675438596491229, 0.7675438596491229, 0.7807017543859649, 0.7719298245614035, 0.8026315789473685, 0.7982456140350878, 0.793859649122807, 0.7807017543859649, 0.7894736842105263, 0.7894736842105263, 0.7894736842105263, 0.7894736842105263, 0.793859649122807, 0.793859649122807, 0.8026315789473685, 0.8026315789473685, 0.8070175438596491, 0.7982456140350878, 0.793859649122807, 0.7982456140350878, 0.7719298245614035, 0.7587719298245614, 0.7631578947368421, 0.7587719298245614, 0.7543859649122807, 0.7631578947368421, 0.7719298245614035, 0.7850877192982456, 0.7807017543859649, 0.8026315789473685, 0.8114035087719298, 0.8114035087719298, 0.8201754385964912, 0.8157894736842105, 0.8201754385964912, 0.8245614035087719, 0.8157894736842105, 0.8201754385964912, 0.8201754385964912, 0.8114035087719298, 0.8157894736842105, 0.8070175438596491, 0.8333333333333334, 0.8289473684210527, 0.8289473684210527, 0.8070175438596491, 0.793859649122807, 0.793859649122807, 0.7807017543859649, 0.7763157894736842, 0.7763157894736842, 0.7763157894736842, 0.7894736842105263, 0.8070175438596491, 0.8026315789473685, 0.8245614035087719, 0.8377192982456141, 0.8377192982456141, 0.8377192982456141, 0.8377192982456141, 0.8421052631578947, 0.8464912280701754, 0.8464912280701754, 0.8464912280701754, 0.8464912280701754, 0.8508771929824561, 0.8508771929824561, 0.8552631578947368, 0.8552631578947368, 0.8421052631578947, 0.8289473684210527, 0.8157894736842105, 0.8157894736842105, 0.8070175438596491, 0.7982456140350878, 0.793859649122807, 0.7763157894736842, 0.7763157894736842, 0.7719298245614035, 0.7763157894736842, 0.7894736842105263, 0.8026315789473685, 0.8114035087719298, 0.8157894736842105, 0.8201754385964912, 0.8245614035087719, 0.8377192982456141, 0.8552631578947368, 0.8421052631578947, 0.8289473684210527, 0.8245614035087719, 0.8333333333333334, 0.8421052631578947, 0.8421052631578947, 0.8508771929824561, 0.8245614035087719, 0.8201754385964912, 0.8114035087719298, 0.8026315789473685, 0.793859649122807, 0.793859649122807, 0.8026315789473685, 0.8114035087719298, 0.8157894736842105, 0.8245614035087719, 0.8245614035087719, 0.8201754385964912, 0.8201754385964912, 0.8114035087719298, 0.8114035087719298, 0.8157894736842105, 0.8245614035087719, 0.8245614035087719, 0.8245614035087719, 0.8245614035087719, 0.8245614035087719, 0.8289473684210527, 0.8201754385964912, 0.8245614035087719, 0.8201754385964912, 0.8201754385964912, 0.8157894736842105, 0.8201754385964912, 0.8245614035087719, 0.8245614035087719, 0.8245614035087719, 0.8245614035087719, 0.8157894736842105, 0.8245614035087719, 0.8333333333333334, 0.8377192982456141, 0.8333333333333334, 0.8245614035087719, 0.8289473684210527, 0.8245614035087719, 0.8289473684210527, 0.8289473684210527, 0.8245614035087719, 0.8114035087719298, 0.8201754385964912, 0.8245614035087719, 0.8245614035087719, 0.8114035087719298, 0.8201754385964912, 0.8289473684210527, 0.8289473684210527, 0.8377192982456141, 0.8289473684210527, 0.8114035087719298, 0.8157894736842105, 0.8201754385964912, 0.8201754385964912, 0.8333333333333334, 0.8377192982456141, 0.8333333333333334], [0.4473684210526316, 0.41228070175438597, 0.5131578947368421, 0.5526315789473685, 0.5526315789473685, 0.5526315789473685, 0.5526315789473685, 0.5526315789473685, 0.5526315789473685, 0.5526315789473685, 0.5526315789473685, 0.5526315789473685, 0.5526315789473685, 0.5526315789473685, 0.5526315789473685, 0.5526315789473685, 0.5526315789473685, 0.5526315789473685, 0.5526315789473685, 0.5526315789473685, 0.5526315789473685, 0.5526315789473685, 0.5526315789473685, 0.5526315789473685, 0.5526315789473685, 0.5526315789473685, 0.5526315789473685, 0.5526315789473685, 0.5526315789473685, 0.5526315789473685, 0.5526315789473685, 0.5526315789473685, 0.5526315789473685, 0.5526315789473685, 0.5526315789473685, 0.5526315789473685, 0.5526315789473685, 0.5526315789473685, 0.5526315789473685, 0.5526315789473685, 0.5526315789473685, 0.5526315789473685, 0.5570175438596491, 0.5570175438596491, 0.5657894736842105, 0.6096491228070176, 0.6447368421052632, 0.6622807017543859, 0.7017543859649122, 0.7236842105263158, 0.7236842105263158, 0.7368421052631579, 0.7368421052631579, 0.7543859649122807, 0.7412280701754386, 0.75, 0.7543859649122807, 0.7456140350877193, 0.7412280701754386, 0.7324561403508771, 0.7192982456140351, 0.7149122807017544, 0.6842105263157895, 0.6929824561403509, 0.706140350877193, 0.7149122807017544, 0.7280701754385965, 0.75, 0.7456140350877193, 0.7456140350877193, 0.7324561403508771, 0.7324561403508771, 0.7456140350877193, 0.7412280701754386, 0.7543859649122807, 0.75, 0.75, 0.7587719298245614, 0.7543859649122807, 0.7543859649122807, 0.7543859649122807, 0.7631578947368421, 0.7543859649122807, 0.7543859649122807, 0.7543859649122807, 0.75, 0.7543859649122807, 0.7587719298245614, 0.7543859649122807, 0.7631578947368421, 0.7543859649122807, 0.7587719298245614, 0.7587719298245614, 0.7763157894736842, 0.7675438596491229, 0.7719298245614035, 0.7675438596491229, 0.7763157894736842, 0.7631578947368421, 0.7719298245614035, 0.7850877192982456, 0.7850877192982456, 0.7894736842105263, 0.793859649122807, 0.7807017543859649, 0.7763157894736842, 0.7763157894736842, 0.7763157894736842, 0.7850877192982456, 0.7982456140350878, 0.793859649122807, 0.7850877192982456, 0.7807017543859649, 0.793859649122807, 0.793859649122807, 0.7894736842105263, 0.7850877192982456, 0.7763157894736842, 0.7587719298245614, 0.7587719298245614, 0.7631578947368421, 0.793859649122807, 0.7850877192982456, 0.793859649122807, 0.7894736842105263, 0.793859649122807, 0.793859649122807, 0.793859649122807, 0.7894736842105263, 0.8026315789473685, 0.7982456140350878, 0.7982456140350878, 0.793859649122807, 0.8070175438596491, 0.8026315789473685, 0.8026315789473685, 0.8026315789473685, 0.8026315789473685, 0.793859649122807, 0.8026315789473685, 0.7982456140350878, 0.793859649122807, 0.793859649122807, 0.793859649122807, 0.793859649122807, 0.793859649122807, 0.7982456140350878, 0.7982456140350878, 0.7982456140350878, 0.7982456140350878, 0.8114035087719298, 0.8026315789473685, 0.8070175438596491, 0.8026315789473685, 0.8070175438596491, 0.7982456140350878, 0.7982456140350878, 0.8070175438596491, 0.8070175438596491, 0.8026315789473685, 0.8070175438596491, 0.8157894736842105, 0.8114035087719298, 0.8070175438596491, 0.8114035087719298, 0.8070175438596491, 0.7982456140350878, 0.7982456140350878, 0.793859649122807, 0.793859649122807, 0.793859649122807, 0.793859649122807, 0.7982456140350878, 0.8026315789473685, 0.8026315789473685, 0.8333333333333334, 0.8157894736842105, 0.7982456140350878, 0.8201754385964912, 0.8114035087719298, 0.8157894736842105, 0.8201754385964912, 0.8070175438596491, 0.8245614035087719, 0.8157894736842105, 0.8157894736842105, 0.7982456140350878, 0.7850877192982456, 0.7850877192982456, 0.7850877192982456, 0.7894736842105263, 0.7894736842105263, 0.7894736842105263, 0.7982456140350878, 0.7982456140350878, 0.7982456140350878, 0.8201754385964912, 0.8245614035087719, 0.8201754385964912, 0.8201754385964912, 0.8289473684210527, 0.8201754385964912, 0.8201754385964912, 0.7982456140350878, 0.793859649122807, 0.8026315789473685, 0.8026315789473685, 0.793859649122807, 0.7894736842105263, 0.7894736842105263, 0.7850877192982456, 0.7894736842105263, 0.7850877192982456, 0.793859649122807, 0.793859649122807, 0.8114035087719298, 0.8114035087719298, 0.8070175438596491, 0.8026315789473685, 0.8070175438596491, 0.8114035087719298, 0.8157894736842105, 0.8026315789473685, 0.7807017543859649, 0.7850877192982456, 0.7850877192982456, 0.7807017543859649, 0.7894736842105263, 0.793859649122807, 0.793859649122807, 0.8070175438596491, 0.8201754385964912, 0.8201754385964912, 0.8289473684210527, 0.8201754385964912, 0.8157894736842105, 0.8333333333333334, 0.8245614035087719, 0.8070175438596491, 0.8157894736842105, 0.8157894736842105, 0.8070175438596491, 0.8070175438596491, 0.8157894736842105, 0.8289473684210527, 0.8333333333333334, 0.8157894736842105, 0.8245614035087719, 0.8245614035087719, 0.8201754385964912, 0.8201754385964912, 0.8201754385964912, 0.8157894736842105, 0.8201754385964912, 0.8333333333333334], [0.5526315789473685, 0.5526315789473685, 0.5526315789473685, 0.5526315789473685, 0.5526315789473685, 0.5526315789473685, 0.5526315789473685, 0.5526315789473685, 0.5526315789473685, 0.5526315789473685, 0.5526315789473685, 0.5526315789473685, 0.5526315789473685, 0.5526315789473685, 0.5526315789473685, 0.5526315789473685, 0.5526315789473685, 0.5526315789473685, 0.5526315789473685, 0.5526315789473685, 0.5526315789473685, 0.5526315789473685, 0.5526315789473685, 0.5526315789473685, 0.5526315789473685, 0.5526315789473685, 0.5526315789473685, 0.5526315789473685, 0.5526315789473685, 0.5657894736842105, 0.6008771929824561, 0.6447368421052632, 0.7017543859649122, 0.7412280701754386, 0.7631578947368421, 0.7587719298245614, 0.7631578947368421, 0.7543859649122807, 0.7456140350877193, 0.7368421052631579, 0.7368421052631579, 0.7368421052631579, 0.7412280701754386, 0.75, 0.7543859649122807, 0.7543859649122807, 0.75, 0.7587719298245614, 0.7631578947368421, 0.75, 0.7543859649122807, 0.7456140350877193, 0.7587719298245614, 0.7675438596491229, 0.7631578947368421, 0.7675438596491229, 0.7675438596491229, 0.7543859649122807, 0.7631578947368421, 0.7543859649122807, 0.7587719298245614, 0.7675438596491229, 0.7631578947368421, 0.7587719298245614, 0.7543859649122807, 0.7543859649122807, 0.75, 0.75, 0.7280701754385965, 0.706140350877193, 0.706140350877193, 0.6973684210526315, 0.7017543859649122, 0.7280701754385965, 0.7412280701754386, 0.7719298245614035, 0.7850877192982456, 0.7982456140350878, 0.7894736842105263, 0.7850877192982456, 0.7850877192982456, 0.7850877192982456, 0.7807017543859649, 0.7850877192982456, 0.7982456140350878, 0.8026315789473685, 0.8201754385964912, 0.8157894736842105, 0.8026315789473685, 0.793859649122807, 0.7982456140350878, 0.8070175438596491, 0.8289473684210527, 0.8026315789473685, 0.7850877192982456, 0.7719298245614035, 0.7631578947368421, 0.7719298245614035, 0.7894736842105263, 0.8026315789473685, 0.8157894736842105, 0.8245614035087719, 0.8157894736842105, 0.7850877192982456, 0.7850877192982456, 0.7850877192982456, 0.7850877192982456, 0.7850877192982456, 0.7982456140350878, 0.8070175438596491, 0.8333333333333334, 0.8201754385964912, 0.8070175438596491, 0.7982456140350878, 0.7982456140350878, 0.8026315789473685, 0.8070175438596491, 0.793859649122807, 0.7982456140350878, 0.8026315789473685, 0.8245614035087719, 0.8114035087719298, 0.8114035087719298, 0.8070175438596491, 0.7807017543859649, 0.7807017543859649, 0.7763157894736842, 0.7850877192982456, 0.793859649122807, 0.8026315789473685, 0.7894736842105263, 0.7982456140350878, 0.7982456140350878, 0.793859649122807, 0.7894736842105263, 0.7894736842105263, 0.7894736842105263, 0.793859649122807, 0.7982456140350878, 0.793859649122807, 0.8070175438596491, 0.8070175438596491, 0.8157894736842105, 0.8157894736842105, 0.8157894736842105, 0.8157894736842105, 0.8114035087719298, 0.8070175438596491, 0.8070175438596491, 0.8070175438596491, 0.8114035087719298, 0.8157894736842105, 0.8114035087719298, 0.8157894736842105, 0.8157894736842105, 0.8289473684210527, 0.8201754385964912, 0.8201754385964912, 0.8201754385964912, 0.8114035087719298, 0.8114035087719298, 0.8333333333333334, 0.8289473684210527, 0.8245614035087719, 0.8157894736842105, 0.8114035087719298, 0.7807017543859649, 0.7675438596491229, 0.7587719298245614, 0.7631578947368421, 0.7587719298245614, 0.7631578947368421, 0.7543859649122807, 0.7587719298245614, 0.7675438596491229, 0.7719298245614035, 0.7763157894736842, 0.8114035087719298, 0.8114035087719298, 0.8201754385964912, 0.8201754385964912, 0.8157894736842105, 0.8026315789473685, 0.8026315789473685, 0.8026315789473685, 0.8114035087719298, 0.8157894736842105, 0.8157894736842105, 0.8157894736842105, 0.8245614035087719, 0.8245614035087719, 0.8333333333333334, 0.8157894736842105, 0.8114035087719298, 0.8070175438596491, 0.793859649122807, 0.793859649122807, 0.793859649122807, 0.793859649122807, 0.7982456140350878, 0.8070175438596491, 0.8114035087719298, 0.8421052631578947, 0.8421052631578947, 0.8333333333333334, 0.8201754385964912, 0.8157894736842105, 0.8201754385964912, 0.8201754385964912, 0.8157894736842105, 0.8333333333333334, 0.8245614035087719, 0.8289473684210527, 0.8377192982456141, 0.8464912280701754, 0.8421052631578947, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.8114035087719298, 0.8157894736842105, 0.8157894736842105, 0.8245614035087719, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.8421052631578947, 0.8464912280701754, 0.8464912280701754, 0.8508771929824561, 0.8552631578947368, 0.8464912280701754, 0.8508771929824561, 0.8552631578947368, 0.8508771929824561, 0.8552631578947368, 0.8552631578947368, 0.8640350877192983, 0.8596491228070176, 0.8421052631578947, 0.8377192982456141, 0.8377192982456141, 0.8245614035087719, 0.8201754385964912, 0.8201754385964912, 0.8245614035087719, 0.8245614035087719, 0.8289473684210527, 0.8421052631578947, 0.8421052631578947, 0.8421052631578947, 0.8421052631578947, 0.8377192982456141, 0.8377192982456141], [0.4473684210526316, 0.45614035087719296, 0.5833333333333334, 0.6096491228070176, 0.5526315789473685, 0.5526315789473685, 0.5526315789473685, 0.5526315789473685, 0.5526315789473685, 0.5526315789473685, 0.5526315789473685, 0.5526315789473685, 0.5526315789473685, 0.5526315789473685, 0.5526315789473685, 0.5526315789473685, 0.5526315789473685, 0.5526315789473685, 0.5526315789473685, 0.5526315789473685, 0.5526315789473685, 0.5526315789473685, 0.5526315789473685, 0.5526315789473685, 0.5526315789473685, 0.5526315789473685, 0.5526315789473685, 0.5526315789473685, 0.5526315789473685, 0.5526315789473685, 0.5526315789473685, 0.5526315789473685, 0.5526315789473685, 0.5526315789473685, 0.5526315789473685, 0.5526315789473685, 0.5526315789473685, 0.5526315789473685, 0.5526315789473685, 0.5657894736842105, 0.5789473684210527, 0.5964912280701754, 0.6622807017543859, 0.7105263157894737, 0.7543859649122807, 0.7543859649122807, 0.7543859649122807, 0.7412280701754386, 0.7456140350877193, 0.7412280701754386, 0.75, 0.7368421052631579, 0.7412280701754386, 0.7280701754385965, 0.7412280701754386, 0.7587719298245614, 0.7368421052631579, 0.7192982456140351, 0.706140350877193, 0.706140350877193, 0.7017543859649122, 0.7017543859649122, 0.7192982456140351, 0.7368421052631579, 0.7675438596491229, 0.7543859649122807, 0.7631578947368421, 0.7631578947368421, 0.7543859649122807, 0.7587719298245614, 0.7631578947368421, 0.7631578947368421, 0.7719298245614035, 0.7675438596491229, 0.7631578947368421, 0.7675438596491229, 0.7763157894736842, 0.7763157894736842, 0.7763157894736842, 0.8070175438596491, 0.8070175438596491, 0.8026315789473685, 0.8026315789473685, 0.8070175438596491, 0.8026315789473685, 0.793859649122807, 0.7894736842105263, 0.7894736842105263, 0.8070175438596491, 0.8070175438596491, 0.8114035087719298, 0.8114035087719298, 0.7982456140350878, 0.793859649122807, 0.8026315789473685, 0.8070175438596491, 0.7982456140350878, 0.7894736842105263, 0.793859649122807, 0.8070175438596491, 0.8114035087719298, 0.8070175438596491, 0.8070175438596491, 0.7982456140350878, 0.7982456140350878, 0.7982456140350878, 0.7850877192982456, 0.7894736842105263, 0.7807017543859649, 0.7850877192982456, 0.7982456140350878, 0.8070175438596491, 0.8114035087719298, 0.8070175438596491, 0.7894736842105263, 0.7807017543859649, 0.7807017543859649, 0.7850877192982456, 0.793859649122807, 0.8070175438596491, 0.8026315789473685, 0.8070175438596491, 0.8026315789473685, 0.7982456140350878, 0.793859649122807, 0.8070175438596491, 0.8157894736842105, 0.8070175438596491, 0.8070175438596491, 0.8026315789473685, 0.7982456140350878, 0.8026315789473685, 0.8026315789473685, 0.8114035087719298, 0.8026315789473685, 0.7982456140350878, 0.7894736842105263, 0.7894736842105263, 0.793859649122807, 0.793859649122807, 0.8070175438596491, 0.8070175438596491, 0.8070175438596491, 0.8070175438596491, 0.8201754385964912, 0.8114035087719298, 0.8114035087719298, 0.8114035087719298, 0.8114035087719298, 0.8070175438596491, 0.8114035087719298, 0.8157894736842105, 0.8157894736842105, 0.8201754385964912, 0.8245614035087719, 0.8289473684210527, 0.8245614035087719, 0.8157894736842105, 0.793859649122807, 0.7894736842105263, 0.7894736842105263, 0.793859649122807, 0.7894736842105263, 0.7850877192982456, 0.7850877192982456, 0.793859649122807, 0.8157894736842105, 0.8245614035087719, 0.8245614035087719, 0.8201754385964912, 0.8114035087719298, 0.7850877192982456, 0.7982456140350878, 0.8026315789473685, 0.8114035087719298, 0.7894736842105263, 0.8026315789473685, 0.8157894736842105, 0.8245614035087719, 0.8114035087719298, 0.8157894736842105, 0.8157894736842105, 0.8157894736842105, 0.8114035087719298, 0.8114035087719298, 0.8201754385964912, 0.8201754385964912, 0.8245614035087719, 0.8157894736842105, 0.8070175438596491, 0.8070175438596491, 0.8114035087719298, 0.8157894736842105, 0.8157894736842105, 0.8157894736842105, 0.8070175438596491, 0.8070175438596491, 0.8070175438596491, 0.8114035087719298, 0.8070175438596491, 0.8026315789473685, 0.8070175438596491, 0.8114035087719298, 0.8114035087719298, 0.8157894736842105, 0.8114035087719298, 0.8114035087719298, 0.8070175438596491, 0.8070175438596491, 0.8070175438596491, 0.8026315789473685, 0.7982456140350878, 0.8026315789473685, 0.8026315789473685, 0.8114035087719298, 0.8245614035087719, 0.8201754385964912, 0.8114035087719298, 0.8114035087719298, 0.8245614035087719, 0.8289473684210527, 0.8157894736842105, 0.8245614035087719, 0.8201754385964912, 0.8201754385964912, 0.8245614035087719, 0.8070175438596491, 0.8026315789473685, 0.8070175438596491, 0.8070175438596491, 0.8114035087719298, 0.8245614035087719, 0.8289473684210527, 0.8333333333333334, 0.8333333333333334, 0.8377192982456141, 0.8421052631578947, 0.8421052631578947, 0.8421052631578947, 0.8421052631578947, 0.8464912280701754, 0.8289473684210527, 0.8245614035087719, 0.8157894736842105, 0.8245614035087719, 0.8201754385964912, 0.8333333333333334, 0.8421052631578947, 0.8289473684210527, 0.8245614035087719, 0.8157894736842105, 0.8201754385964912, 0.8289473684210527, 0.8289473684210527, 0.8377192982456141], [0.5219298245614035, 0.5526315789473685, 0.5526315789473685, 0.5526315789473685, 0.5526315789473685, 0.5526315789473685, 0.5526315789473685, 0.5526315789473685, 0.5526315789473685, 0.5526315789473685, 0.5526315789473685, 0.5526315789473685, 0.5526315789473685, 0.5526315789473685, 0.5526315789473685, 0.5526315789473685, 0.5526315789473685, 0.5526315789473685, 0.5526315789473685, 0.5526315789473685, 0.5526315789473685, 0.5526315789473685, 0.5526315789473685, 0.5526315789473685, 0.5526315789473685, 0.5526315789473685, 0.5526315789473685, 0.5526315789473685, 0.5526315789473685, 0.5526315789473685, 0.5526315789473685, 0.5526315789473685, 0.5526315789473685, 0.5657894736842105, 0.5964912280701754, 0.6622807017543859, 0.6798245614035088, 0.6885964912280702, 0.706140350877193, 0.7192982456140351, 0.7324561403508771, 0.7368421052631579, 0.7236842105263158, 0.7324561403508771, 0.75, 0.7543859649122807, 0.75, 0.7412280701754386, 0.7368421052631579, 0.7324561403508771, 0.7368421052631579, 0.7368421052631579, 0.7412280701754386, 0.7412280701754386, 0.7412280701754386, 0.7412280701754386, 0.75, 0.7587719298245614, 0.7543859649122807, 0.7587719298245614, 0.7763157894736842, 0.7763157894736842, 0.7763157894736842, 0.7763157894736842, 0.7807017543859649, 0.7763157894736842, 0.7850877192982456, 0.793859649122807, 0.7850877192982456, 0.7807017543859649, 0.7807017543859649, 0.7807017543859649, 0.793859649122807, 0.7894736842105263, 0.7894736842105263, 0.7894736842105263, 0.7894736842105263, 0.7850877192982456, 0.7982456140350878, 0.8070175438596491, 0.8070175438596491, 0.8070175438596491, 0.8201754385964912, 0.8157894736842105, 0.8114035087719298, 0.8157894736842105, 0.8157894736842105, 0.8157894736842105, 0.8157894736842105, 0.8201754385964912, 0.8245614035087719, 0.8114035087719298, 0.8114035087719298, 0.8157894736842105, 0.8070175438596491, 0.7982456140350878, 0.7894736842105263, 0.7807017543859649, 0.7807017543859649, 0.7807017543859649, 0.7807017543859649, 0.7850877192982456, 0.793859649122807, 0.7982456140350878, 0.7982456140350878, 0.793859649122807, 0.7850877192982456, 0.7850877192982456, 0.8026315789473685, 0.8070175438596491, 0.8157894736842105, 0.8070175438596491, 0.8157894736842105, 0.8157894736842105, 0.8201754385964912, 0.8333333333333334, 0.8333333333333334, 0.8201754385964912, 0.8070175438596491, 0.8026315789473685, 0.8026315789473685, 0.8026315789473685, 0.8157894736842105, 0.8245614035087719, 0.8421052631578947, 0.8070175438596491, 0.8026315789473685, 0.7894736842105263, 0.793859649122807, 0.7850877192982456, 0.7894736842105263, 0.7850877192982456, 0.7894736842105263, 0.793859649122807, 0.7850877192982456, 0.7850877192982456, 0.7850877192982456, 0.7894736842105263, 0.793859649122807, 0.7982456140350878, 0.8026315789473685, 0.8114035087719298, 0.8333333333333334, 0.8421052631578947, 0.8377192982456141, 0.8201754385964912, 0.8201754385964912, 0.8070175438596491, 0.8157894736842105, 0.8114035087719298, 0.8157894736842105, 0.8157894736842105, 0.8157894736842105, 0.8245614035087719, 0.8421052631578947, 0.8333333333333334, 0.8289473684210527, 0.8245614035087719, 0.7982456140350878, 0.793859649122807, 0.793859649122807, 0.793859649122807, 0.8026315789473685, 0.8289473684210527, 0.8245614035087719, 0.8201754385964912, 0.8289473684210527, 0.8333333333333334, 0.8464912280701754, 0.8333333333333334, 0.8333333333333334, 0.8421052631578947, 0.8377192982456141, 0.8377192982456141, 0.8377192982456141, 0.8377192982456141, 0.8377192982456141, 0.8333333333333334, 0.8464912280701754, 0.8377192982456141, 0.8245614035087719, 0.8245614035087719, 0.7894736842105263, 0.7675438596491229, 0.7631578947368421, 0.7719298245614035, 0.7719298245614035, 0.7675438596491229, 0.7807017543859649, 0.8070175438596491, 0.8157894736842105, 0.8201754385964912, 0.8245614035087719, 0.8421052631578947, 0.8421052631578947, 0.8377192982456141, 0.8201754385964912, 0.8201754385964912, 0.8377192982456141, 0.8377192982456141, 0.8289473684210527, 0.8245614035087719, 0.8289473684210527, 0.8201754385964912, 0.8157894736842105, 0.8114035087719298, 0.8201754385964912, 0.8201754385964912, 0.8157894736842105, 0.8070175438596491, 0.8070175438596491, 0.8026315789473685, 0.8026315789473685, 0.8114035087719298, 0.8245614035087719, 0.8289473684210527, 0.8201754385964912, 0.8377192982456141, 0.8245614035087719, 0.8201754385964912, 0.8070175438596491, 0.8157894736842105, 0.8114035087719298, 0.8114035087719298, 0.8157894736842105, 0.8245614035087719, 0.8289473684210527, 0.8333333333333334, 0.8245614035087719, 0.8114035087719298, 0.8201754385964912, 0.8070175438596491, 0.7982456140350878, 0.7982456140350878, 0.8157894736842105, 0.8245614035087719, 0.8333333333333334, 0.8377192982456141, 0.8333333333333334, 0.8289473684210527, 0.8201754385964912, 0.8245614035087719, 0.8157894736842105, 0.8114035087719298, 0.8245614035087719, 0.8245614035087719, 0.8289473684210527, 0.8333333333333334, 0.8377192982456141, 0.8421052631578947, 0.8289473684210527, 0.8245614035087719, 0.8245614035087719, 0.8245614035087719, 0.8333333333333334]]}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Evaluate the Bi-LSTM model, we reach a score of around 82% on the test set!"
      ],
      "metadata": {
        "id": "QnA7NvT2jOX8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.eval()\n",
        "total_acc = 0\n",
        "total_count = 0\n",
        "for _, d in enumerate(val_loader_sexist):\n",
        "    inputs, labels = d\n",
        "    outputs = model(inputs)\n",
        "\n",
        "    # forward + backward + optimize\n",
        "    outputs = model(inputs)\n",
        "\n",
        "    targets = labels.to(torch.float32)\n",
        "    outputs = outputs.to(torch.float32)\n",
        "\n",
        "    total_acc += (torch.round(outputs) == labels).sum().item()\n",
        "    total_count += labels.size(0)\n",
        "\n",
        "accuracy = total_acc/total_count\n",
        "print(accuracy)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qFkk1qXSvBin",
        "outputId": "460e04c6-385e-41fa-c043-a573b5299b86"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.8114035087719298\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Getting the MAML algorithm!"
      ],
      "metadata": {
        "id": "iWeFvw4BskBi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Embedding layers\n",
        "\n",
        "#Sexist embedding layers\n",
        "embedding_sexist = nn.Embedding(dataset_sexist.embedding_matrix.shape[0], 50)\n",
        "embedding_sexist.weight=nn.Parameter(torch.tensor(dataset_sexist.embedding_matrix, dtype=torch.float32))\n",
        "\n",
        "#Amazon embedding layers\n",
        "embedding_amazon = nn.Embedding(dataset_amazon.embedding_matrix.shape[0], 50)\n",
        "embedding_amazon.weight=nn.Parameter(torch.tensor(dataset_amazon.embedding_matrix, dtype=torch.float32))\n",
        "\n",
        "#IMDB embedding layers\n",
        "embedding_imdb = nn.Embedding(dataset_imdb.embedding_matrix.shape[0], 50)\n",
        "embedding_imdb.weight=nn.Parameter(torch.tensor(dataset_imdb.embedding_matrix, dtype=torch.float32))"
      ],
      "metadata": {
        "id": "d_J1QhE9T5O1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.optim.adam import Adam\n",
        "import copy\n",
        "\n",
        "class MAML():\n",
        "    def __init__(self, epochs, inner_updates, loss_fn, inner_stepsize, meta_stepsize, dict_embeddings, args_learner, tasks_list, target_task) -> None:\n",
        "        self.epochs = epochs\n",
        "        self.inner_updates = inner_updates \n",
        "        self.learner = BiLSTM(args_learner)\n",
        "        self.loss_fn = loss_fn\n",
        "        self.inner_stepsize = inner_stepsize\n",
        "        self.meta_step_size = meta_stepsize\n",
        "        self.embedding_dict = dict_embeddings\n",
        "        self.tasks_list = tasks_list #list of (train_loader, val_loader, key)\n",
        "        self.target_train_loader, self.target_val_loader, self.key = target_task #thruple (train_loader, val_loader, key)\n",
        "        self.args_learner = args_learner\n",
        "        self.results = [] #each position is the ith train call\n",
        "        self.models = []\n",
        "        self.eval = 0\n",
        "        self.t = 0\n",
        "\n",
        "    def evaluate(self, weights=None):\n",
        "        print(f'Evaluation: {self.eval}')\n",
        "        self.eval += 1\n",
        "        #Create result dict at this evaluation position\n",
        "        results = {\n",
        "            'loss' : [],\n",
        "            'accuracy_training': [],\n",
        "            'accuracy_validation' : []\n",
        "        }\n",
        "        if weights == None:\n",
        "            # Make a copy of the model\n",
        "            args = copy.deepcopy(self.args_learner)\n",
        "            args['requires_grad'] = False\n",
        "            model = BiLSTM(args)\n",
        "\n",
        "            #load the state dict cause you'll be training, don't want to mess up the weights\n",
        "            state_dict = copy.deepcopy(self.learner.state_dict())\n",
        "            model.load_state_dict(state_dict)\n",
        "        else:\n",
        "            model = BiLSTM(args_learner)\n",
        "            model.load(weights)\n",
        "\n",
        "        # Train\n",
        "        #for e in range(5):\n",
        "        for i, data in enumerate(self.target_train_loader):\n",
        "            if i == 4:\n",
        "                break\n",
        "            model.train()\n",
        "            running_loss = 0\n",
        "            # get the inputs; data is a list of [inputs, labels]\n",
        "            inputs, labels = data\n",
        "\n",
        "            # zero the parameter gradients\n",
        "            optim.Adam(model.parameters(), lr=0.01).zero_grad()\n",
        "\n",
        "            # forward + backward + optimize\n",
        "            outputs = model(inputs)\n",
        "            #Get the loss on the batch\n",
        "            targets = labels.to(torch.float32)\n",
        "            outputs = outputs.to(torch.float32)\n",
        "            \n",
        "            loss = self.loss_fn(outputs, targets)\n",
        "\n",
        "            #gets the gradient on the batch for each parameter\n",
        "            loss.backward()\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), 0.1)\n",
        "\n",
        "            #take size lr step in gradient direction\n",
        "            optim.Adam(model.parameters(), lr=0.01).step()\n",
        "            total_acc = 0\n",
        "            total_count = 0\n",
        "            total_acc += (torch.round(outputs) == labels).sum().item()\n",
        "            total_count += labels.size(0)\n",
        "\n",
        "            # print statistics\n",
        "            running_loss += loss.item()\n",
        "            \n",
        "            #validation\n",
        "            model.eval()\n",
        "\n",
        "            #set metrics to 0\n",
        "            total_accuracy_val = 0\n",
        "            total_count_val = 0\n",
        "\n",
        "            for i_val, data_val in enumerate(self.target_val_loader):\n",
        "                inputs_val, labels_val = data_val\n",
        "\n",
        "                # forward + backward + optimize\n",
        "                outputs_val = model(inputs_val)\n",
        "\n",
        "                targets_val = labels_val.to(torch.float32)\n",
        "                outputs_val = outputs_val.to(torch.float32)\n",
        "\n",
        "                total_accuracy_val += (torch.round(outputs_val) == labels_val).sum().item()\n",
        "                total_count_val += labels_val.size(0)\n",
        "            \n",
        "            results['accuracy_training'].append(total_acc/total_count)\n",
        "            results['accuracy_validation'].append(total_accuracy_val/total_count_val)\n",
        "            results['loss'].append(running_loss/total_count)\n",
        "\n",
        "            #print(f'acc_train: {total_acc/total_count} \\| acc_val: {total_accuracy_val/total_count_val} \\| avg_loss: {running_loss/total_count}')\n",
        "            \n",
        "            #set the metrics to 0\n",
        "            running_loss = 0\n",
        "            total_acc, total_count = 0, 0\n",
        "\n",
        "        self.results.append({'results':results, 't': self.t, 'i': i, 'model_state_val': copy.deepcopy(model.state_dict()), 'model_state': copy.deepcopy(self.learner.state_dict())})\n",
        "\n",
        "        print(max(results['accuracy_validation']), i)\n",
        "        return model.state_dict()\n",
        "\n",
        "    def maml_loop(self):\n",
        "        print(f'Start_train: {self.t}')\n",
        "        self.t += 1\n",
        "        grads = []\n",
        "        #this is the meta loop, could sample from the # of tasks\n",
        "        for (train, val, key) in self.tasks_list:\n",
        "            #create a new model for gradient update in inner loop\n",
        "            #this is just to keep track of the gradien update\n",
        "            new_model = BiLSTM(self.args_learner)\n",
        "            state_dict = copy.deepcopy(self.learner.state_dict())\n",
        "\n",
        "            #load the correct embedding\n",
        "            new_model._modules['embedding'] = self.embedding_dict[key]\n",
        "            state_dict['embedding.weight'] = self.embedding_dict[key].weight\n",
        "\n",
        "            new_model.load_state_dict(state_dict)\n",
        "            \n",
        "\n",
        "            #run the inner loop on the new model for a given task\n",
        "            #just to keep track\n",
        "            fast_weights = dict((name, param) for (name, param) in new_model.named_parameters())\n",
        "\n",
        "            for updates in range(self.inner_updates):\n",
        "                in_, target = train.__iter__().next()\n",
        "\n",
        "                out = new_model.forward(in_)\n",
        "\n",
        "                out = out.to(torch.float32)\n",
        "                target = target.to(torch.float32)\n",
        "\n",
        "                #fix issue for cross entropy, maybe fixes the other issue about auto_grad\n",
        "                loss = self.loss_fn(out, target)\n",
        "                #get the gradient for task\n",
        "                g = torch.autograd.grad(loss, new_model.parameters(), create_graph=True)\n",
        "\n",
        "                #zero the embedding gradients since we're not learning those\n",
        "                g = tuple(g[i] if i != 0 else torch.zeros(g[0].shape, requires_grad=True) for i in range(len(g)))\n",
        "\n",
        "                #update weights\n",
        "                fast_weights = dict((name, param - self.inner_stepsize * g) for ((name, param), g) in zip(fast_weights.items(), g))\n",
        "\n",
        "                #load those weights into the new_model, embedding weights is the same\n",
        "                new_model.load_state_dict(fast_weights)\n",
        "\n",
        "            #Test the net after training\n",
        "            #write some print statements for sanity checks\n",
        "\n",
        "            #compute the meta_gradient on val and return it\n",
        "            in_, target = val.__iter__().next()\n",
        "            out = new_model.forward(in_)\n",
        "            out = out.to(torch.float32)\n",
        "            target = target.to(torch.float32)\n",
        "\n",
        "            #fix issue for cross entropy, maybe fixes the other issue about auto_grad\n",
        "            loss = self.loss_fn(out, target)\n",
        "\n",
        "            #normalizes the loss over the batch size\n",
        "            loss = loss / len(tasks_list)\n",
        "\n",
        "            g = torch.autograd.grad(loss, new_model.parameters(), create_graph=True)\n",
        "\n",
        "            #zero the gradients\n",
        "            #set to the learner model embedding weights\n",
        "            learner_embedding_size = copy.deepcopy(self.learner.state_dict())\n",
        "            learner_embedding_size = learner_embedding_size['embedding.weight'].shape\n",
        "            g = tuple(g[i] if i != 0 else torch.zeros(learner_embedding_size, requires_grad=True) for i in range(len(g)))\n",
        "\n",
        "            meta_grads = {name:g for ((name, _), g) in zip(new_model.named_parameters(), g)}\n",
        "\n",
        "            #pass the meta-task gradients to the \n",
        "            grads.append(meta_grads)\n",
        "            \n",
        "        #load new task completely for dummy, this task is the target task\n",
        "        in_, target = self.target_train_loader.__iter__().next()\n",
        "\n",
        "        # Do a dummy forward/backward pass to get correct grads into learner\n",
        "        # We will be replacing the gradients with hooks regardless\n",
        "        out = self.learner.forward(in_)\n",
        "        out = out.to(torch.float32)\n",
        "        target = target.to(torch.float32)\n",
        "        loss = self.loss_fn(out, target)\n",
        "\n",
        "        loss.backward(retain_graph=True)\n",
        "\n",
        "        # Unpack the gradients from dictionary of meta-gradients\n",
        "        gradients = {k: sum(d[k] for d in grads) for k in grads[0].keys()}\n",
        "        \n",
        "        hooks = []\n",
        "        for(k,v) in self.learner.named_parameters():\n",
        "            def get_closure():\n",
        "                key = k\n",
        "                def replace_grad(grad):\n",
        "                    return gradients[key]\n",
        "                return replace_grad\n",
        "            hooks.append(v.register_hook(get_closure()))\n",
        "        \n",
        "        # Compute grads for current step, replace with summed gradients as defined by hook\n",
        "        # DO DOUBLE CHECK THAT THE GRADIENTS ARE BEING UPDATED CORRECTLY WITH HOOKS\n",
        "        optim.Adam(self.learner.parameters(), lr=self.meta_step_size).zero_grad()\n",
        "        loss.backward()\n",
        "\n",
        "        # Update the net parameters with the accumulated gradient according to optimizer\n",
        "        optim.Adam(self.learner.parameters(), lr=self.meta_step_size).step()\n",
        "\n",
        "        # Remove the hooks before next training phase\n",
        "        for h in hooks:\n",
        "            h.remove()\n",
        "\n",
        "        #save the state dict so it can be used later\n",
        "        self.models.append(copy.deepcopy(self.learner.state_dict()))"
      ],
      "metadata": {
        "id": "dt6mBT78yHMn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Below are the arguments for the BiLSTM model which we will use as a learner model in the MAML meta-learning algorithm"
      ],
      "metadata": {
        "id": "UqsXQjVojcC2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tasks_list = [(train_loader_amazon, val_loader_amazon, 'amazon'), (train_loader_imdb, val_loader_imdb, 'imdb')]\n",
        "\n",
        "embedding_dict = {\n",
        "    'sexist': embedding_sexist,\n",
        "    'amazon': embedding_amazon,\n",
        "    'imdb': embedding_imdb,\n",
        "}\n",
        "\n",
        "# Here we will be changing the input matrix embedding with every different dataset so we set required_grad to True\n",
        "args_learner = {\n",
        "           'hidden_dim' : 32, #number of hidden dim\n",
        "           'vocab_size' : dataset_sexist.embedding_matrix.shape[0], #number of rows in word embedding matrix\n",
        "           'input_size' : 50,\n",
        "           'output_dim' : 1, #number of classes you're predicting\n",
        "           'embedding_matrix' : dataset_sexist.embedding_matrix, #the embedding matrix you've built in dataset constructor\n",
        "           'drp' : 0.2, #dropout layer for forward LSTM\n",
        "           'requires_grad': True,\n",
        "    }"
      ],
      "metadata": {
        "id": "63ik73HFehxl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here we get the average of the results of the learner model train with MAML. We use this to determine how many iterations are required to get a good meta-trained model. "
      ],
      "metadata": {
        "id": "k5tYdUlujlSN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "results = []\n",
        "\n",
        "#mother of all experiments\n",
        "#get 10 values for this step size\n",
        "#this gave us the below strings\n",
        "\n",
        "for lr in range(10):\n",
        "    maml = MAML(epochs=15, \n",
        "    inner_updates=2, \n",
        "    loss_fn=nn.BCELoss(), \n",
        "    inner_stepsize=0.01, \n",
        "    meta_stepsize=0.001, \n",
        "    dict_embeddings=embedding_dict, \n",
        "    args_learner=args_learner, \n",
        "    tasks_list = tasks_list, \n",
        "    target_task = (train_loader_sexist, val_loader_sexist, 'sexist'))\n",
        "\n",
        "    for i in range(50):\n",
        "        maml.maml_loop()\n",
        "        maml.evaluate()\n",
        "    results.append(copy.deepcopy(maml.results))"
      ],
      "metadata": {
        "id": "p6F2SBi2DZCO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#save the model\n",
        "\n",
        "#save with optimal params\n",
        "optimal_val = results[36]['model_state_val']\n",
        "optimal_state = results[36]['model_state']\n",
        "torch.save(optimal_val, \"/content/drive/MyDrive/COMP550 Final Project/Models/Optimal Parameters/model_state_val\")\n",
        "torch.save(optimal_state, \"/content/drive/MyDrive/COMP550 Final Project/Models/Optimal Parameters/model_state\")\n",
        "\n",
        "#save with best, it was model after 36 iterations\n",
        "best_val = results[36]['model_state_val']\n",
        "best_state = results[36]['model_state']\n",
        "torch.save(best_val, \"/content/drive/MyDrive/COMP550 Final Project/Models/Best/model_state_val\")\n",
        "torch.save(best_state, \"/content/drive/MyDrive/COMP550 Final Project/Models/Best/model_state\")"
      ],
      "metadata": {
        "id": "QPeU0tJJuCc4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Plotting*\n"
      ],
      "metadata": {
        "id": "pluv-X1O2nxK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def process_strings(s):\n",
        "    for i in range(50):\n",
        "        s = s.replace(f'Start_train: {i} Evaluation: {i}', '')\n",
        "    s = s.split(' 4')\n",
        "    s = [i.replace(' ', '') for i in s]\n",
        "    s.pop()\n",
        "    s = [float(i) for i in s]\n",
        "\n",
        "    return s"
      ],
      "metadata": {
        "id": "ilrkLecqhp6D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Below are the results of the loop which found the best number of loops to train the meta-learner"
      ],
      "metadata": {
        "id": "OCMl6EhIkarU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "c1 = \"Start_train: 0 Evaluation: 0 0.6096491228070176 4 Start_train: 1 Evaluation: 1 0.7017543859649122 4 Start_train: 2 Evaluation: 2 0.5570175438596491 4 Start_train: 3 Evaluation: 3 0.5570175438596491 4 Start_train: 4 Evaluation: 4 0.5701754385964912 4 Start_train: 5 Evaluation: 5 0.5921052631578947 4 Start_train: 6 Evaluation: 6 0.5570175438596491 4 Start_train: 7 Evaluation: 7 0.5789473684210527 4 Start_train: 8 Evaluation: 8 0.5570175438596491 4 Start_train: 9 Evaluation: 9 0.5570175438596491 4 Start_train: 10 Evaluation: 10 0.5877192982456141 4 Start_train: 11 Evaluation: 11 0.5570175438596491 4 Start_train: 12 Evaluation: 12 0.5570175438596491 4 Start_train: 13 Evaluation: 13 0.618421052631579 4 Start_train: 14 Evaluation: 14 0.5570175438596491 4 Start_train: 15 Evaluation: 15 0.6929824561403509 4 Start_train: 16 Evaluation: 16 0.618421052631579 4 Start_train: 17 Evaluation: 17 0.6973684210526315 4 Start_train: 18 Evaluation: 18 0.5570175438596491 4 Start_train: 19 Evaluation: 19 0.5745614035087719 4 Start_train: 20 Evaluation: 20 0.706140350877193 4 Start_train: 21 Evaluation: 21 0.6973684210526315 4 Start_train: 22 Evaluation: 22 0.5570175438596491 4 Start_train: 23 Evaluation: 23 0.7149122807017544 4 Start_train: 24 Evaluation: 24 0.6491228070175439 4 Start_train: 25 Evaluation: 25 0.5657894736842105 4 Start_train: 26 Evaluation: 26 0.5657894736842105 4 Start_train: 27 Evaluation: 27 0.6140350877192983 4 Start_train: 28 Evaluation: 28 0.6929824561403509 4 Start_train: 29 Evaluation: 29 0.6798245614035088 4 Start_train: 30 Evaluation: 30 0.6842105263157895 4 Start_train: 31 Evaluation: 31 0.5570175438596491 4 Start_train: 32 Evaluation: 32 0.5789473684210527 4 Start_train: 33 Evaluation: 33 0.5570175438596491 4 Start_train: 34 Evaluation: 34 0.5657894736842105 4 Start_train: 35 Evaluation: 35 0.5833333333333334 4 Start_train: 36 Evaluation: 36 0.6666666666666666 4 Start_train: 37 Evaluation: 37 0.631578947368421 4 Start_train: 38 Evaluation: 38 0.6842105263157895 4 Start_train: 39 Evaluation: 39 0.6228070175438597 4 Start_train: 40 Evaluation: 40 0.6052631578947368 4 Start_train: 41 Evaluation: 41 0.6403508771929824 4 Start_train: 42 Evaluation: 42 0.5657894736842105 4 Start_train: 43 Evaluation: 43 0.6578947368421053 4 Start_train: 44 Evaluation: 44 0.5701754385964912 4 Start_train: 45 Evaluation: 45 0.5833333333333334 4 Start_train: 46 Evaluation: 46 0.5570175438596491 4 Start_train: 47 Evaluation: 47 0.5570175438596491 4 Start_train: 48 Evaluation: 48 0.5745614035087719 4 Start_train: 49 Evaluation: 49 0.7017543859649122 4\"\n",
        "c2 = \"Start_train: 0 Evaluation: 0 0.5614035087719298 4 Start_train: 1 Evaluation: 1 0.6403508771929824 4 Start_train: 2 Evaluation: 2 0.5570175438596491 4 Start_train: 3 Evaluation: 3 0.5570175438596491 4 Start_train: 4 Evaluation: 4 0.5570175438596491 4 Start_train: 5 Evaluation: 5 0.5570175438596491 4 Start_train: 6 Evaluation: 6 0.6885964912280702 4 Start_train: 7 Evaluation: 7 0.5570175438596491 4 Start_train: 8 Evaluation: 8 0.6271929824561403 4 Start_train: 9 Evaluation: 9 0.631578947368421 4 Start_train: 10 Evaluation: 10 0.5570175438596491 4 Start_train: 11 Evaluation: 11 0.7149122807017544 4 Start_train: 12 Evaluation: 12 0.618421052631579 4 Start_train: 13 Evaluation: 13 0.44298245614035087 4 Start_train: 14 Evaluation: 14 0.5570175438596491 4 Start_train: 15 Evaluation: 15 0.5701754385964912 4 Start_train: 16 Evaluation: 16 0.5570175438596491 4 Start_train: 17 Evaluation: 17 0.6535087719298246 4 Start_train: 18 Evaluation: 18 0.6535087719298246 4 Start_train: 19 Evaluation: 19 0.6447368421052632 4 Start_train: 20 Evaluation: 20 0.5570175438596491 4 Start_train: 21 Evaluation: 21 0.5921052631578947 4 Start_train: 22 Evaluation: 22 0.6842105263157895 4 Start_train: 23 Evaluation: 23 0.7236842105263158 4 Start_train: 24 Evaluation: 24 0.6447368421052632 4 Start_train: 25 Evaluation: 25 0.5921052631578947 4 Start_train: 26 Evaluation: 26 0.6622807017543859 4 Start_train: 27 Evaluation: 27 0.5570175438596491 4 Start_train: 28 Evaluation: 28 0.5570175438596491 4 Start_train: 29 Evaluation: 29 0.6447368421052632 4 Start_train: 30 Evaluation: 30 0.5614035087719298 4 Start_train: 31 Evaluation: 31 0.6622807017543859 4 Start_train: 32 Evaluation: 32 0.6403508771929824 4 Start_train: 33 Evaluation: 33 0.6008771929824561 4 Start_train: 34 Evaluation: 34 0.6754385964912281 4 Start_train: 35 Evaluation: 35 0.7236842105263158 4 Start_train: 36 Evaluation: 36 0.5657894736842105 4 Start_train: 37 Evaluation: 37 0.5701754385964912 4 Start_train: 38 Evaluation: 38 0.5570175438596491 4 Start_train: 39 Evaluation: 39 0.5657894736842105 4 Start_train: 40 Evaluation: 40 0.6798245614035088 4 Start_train: 41 Evaluation: 41 0.5657894736842105 4 Start_train: 42 Evaluation: 42 0.5701754385964912 4 Start_train: 43 Evaluation: 43 0.5614035087719298 4 Start_train: 44 Evaluation: 44 0.7105263157894737 4 Start_train: 45 Evaluation: 45 0.5833333333333334 4 Start_train: 46 Evaluation: 46 0.5833333333333334 4 Start_train: 47 Evaluation: 47 0.5657894736842105 4 Start_train: 48 Evaluation: 48 0.6842105263157895 4 Start_train: 49 Evaluation: 49 0.5964912280701754 4\"\n",
        "c3 = \"Start_train: 0 Evaluation: 0 0.5570175438596491 4 Start_train: 1 Evaluation: 1 0.5570175438596491 4 Start_train: 2 Evaluation: 2 0.5570175438596491 4 Start_train: 3 Evaluation: 3 0.5570175438596491 4 Start_train: 4 Evaluation: 4 0.5570175438596491 4 Start_train: 5 Evaluation: 5 0.5570175438596491 4 Start_train: 6 Evaluation: 6 0.5570175438596491 4 Start_train: 7 Evaluation: 7 0.5745614035087719 4 Start_train: 8 Evaluation: 8 0.5570175438596491 4 Start_train: 9 Evaluation: 9 0.6929824561403509 4 Start_train: 10 Evaluation: 10 0.6973684210526315 4 Start_train: 11 Evaluation: 11 0.5745614035087719 4 Start_train: 12 Evaluation: 12 0.6228070175438597 4 Start_train: 13 Evaluation: 13 0.6754385964912281 4 Start_train: 14 Evaluation: 14 0.618421052631579 4 Start_train: 15 Evaluation: 15 0.5614035087719298 4 Start_train: 16 Evaluation: 16 0.5570175438596491 4 Start_train: 17 Evaluation: 17 0.6535087719298246 4 Start_train: 18 Evaluation: 18 0.7105263157894737 4 Start_train: 19 Evaluation: 19 0.5570175438596491 4 Start_train: 20 Evaluation: 20 0.706140350877193 4 Start_train: 21 Evaluation: 21 0.7017543859649122 4 Start_train: 22 Evaluation: 22 0.7412280701754386 4 Start_train: 23 Evaluation: 23 0.5570175438596491 4 Start_train: 24 Evaluation: 24 0.5570175438596491 4 Start_train: 25 Evaluation: 25 0.6578947368421053 4 Start_train: 26 Evaluation: 26 0.5570175438596491 4 Start_train: 27 Evaluation: 27 0.5570175438596491 4 Start_train: 28 Evaluation: 28 0.6842105263157895 4 Start_train: 29 Evaluation: 29 0.6491228070175439 4 Start_train: 30 Evaluation: 30 0.5570175438596491 4 Start_train: 31 Evaluation: 31 0.5570175438596491 4 Start_train: 32 Evaluation: 32 0.5833333333333334 4 Start_train: 33 Evaluation: 33 0.5789473684210527 4 Start_train: 34 Evaluation: 34 0.5570175438596491 4 Start_train: 35 Evaluation: 35 0.6666666666666666 4 Start_train: 36 Evaluation: 36 0.5570175438596491 4 Start_train: 37 Evaluation: 37 0.5570175438596491 4 Start_train: 38 Evaluation: 38 0.618421052631579 4 Start_train: 39 Evaluation: 39 0.5570175438596491 4 Start_train: 40 Evaluation: 40 0.5570175438596491 4 Start_train: 41 Evaluation: 41 0.6052631578947368 4 Start_train: 42 Evaluation: 42 0.5701754385964912 4 Start_train: 43 Evaluation: 43 0.6929824561403509 4 Start_train: 44 Evaluation: 44 0.6622807017543859 4 Start_train: 45 Evaluation: 45 0.6008771929824561 4 Start_train: 46 Evaluation: 46 0.7149122807017544 4 Start_train: 47 Evaluation: 47 0.6096491228070176 4 Start_train: 48 Evaluation: 48 0.5921052631578947 4 Start_train: 49 Evaluation: 49 0.631578947368421 4\"\n",
        "c4 = \"Start_train: 0 Evaluation: 0 0.5570175438596491 4 Start_train: 1 Evaluation: 1 0.5570175438596491 4 Start_train: 2 Evaluation: 2 0.6622807017543859 4 Start_train: 3 Evaluation: 3 0.5570175438596491 4 Start_train: 4 Evaluation: 4 0.5570175438596491 4 Start_train: 5 Evaluation: 5 0.5570175438596491 4 Start_train: 6 Evaluation: 6 0.5570175438596491 4 Start_train: 7 Evaluation: 7 0.6535087719298246 4 Start_train: 8 Evaluation: 8 0.6535087719298246 4 Start_train: 9 Evaluation: 9 0.5570175438596491 4 Start_train: 10 Evaluation: 10 0.5570175438596491 4 Start_train: 11 Evaluation: 11 0.5570175438596491 4 Start_train: 12 Evaluation: 12 0.6008771929824561 4 Start_train: 13 Evaluation: 13 0.5570175438596491 4 Start_train: 14 Evaluation: 14 0.6798245614035088 4 Start_train: 15 Evaluation: 15 0.5570175438596491 4 Start_train: 16 Evaluation: 16 0.5570175438596491 4 Start_train: 17 Evaluation: 17 0.5570175438596491 4 Start_train: 18 Evaluation: 18 0.5570175438596491 4 Start_train: 19 Evaluation: 19 0.6885964912280702 4 Start_train: 20 Evaluation: 20 0.5570175438596491 4 Start_train: 21 Evaluation: 21 0.5570175438596491 4 Start_train: 22 Evaluation: 22 0.6052631578947368 4 Start_train: 23 Evaluation: 23 0.6578947368421053 4 Start_train: 24 Evaluation: 24 0.5701754385964912 4 Start_train: 25 Evaluation: 25 0.5570175438596491 4 Start_train: 26 Evaluation: 26 0.5570175438596491 4 Start_train: 27 Evaluation: 27 0.6140350877192983 4 Start_train: 28 Evaluation: 28 0.6885964912280702 4 Start_train: 29 Evaluation: 29 0.6578947368421053 4 Start_train: 30 Evaluation: 30 0.5570175438596491 4 Start_train: 31 Evaluation: 31 0.6008771929824561 4 Start_train: 32 Evaluation: 32 0.543859649122807 4 Start_train: 33 Evaluation: 33 0.5570175438596491 4 Start_train: 34 Evaluation: 34 0.6710526315789473 4 Start_train: 35 Evaluation: 35 0.5570175438596491 4 Start_train: 36 Evaluation: 36 0.5570175438596491 4 Start_train: 37 Evaluation: 37 0.6228070175438597 4 Start_train: 38 Evaluation: 38 0.5789473684210527 4 Start_train: 39 Evaluation: 39 0.7149122807017544 4 Start_train: 40 Evaluation: 40 0.5570175438596491 4 Start_train: 41 Evaluation: 41 0.5833333333333334 4 Start_train: 42 Evaluation: 42 0.6096491228070176 4 Start_train: 43 Evaluation: 43 0.6535087719298246 4 Start_train: 44 Evaluation: 44 0.6754385964912281 4 Start_train: 45 Evaluation: 45 0.6754385964912281 4 Start_train: 46 Evaluation: 46 0.5921052631578947 4 Start_train: 47 Evaluation: 47 0.706140350877193 4 Start_train: 48 Evaluation: 48 0.5657894736842105 4 Start_train: 49 Evaluation: 49 0.6973684210526315 4\"\n",
        "c5 = \"Start_train: 0 Evaluation: 0 0.706140350877193 4 Start_train: 1 Evaluation: 1 0.6491228070175439 4 Start_train: 2 Evaluation: 2 0.5745614035087719 4 Start_train: 3 Evaluation: 3 0.6710526315789473 4 Start_train: 4 Evaluation: 4 0.5570175438596491 4 Start_train: 5 Evaluation: 5 0.5877192982456141 4 Start_train: 6 Evaluation: 6 0.5570175438596491 4 Start_train: 7 Evaluation: 7 0.5614035087719298 4 Start_train: 8 Evaluation: 8 0.5570175438596491 4 Start_train: 9 Evaluation: 9 0.6140350877192983 4 Start_train: 10 Evaluation: 10 0.5789473684210527 4 Start_train: 11 Evaluation: 11 0.5570175438596491 4 Start_train: 12 Evaluation: 12 0.5570175438596491 4 Start_train: 13 Evaluation: 13 0.5921052631578947 4 Start_train: 14 Evaluation: 14 0.5833333333333334 4 Start_train: 15 Evaluation: 15 0.7192982456140351 4 Start_train: 16 Evaluation: 16 0.5570175438596491 4 Start_train: 17 Evaluation: 17 0.6798245614035088 4 Start_train: 18 Evaluation: 18 0.618421052631579 4 Start_train: 19 Evaluation: 19 0.5570175438596491 4 Start_train: 20 Evaluation: 20 0.5745614035087719 4 Start_train: 21 Evaluation: 21 0.6578947368421053 4 Start_train: 22 Evaluation: 22 0.6271929824561403 4 Start_train: 23 Evaluation: 23 0.6535087719298246 4 Start_train: 24 Evaluation: 24 0.5570175438596491 4 Start_train: 25 Evaluation: 25 0.6578947368421053 4 Start_train: 26 Evaluation: 26 0.7149122807017544 4 Start_train: 27 Evaluation: 27 0.6535087719298246 4 Start_train: 28 Evaluation: 28 0.5657894736842105 4 Start_train: 29 Evaluation: 29 0.6052631578947368 4 Start_train: 30 Evaluation: 30 0.7236842105263158 4 Start_train: 31 Evaluation: 31 0.6228070175438597 4 Start_train: 32 Evaluation: 32 0.5657894736842105 4 Start_train: 33 Evaluation: 33 0.6403508771929824 4 Start_train: 34 Evaluation: 34 0.5701754385964912 4 Start_train: 35 Evaluation: 35 0.5570175438596491 4 Start_train: 36 Evaluation: 36 0.6403508771929824 4 Start_train: 37 Evaluation: 37 0.6710526315789473 4 Start_train: 38 Evaluation: 38 0.5877192982456141 4 Start_train: 39 Evaluation: 39 0.6008771929824561 4 Start_train: 40 Evaluation: 40 0.5570175438596491 4 Start_train: 41 Evaluation: 41 0.6973684210526315 4 Start_train: 42 Evaluation: 42 0.5614035087719298 4 Start_train: 43 Evaluation: 43 0.706140350877193 4 Start_train: 44 Evaluation: 44 0.6447368421052632 4 Start_train: 45 Evaluation: 45 0.6754385964912281 4 Start_train: 46 Evaluation: 46 0.5745614035087719 4 Start_train: 47 Evaluation: 47 0.6622807017543859 4 Start_train: 48 Evaluation: 48 0.5570175438596491 4 Start_train: 49 Evaluation: 49 0.5877192982456141 4\"\n",
        "c6 = \"Start_train: 0 Evaluation: 0 0.6798245614035088 4 Start_train: 1 Evaluation: 1 0.7105263157894737 4 Start_train: 2 Evaluation: 2 0.5921052631578947 4 Start_train: 3 Evaluation: 3 0.7412280701754386 4 Start_train: 4 Evaluation: 4 0.6359649122807017 4 Start_train: 5 Evaluation: 5 0.6491228070175439 4 Start_train: 6 Evaluation: 6 0.5570175438596491 4 Start_train: 7 Evaluation: 7 0.5570175438596491 4 Start_train: 8 Evaluation: 8 0.7192982456140351 4 Start_train: 9 Evaluation: 9 0.6403508771929824 4 Start_train: 10 Evaluation: 10 0.5657894736842105 4 Start_train: 11 Evaluation: 11 0.5570175438596491 4 Start_train: 12 Evaluation: 12 0.6710526315789473 4 Start_train: 13 Evaluation: 13 0.6228070175438597 4 Start_train: 14 Evaluation: 14 0.5570175438596491 4 Start_train: 15 Evaluation: 15 0.618421052631579 4 Start_train: 16 Evaluation: 16 0.5701754385964912 4 Start_train: 17 Evaluation: 17 0.6403508771929824 4 Start_train: 18 Evaluation: 18 0.5570175438596491 4 Start_train: 19 Evaluation: 19 0.5570175438596491 4 Start_train: 20 Evaluation: 20 0.7192982456140351 4 Start_train: 21 Evaluation: 21 0.6929824561403509 4 Start_train: 22 Evaluation: 22 0.5921052631578947 4 Start_train: 23 Evaluation: 23 0.5657894736842105 4 Start_train: 24 Evaluation: 24 0.5570175438596491 4 Start_train: 25 Evaluation: 25 0.6008771929824561 4 Start_train: 26 Evaluation: 26 0.6666666666666666 4 Start_train: 27 Evaluation: 27 0.5570175438596491 4 Start_train: 28 Evaluation: 28 0.5570175438596491 4 Start_train: 29 Evaluation: 29 0.6885964912280702 4 Start_train: 30 Evaluation: 30 0.7105263157894737 4 Start_train: 31 Evaluation: 31 0.5570175438596491 4 Start_train: 32 Evaluation: 32 0.7149122807017544 4 Start_train: 33 Evaluation: 33 0.5570175438596491 4 Start_train: 34 Evaluation: 34 0.5570175438596491 4 Start_train: 35 Evaluation: 35 0.5570175438596491 4 Start_train: 36 Evaluation: 36 0.5570175438596491 4 Start_train: 37 Evaluation: 37 0.5570175438596491 4 Start_train: 38 Evaluation: 38 0.6754385964912281 4 Start_train: 39 Evaluation: 39 0.5570175438596491 4 Start_train: 40 Evaluation: 40 0.5570175438596491 4 Start_train: 41 Evaluation: 41 0.6622807017543859 4 Start_train: 42 Evaluation: 42 0.6535087719298246 4 Start_train: 43 Evaluation: 43 0.7017543859649122 4 Start_train: 44 Evaluation: 44 0.6271929824561403 4 Start_train: 45 Evaluation: 45 0.6228070175438597 4 Start_train: 46 Evaluation: 46 0.5570175438596491 4 Start_train: 47 Evaluation: 47 0.7149122807017544 4 Start_train: 48 Evaluation: 48 0.6578947368421053 4 Start_train: 49 Evaluation: 49 0.6754385964912281 4\"\n",
        "c7 = \"Start_train: 0 Evaluation: 0 0.5570175438596491 4 Start_train: 1 Evaluation: 1 0.5570175438596491 4 Start_train: 2 Evaluation: 2 0.7017543859649122 4 Start_train: 3 Evaluation: 3 0.5570175438596491 4 Start_train: 4 Evaluation: 4 0.5570175438596491 4 Start_train: 5 Evaluation: 5 0.5570175438596491 4 Start_train: 6 Evaluation: 6 0.5570175438596491 4 Start_train: 7 Evaluation: 7 0.6271929824561403 4 Start_train: 8 Evaluation: 8 0.6052631578947368 4 Start_train: 9 Evaluation: 9 0.6491228070175439 4 Start_train: 10 Evaluation: 10 0.5570175438596491 4 Start_train: 11 Evaluation: 11 0.5570175438596491 4 Start_train: 12 Evaluation: 12 0.6535087719298246 4 Start_train: 13 Evaluation: 13 0.6140350877192983 4 Start_train: 14 Evaluation: 14 0.6491228070175439 4 Start_train: 15 Evaluation: 15 0.5570175438596491 4 Start_train: 16 Evaluation: 16 0.7456140350877193 4 Start_train: 17 Evaluation: 17 0.7368421052631579 4 Start_train: 18 Evaluation: 18 0.5657894736842105 4 Start_train: 19 Evaluation: 19 0.6140350877192983 4 Start_train: 20 Evaluation: 20 0.6578947368421053 4 Start_train: 21 Evaluation: 21 0.5570175438596491 4 Start_train: 22 Evaluation: 22 0.5570175438596491 4 Start_train: 23 Evaluation: 23 0.5570175438596491 4 Start_train: 24 Evaluation: 24 0.5745614035087719 4 Start_train: 25 Evaluation: 25 0.5614035087719298 4 Start_train: 26 Evaluation: 26 0.6929824561403509 4 Start_train: 27 Evaluation: 27 0.6842105263157895 4 Start_train: 28 Evaluation: 28 0.6096491228070176 4 Start_train: 29 Evaluation: 29 0.7631578947368421 4 Start_train: 30 Evaluation: 30 0.6271929824561403 4 Start_train: 31 Evaluation: 31 0.6008771929824561 4 Start_train: 32 Evaluation: 32 0.706140350877193 4 Start_train: 33 Evaluation: 33 0.5570175438596491 4 Start_train: 34 Evaluation: 34 0.5921052631578947 4 Start_train: 35 Evaluation: 35 0.706140350877193 4 Start_train: 36 Evaluation: 36 0.5921052631578947 4 Start_train: 37 Evaluation: 37 0.6666666666666666 4 Start_train: 38 Evaluation: 38 0.5921052631578947 4 Start_train: 39 Evaluation: 39 0.6096491228070176 4 Start_train: 40 Evaluation: 40 0.6622807017543859 4 Start_train: 41 Evaluation: 41 0.5964912280701754 4 Start_train: 42 Evaluation: 42 0.7192982456140351 4 Start_train: 43 Evaluation: 43 0.5614035087719298 4 Start_train: 44 Evaluation: 44 0.6008771929824561 4 Start_train: 45 Evaluation: 45 0.5964912280701754 4 Start_train: 46 Evaluation: 46 0.7236842105263158 4 Start_train: 47 Evaluation: 47 0.5877192982456141 4 Start_train: 48 Evaluation: 48 0.5570175438596491 4 Start_train: 49 Evaluation: 49 0.5570175438596491 4\"\n",
        "c8 = \"Start_train: 0 Evaluation: 0 0.5570175438596491 4 Start_train: 1 Evaluation: 1 0.5570175438596491 4 Start_train: 2 Evaluation: 2 0.7017543859649122 4 Start_train: 3 Evaluation: 3 0.5570175438596491 4 Start_train: 4 Evaluation: 4 0.5570175438596491 4 Start_train: 5 Evaluation: 5 0.5570175438596491 4 Start_train: 6 Evaluation: 6 0.5570175438596491 4 Start_train: 7 Evaluation: 7 0.6271929824561403 4 Start_train: 8 Evaluation: 8 0.6052631578947368 4 Start_train: 9 Evaluation: 9 0.6491228070175439 4 Start_train: 10 Evaluation: 10 0.5570175438596491 4 Start_train: 11 Evaluation: 11 0.5570175438596491 4 Start_train: 12 Evaluation: 12 0.6535087719298246 4 Start_train: 13 Evaluation: 13 0.6140350877192983 4 Start_train: 14 Evaluation: 14 0.6491228070175439 4 Start_train: 15 Evaluation: 15 0.5570175438596491 4 Start_train: 16 Evaluation: 16 0.7456140350877193 4 Start_train: 17 Evaluation: 17 0.7368421052631579 4 Start_train: 18 Evaluation: 18 0.5657894736842105 4 Start_train: 19 Evaluation: 19 0.6140350877192983 4 Start_train: 20 Evaluation: 20 0.6578947368421053 4 Start_train: 21 Evaluation: 21 0.5570175438596491 4 Start_train: 22 Evaluation: 22 0.5570175438596491 4 Start_train: 23 Evaluation: 23 0.5570175438596491 4 Start_train: 24 Evaluation: 24 0.5745614035087719 4 Start_train: 25 Evaluation: 25 0.5614035087719298 4 Start_train: 26 Evaluation: 26 0.6929824561403509 4 Start_train: 27 Evaluation: 27 0.6842105263157895 4 Start_train: 28 Evaluation: 28 0.6096491228070176 4 Start_train: 29 Evaluation: 29 0.7631578947368421 4 Start_train: 30 Evaluation: 30 0.6271929824561403 4 Start_train: 31 Evaluation: 31 0.6008771929824561 4 Start_train: 32 Evaluation: 32 0.706140350877193 4 Start_train: 33 Evaluation: 33 0.5570175438596491 4 Start_train: 34 Evaluation: 34 0.5921052631578947 4 Start_train: 35 Evaluation: 35 0.706140350877193 4 Start_train: 36 Evaluation: 36 0.5921052631578947 4 Start_train: 37 Evaluation: 37 0.6666666666666666 4 Start_train: 38 Evaluation: 38 0.5921052631578947 4 Start_train: 39 Evaluation: 39 0.6096491228070176 4 Start_train: 40 Evaluation: 40 0.6622807017543859 4 Start_train: 41 Evaluation: 41 0.5964912280701754 4 Start_train: 42 Evaluation: 42 0.7192982456140351 4 Start_train: 43 Evaluation: 43 0.5614035087719298 4 Start_train: 44 Evaluation: 44 0.6008771929824561 4 Start_train: 45 Evaluation: 45 0.5964912280701754 4 Start_train: 46 Evaluation: 46 0.7236842105263158 4 Start_train: 47 Evaluation: 47 0.5877192982456141 4 Start_train: 48 Evaluation: 48 0.5570175438596491 4 Start_train: 49 Evaluation: 49 0.5570175438596491 4\"\n",
        "c9 = \"Start_train: 0 Evaluation: 0 0.631578947368421 4 Start_train: 1 Evaluation: 1 0.7412280701754386 4 Start_train: 2 Evaluation: 2 0.5175438596491229 4 Start_train: 3 Evaluation: 3 0.6710526315789473 4 Start_train: 4 Evaluation: 4 0.6929824561403509 4 Start_train: 5 Evaluation: 5 0.6666666666666666 4 Start_train: 6 Evaluation: 6 0.7324561403508771 4 Start_train: 7 Evaluation: 7 0.5570175438596491 4 Start_train: 8 Evaluation: 8 0.5789473684210527 4 Start_train: 9 Evaluation: 9 0.5570175438596491 4 Start_train: 10 Evaluation: 10 0.7236842105263158 4 Start_train: 11 Evaluation: 11 0.6973684210526315 4 Start_train: 12 Evaluation: 12 0.6666666666666666 4 Start_train: 13 Evaluation: 13 0.5789473684210527 4 Start_train: 14 Evaluation: 14 0.6666666666666666 4 Start_train: 15 Evaluation: 15 0.7017543859649122 4 Start_train: 16 Evaluation: 16 0.6929824561403509 4 Start_train: 17 Evaluation: 17 0.5570175438596491 4 Start_train: 18 Evaluation: 18 0.706140350877193 4 Start_train: 19 Evaluation: 19 0.6666666666666666 4 Start_train: 20 Evaluation: 20 0.7280701754385965 4 Start_train: 21 Evaluation: 21 0.5833333333333334 4 Start_train: 22 Evaluation: 22 0.5570175438596491 4 Start_train: 23 Evaluation: 23 0.6973684210526315 4 Start_train: 24 Evaluation: 24 0.6710526315789473 4 Start_train: 25 Evaluation: 25 0.5570175438596491 4 Start_train: 26 Evaluation: 26 0.6359649122807017 4 Start_train: 27 Evaluation: 27 0.5833333333333334 4 Start_train: 28 Evaluation: 28 0.5570175438596491 4 Start_train: 29 Evaluation: 29 0.6491228070175439 4 Start_train: 30 Evaluation: 30 0.7105263157894737 4 Start_train: 31 Evaluation: 31 0.6885964912280702 4 Start_train: 32 Evaluation: 32 0.5964912280701754 4 Start_train: 33 Evaluation: 33 0.5701754385964912 4 Start_train: 34 Evaluation: 34 0.706140350877193 4 Start_train: 35 Evaluation: 35 0.5570175438596491 4 Start_train: 36 Evaluation: 36 0.6666666666666666 4 Start_train: 37 Evaluation: 37 0.7280701754385965 4 Start_train: 38 Evaluation: 38 0.5614035087719298 4 Start_train: 39 Evaluation: 39 0.5614035087719298 4 Start_train: 40 Evaluation: 40 0.6535087719298246 4 Start_train: 41 Evaluation: 41 0.5570175438596491 4 Start_train: 42 Evaluation: 42 0.5570175438596491 4 Start_train: 43 Evaluation: 43 0.5570175438596491 4 Start_train: 44 Evaluation: 44 0.6447368421052632 4 Start_train: 45 Evaluation: 45 0.5789473684210527 4 Start_train: 46 Evaluation: 46 0.706140350877193 4 Start_train: 47 Evaluation: 47 0.618421052631579 4 Start_train: 48 Evaluation: 48 0.5570175438596491 4 Start_train: 49 Evaluation: 49 0.6666666666666666 4\"\n",
        "c10 = \"Start_train: 0 Evaluation: 0 0.5701754385964912 4 Start_train: 1 Evaluation: 1 0.5570175438596491 4 Start_train: 2 Evaluation: 2 0.5570175438596491 4 Start_train: 3 Evaluation: 3 0.5570175438596491 4 Start_train: 4 Evaluation: 4 0.5570175438596491 4 Start_train: 5 Evaluation: 5 0.6622807017543859 4 Start_train: 6 Evaluation: 6 0.5833333333333334 4 Start_train: 7 Evaluation: 7 0.45614035087719296 4 Start_train: 8 Evaluation: 8 0.5745614035087719 4 Start_train: 9 Evaluation: 9 0.6447368421052632 4 Start_train: 10 Evaluation: 10 0.6798245614035088 4 Start_train: 11 Evaluation: 11 0.618421052631579 4 Start_train: 12 Evaluation: 12 0.7149122807017544 4 Start_train: 13 Evaluation: 13 0.6271929824561403 4 Start_train: 14 Evaluation: 14 0.5570175438596491 4 Start_train: 15 Evaluation: 15 0.631578947368421 4 Start_train: 16 Evaluation: 16 0.6403508771929824 4 Start_train: 17 Evaluation: 17 0.6929824561403509 4 Start_train: 18 Evaluation: 18 0.6008771929824561 4 Start_train: 19 Evaluation: 19 0.5570175438596491 4 Start_train: 20 Evaluation: 20 0.7236842105263158 4 Start_train: 21 Evaluation: 21 0.6228070175438597 4 Start_train: 22 Evaluation: 22 0.5657894736842105 4 Start_train: 23 Evaluation: 23 0.6140350877192983 4 Start_train: 24 Evaluation: 24 0.5570175438596491 4 Start_train: 25 Evaluation: 25 0.706140350877193 4 Start_train: 26 Evaluation: 26 0.5614035087719298 4 Start_train: 27 Evaluation: 27 0.5614035087719298 4 Start_train: 28 Evaluation: 28 0.5614035087719298 4 Start_train: 29 Evaluation: 29 0.706140350877193 4 Start_train: 30 Evaluation: 30 0.5570175438596491 4 Start_train: 31 Evaluation: 31 0.7236842105263158 4 Start_train: 32 Evaluation: 32 0.7236842105263158 4 Start_train: 33 Evaluation: 33 0.6359649122807017 4 Start_train: 34 Evaluation: 34 0.5789473684210527 4 Start_train: 35 Evaluation: 35 0.5745614035087719 4 Start_train: 36 Evaluation: 36 0.5745614035087719 4 Start_train: 37 Evaluation: 37 0.5745614035087719 4 Start_train: 38 Evaluation: 38 0.7236842105263158 4 Start_train: 39 Evaluation: 39 0.5570175438596491 4 Start_train: 40 Evaluation: 40 0.618421052631579 4 Start_train: 41 Evaluation: 41 0.7192982456140351 4 Start_train: 42 Evaluation: 42 0.5570175438596491 4 Start_train: 43 Evaluation: 43 0.5570175438596491 4 Start_train: 44 Evaluation: 44 0.7017543859649122 4 Start_train: 45 Evaluation: 45 0.7236842105263158 4 Start_train: 46 Evaluation: 46 0.5921052631578947 4 Start_train: 47 Evaluation: 47 0.6271929824561403 4 Start_train: 48 Evaluation: 48 0.7105263157894737 4 Start_train: 49 Evaluation: 49 0.7280701754385965 4\"\n",
        "\n",
        "results_raw = [c1, c2, c3, c4, c5, c6, c7, c8, c9, c10]\n",
        "results =[process_strings(i) for i in results_raw]"
      ],
      "metadata": {
        "id": "bCke1gWWjwwb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "averages = []\n",
        "\n",
        "#get the average\n",
        "for r in range(0, len(results[0])):\n",
        "    num = 0\n",
        "    for c in range(0, len(results)):\n",
        "        num += results[c][r]\n",
        "    num = num/len(results)\n",
        "    averages.append(num)\n",
        "print(averages)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LPsBBL8nnV5z",
        "outputId": "5b63ef70-933d-45cd-8e70-291a61e9ea83"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0.5986842105263157, 0.6228070175438595, 0.5978070175438596, 0.5982456140350877, 0.5798245614035087, 0.594298245614035, 0.5903508771929823, 0.575, 0.6035087719298247, 0.619298245614035, 0.606140350877193, 0.5947368421052631, 0.631578947368421, 0.5942982456140351, 0.6074561403508772, 0.6166666666666666, 0.6241228070175439, 0.6605263157894736, 0.6092105263157894, 0.6030701754385964, 0.6587719298245613, 0.6219298245614034, 0.6043859649122807, 0.6298245614035087, 0.5912280701754385, 0.6017543859649123, 0.6307017543859649, 0.606578947368421, 0.6083333333333332, 0.6807017543859649, 0.631578947368421, 0.6171052631578948, 0.6359649122807017, 0.581140350877193, 0.6065789473684211, 0.6188596491228069, 0.5969298245614035, 0.624561403508772, 0.6171052631578947, 0.5956140350877192, 0.6109649122807017, 0.6223684210526316, 0.6083333333333333, 0.6210526315789473, 0.643859649122807, 0.6236842105263158, 0.6324561403508772, 0.6236842105263157, 0.6013157894736841, 0.6399122807017544]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Plotted below is the best result of the Learner model trained with MAML, after having seen 64 datapoints once ie. let the model see 64 datapoints and test on the validation data."
      ],
      "metadata": {
        "id": "Mkg9uEkOs9IK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "plt.plot([i for i in range(50)], averages)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 282
        },
        "id": "04orXtiydJMw",
        "outputId": "17b9e31f-e88c-4faf-f1e1-327df9735689"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[<matplotlib.lines.Line2D at 0x7f6862bbf2d0>]"
            ]
          },
          "metadata": {},
          "execution_count": 58
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. Experimentation!\n",
        "\n",
        "To-do:\n",
        "\n",
        "\n",
        "*   Train vanilla on varying sizes of the training dataset and get numbers on the validation dataset\n",
        "*   Train MAML updated on varying size of the training dataset and get numbers on the validation dataset\n",
        "\n",
        "Uber time"
      ],
      "metadata": {
        "id": "Asexwgu0ktCt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#input: - the model you want to test\n",
        "#       - the dictionary in which you want to save the results\n",
        "def experiment(model):\n",
        "    results_experiment = {\n",
        "    'accuracy_training': [],\n",
        "    'loss': [],\n",
        "    'accuracy_validation': []\n",
        "}\n",
        "    model = model\n",
        "    criterion = nn.BCELoss()\n",
        "    optimizer = optim.Adam(model.parameters())\n",
        "    training_score = []\n",
        "    loss_score = []\n",
        "    validation_score = []\n",
        "\n",
        "    for e in range(5):\n",
        "        for i, data in enumerate(train_loader_sexist):\n",
        "            model.train()\n",
        "\n",
        "            running_loss = 0\n",
        "            # get the inputs; data is a list of [inputs, labels]\n",
        "            inputs, labels = data\n",
        "\n",
        "            # zero the parameter gradients\n",
        "            optim.Adam(model.parameters(), lr=0.01).zero_grad()\n",
        "\n",
        "            # forward + backward + optimize\n",
        "            outputs = model(inputs)\n",
        "            #Get the loss on the batch\n",
        "            targets = labels.to(torch.float32)\n",
        "            outputs = outputs.to(torch.float32)\n",
        "            \n",
        "            loss = criterion(outputs, targets)\n",
        "\n",
        "            #gets the gradient on the batch for each parameter\n",
        "            loss.backward()\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), 0.1)\n",
        "\n",
        "            #take size lr step in gradient direction\n",
        "            optimizer.step()\n",
        "            total_acc = 0\n",
        "            total_count = 0\n",
        "            total_acc += (torch.round(outputs) == labels).sum().item()\n",
        "            total_count += labels.size(0)\n",
        "\n",
        "            # print statistics\n",
        "            running_loss += loss.item()\n",
        "            \n",
        "            #validation\n",
        "            model.eval()\n",
        "\n",
        "            #set metrics to 0\n",
        "            total_accuracy_val = 0\n",
        "            total_count_val = 0\n",
        "\n",
        "            for i_val, data_val in enumerate(val_loader_sexist):\n",
        "                inputs_val, labels_val = data_val\n",
        "\n",
        "                # forward + backward + optimize\n",
        "                outputs_val = model(inputs_val)\n",
        "\n",
        "                targets_val = labels_val.to(torch.float32)\n",
        "                outputs_val = outputs_val.to(torch.float32)\n",
        "\n",
        "                total_accuracy_val += (torch.round(outputs_val) == labels_val).sum().item()\n",
        "                total_count_val += labels_val.size(0)\n",
        "            \n",
        "            training_score.append(total_acc/total_count)\n",
        "            validation_score.append(total_accuracy_val/total_count_val)\n",
        "            loss_score.append(running_loss/total_count)\n",
        "\n",
        "            #print(f'acc_train: {total_acc/total_count} \\| acc_val: {total_accuracy_val/total_count_val} \\| avg_loss: {running_loss/total_count}')\n",
        "            \n",
        "            #set the metrics to 0\n",
        "            running_loss = 0\n",
        "            total_acc, total_count = 0, 0\n",
        "\n",
        "    results_experiment['accuracy_training'].append(training_score)\n",
        "    results_experiment['accuracy_validation'].append(validation_score)\n",
        "    results_experiment['loss'].append(loss_score)\n",
        "\n",
        "    return results_experiment"
      ],
      "metadata": {
        "id": "NZNOXi0suMzR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "embedding_matrix = torch.load(\"/content/drive/MyDrive/COMP550 Final Project/Models/Optimal Parameters/model_state\")['embedding.weight']\n",
        "\n",
        "#fill the args dictionary with the required parameters\n",
        "args_model = {\n",
        "    'hidden_dim' : 32, #number of hidden dim\n",
        "    'vocab_size' : embedding_matrix.shape[0], #number of rows in word embedding matrix\n",
        "    'input_size' : 50,\n",
        "    'output_dim' : 1, #number of classes you're predicting\n",
        "    'embedding_matrix' : embedding_matrix, #the embedding matrix you've built in dataset constructor\n",
        "    'drp' : 0.2, #dropout layer for forward LSTM\n",
        "    'requires_grad': False,\n",
        "}\n",
        "\n",
        "#create the MAML model\n",
        "maml_trained_model = BiLSTM(args_model)\n",
        "maml_trained_model = maml_trained_model.load_state_dict(torch.load(\"/content/drive/MyDrive/COMP550 Final Project/Models/Optimal Parameters/model_state\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pF2ruL4gypdB",
        "outputId": "d6a4e048-5f0a-4e18-abc3-fc939802435b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:23: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "maml_trained_model.parameters()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 165
        },
        "id": "_cjiuzH2zEjV",
        "outputId": "a7b6cc5b-aa3d-46a6-d2e3-f7abb7ff7790"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-24-3a9e30502d3f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmaml_trained_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m: '_IncompatibleKeys' object has no attribute 'parameters'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "results_experiment = experiment(maml_trained_model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 287
        },
        "id": "NPmfvqtkyrfs",
        "outputId": "04d225f4-8376-4785-b883-664a0fad8793"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-23-9f19d18026e7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mresults_experiment\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mexperiment\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmaml_trained_model\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-22-08225da9310b>\u001b[0m in \u001b[0;36mexperiment\u001b[0;34m(model)\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0mcriterion\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mBCELoss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m     \u001b[0moptimizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAdam\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m     \u001b[0mtraining_score\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0mloss_score\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: '_IncompatibleKeys' object has no attribute 'parameters'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot()"
      ],
      "metadata": {
        "id": "ln-mXj_Iy4_W"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}